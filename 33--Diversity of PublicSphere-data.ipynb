{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\sstolwi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\sstolwi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.1.1 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"c:\\Users\\sstolwi\\Github\\llmdiv\\Lib\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"c:\\Users\\sstolwi\\Github\\llmdiv\\Lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"c:\\Users\\sstolwi\\Github\\llmdiv\\Lib\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"c:\\Users\\sstolwi\\Github\\llmdiv\\Lib\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.1776.0_x64__qbz5n2kfra8p0\\Lib\\asyncio\\base_events.py\", line 641, in run_forever\n",
      "    self._run_once()\n",
      "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.1776.0_x64__qbz5n2kfra8p0\\Lib\\asyncio\\base_events.py\", line 1986, in _run_once\n",
      "    handle._run()\n",
      "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.1776.0_x64__qbz5n2kfra8p0\\Lib\\asyncio\\events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"c:\\Users\\sstolwi\\Github\\llmdiv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"c:\\Users\\sstolwi\\Github\\llmdiv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"c:\\Users\\sstolwi\\Github\\llmdiv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"c:\\Users\\sstolwi\\Github\\llmdiv\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"c:\\Users\\sstolwi\\Github\\llmdiv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"c:\\Users\\sstolwi\\Github\\llmdiv\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"c:\\Users\\sstolwi\\Github\\llmdiv\\Lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"c:\\Users\\sstolwi\\Github\\llmdiv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"c:\\Users\\sstolwi\\Github\\llmdiv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"c:\\Users\\sstolwi\\Github\\llmdiv\\Lib\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"c:\\Users\\sstolwi\\Github\\llmdiv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"c:\\Users\\sstolwi\\Github\\llmdiv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"c:\\Users\\sstolwi\\Github\\llmdiv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\sstolwi\\AppData\\Local\\Temp\\ipykernel_24376\\1102504478.py\", line 16, in <module>\n",
      "    import src\n",
      "  File \"c:\\Users\\sstolwi\\Github\\TWON-Metrics\\src\\__init__.py\", line 1, in <module>\n",
      "    from .hf_classify import HFClassify\n",
      "  File \"c:\\Users\\sstolwi\\Github\\TWON-Metrics\\src\\hf_classify.py\", line 5, in <module>\n",
      "    import torch\n",
      "  File \"c:\\Users\\sstolwi\\Github\\llmdiv\\Lib\\site-packages\\torch\\__init__.py\", line 1477, in <module>\n",
      "    from .functional import *  # noqa: F403\n",
      "  File \"c:\\Users\\sstolwi\\Github\\llmdiv\\Lib\\site-packages\\torch\\functional.py\", line 9, in <module>\n",
      "    import torch.nn.functional as F\n",
      "  File \"c:\\Users\\sstolwi\\Github\\llmdiv\\Lib\\site-packages\\torch\\nn\\__init__.py\", line 1, in <module>\n",
      "    from .modules import *  # noqa: F403\n",
      "  File \"c:\\Users\\sstolwi\\Github\\llmdiv\\Lib\\site-packages\\torch\\nn\\modules\\__init__.py\", line 35, in <module>\n",
      "    from .transformer import TransformerEncoder, TransformerDecoder, \\\n",
      "  File \"c:\\Users\\sstolwi\\Github\\llmdiv\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 20, in <module>\n",
      "    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
      "c:\\Users\\sstolwi\\Github\\llmdiv\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at ..\\torch\\csrc\\utils\\tensor_numpy.cpp:84.)\n",
      "  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
      "c:\\Users\\sstolwi\\Github\\llmdiv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import typing\n",
    "\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import config\n",
    "import src\n",
    "import requests\n",
    "import tqdm\n",
    "import json\n",
    "import numpy as np\n",
    "import logging\n",
    "import torch\n",
    "device = torch.device('cpu')\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import cltrier_lib as lib\n",
    "\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CFG = config.Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>StartDate</th>\n",
       "      <th>RecordedDate</th>\n",
       "      <th>IPAddress</th>\n",
       "      <th>Finished</th>\n",
       "      <th>Coder</th>\n",
       "      <th>ID</th>\n",
       "      <th>Mark_ID</th>\n",
       "      <th>Genre</th>\n",
       "      <th>topiccode</th>\n",
       "      <th>Platform</th>\n",
       "      <th>...</th>\n",
       "      <th>dislikeCount_video</th>\n",
       "      <th>likeCount_video</th>\n",
       "      <th>date_difference</th>\n",
       "      <th>commentCount_video</th>\n",
       "      <th>replyCount_comment</th>\n",
       "      <th>topic</th>\n",
       "      <th>subscribers</th>\n",
       "      <th>HATELIST_FOCUSED_DUMMY</th>\n",
       "      <th>Time_comment_year</th>\n",
       "      <th>Time_video_year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5/30/2021 13:03:17</td>\n",
       "      <td>5/30/2021 13:04:17</td>\n",
       "      <td>62.194.51.29</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>UgyPHwv8G0cDE6-wEgl4AaABAg.8_0ZjJKSJty8_0kXGkAd2U</td>\n",
       "      <td>119</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2017</td>\n",
       "      <td>2017.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10/11/2021 10:34:05</td>\n",
       "      <td>10/11/2021 10:36:46</td>\n",
       "      <td>213.127.109.191</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>Ugx2WXq9UdV8mPPjejJ4AaABAg.8yHCKV0Boe58yYRxEQEF45</td>\n",
       "      <td>282</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>195.0</td>\n",
       "      <td>3817.0</td>\n",
       "      <td>743.0</td>\n",
       "      <td>1748.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>economy</td>\n",
       "      <td>3630000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2019</td>\n",
       "      <td>2019.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9/9/2021 18:49:48</td>\n",
       "      <td>9/9/2021 18:51:32</td>\n",
       "      <td>213.127.110.0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1110578710648890000</td>\n",
       "      <td>372</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2019</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6/6/2021 16:12:46</td>\n",
       "      <td>6/6/2021 16:16:16</td>\n",
       "      <td>213.127.76.145</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>UgwUPFScjJ0MCeaP2F54AaABAg.8lvp3fc9Euf8lvvgsUgEgV</td>\n",
       "      <td>769</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2018</td>\n",
       "      <td>2018.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6/13/2021 13:25:49</td>\n",
       "      <td>6/13/2021 13:27:28</td>\n",
       "      <td>213.127.82.232</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>UgwWKCWtSJdFvjGHvTp4AaABAg.8kUC5dGrQ2H8kUDRihE2f3</td>\n",
       "      <td>1206</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2018</td>\n",
       "      <td>2018.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3857</th>\n",
       "      <td>8/19/2021 14:50:13</td>\n",
       "      <td>8/19/2021 14:54:28</td>\n",
       "      <td>62.194.51.29</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1152219467579100000</td>\n",
       "      <td>10000695</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2019</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3858</th>\n",
       "      <td>8/19/2021 15:10:27</td>\n",
       "      <td>8/19/2021 15:12:21</td>\n",
       "      <td>62.194.51.29</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1085362296472430000</td>\n",
       "      <td>10007008</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2019</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3859</th>\n",
       "      <td>10/6/2021 16:08:39</td>\n",
       "      <td>10/6/2021 16:10:42</td>\n",
       "      <td>213.127.113.113</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>UghFY3QJ6nmT_ngCoAEC.7-H0Z7--wxd8goqpaPs-bl</td>\n",
       "      <td>20000102</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2820.0</td>\n",
       "      <td>12475.0</td>\n",
       "      <td>3803.0</td>\n",
       "      <td>4785.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>east</td>\n",
       "      <td>6740000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2018</td>\n",
       "      <td>2010.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3860</th>\n",
       "      <td>10/15/2021 18:30:04</td>\n",
       "      <td>10/15/2021 18:35:40</td>\n",
       "      <td>213.127.109.191</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>UgyWabsmmnq3zam4DgZ4AaABAg</td>\n",
       "      <td>20000418</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>118.0</td>\n",
       "      <td>31761.0</td>\n",
       "      <td>1531.0</td>\n",
       "      <td>2206.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>east</td>\n",
       "      <td>6800000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2018</td>\n",
       "      <td>2015.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3861</th>\n",
       "      <td>11/19/2021 17:49:17</td>\n",
       "      <td>11/19/2021 17:51:04</td>\n",
       "      <td>213.127.109.191</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>UgwPOHIDyICm10k0Mvx4AaABAg</td>\n",
       "      <td>20001003</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1358.0</td>\n",
       "      <td>5740.0</td>\n",
       "      <td>2276.0</td>\n",
       "      <td>2887.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>east</td>\n",
       "      <td>549000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2018</td>\n",
       "      <td>2018.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3862 rows × 79 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                StartDate         RecordedDate        IPAddress  Finished  \\\n",
       "0      5/30/2021 13:03:17   5/30/2021 13:04:17     62.194.51.29         1   \n",
       "1     10/11/2021 10:34:05  10/11/2021 10:36:46  213.127.109.191         1   \n",
       "2       9/9/2021 18:49:48    9/9/2021 18:51:32    213.127.110.0         1   \n",
       "3       6/6/2021 16:12:46    6/6/2021 16:16:16   213.127.76.145         1   \n",
       "4      6/13/2021 13:25:49   6/13/2021 13:27:28   213.127.82.232         1   \n",
       "...                   ...                  ...              ...       ...   \n",
       "3857   8/19/2021 14:50:13   8/19/2021 14:54:28     62.194.51.29         1   \n",
       "3858   8/19/2021 15:10:27   8/19/2021 15:12:21     62.194.51.29         1   \n",
       "3859   10/6/2021 16:08:39   10/6/2021 16:10:42  213.127.113.113         1   \n",
       "3860  10/15/2021 18:30:04  10/15/2021 18:35:40  213.127.109.191         1   \n",
       "3861  11/19/2021 17:49:17  11/19/2021 17:51:04  213.127.109.191         1   \n",
       "\n",
       "      Coder                                                 ID   Mark_ID  \\\n",
       "0         6  UgyPHwv8G0cDE6-wEgl4AaABAg.8_0ZjJKSJty8_0kXGkAd2U       119   \n",
       "1         6  Ugx2WXq9UdV8mPPjejJ4AaABAg.8yHCKV0Boe58yYRxEQEF45       282   \n",
       "2         6                                1110578710648890000       372   \n",
       "3         6  UgwUPFScjJ0MCeaP2F54AaABAg.8lvp3fc9Euf8lvvgsUgEgV       769   \n",
       "4         6  UgwWKCWtSJdFvjGHvTp4AaABAg.8kUC5dGrQ2H8kUDRihE2f3      1206   \n",
       "...     ...                                                ...       ...   \n",
       "3857      6                                1152219467579100000  10000695   \n",
       "3858      6                                1085362296472430000  10007008   \n",
       "3859      6        UghFY3QJ6nmT_ngCoAEC.7-H0Z7--wxd8goqpaPs-bl  20000102   \n",
       "3860      6                         UgyWabsmmnq3zam4DgZ4AaABAg  20000418   \n",
       "3861      6                         UgwPOHIDyICm10k0Mvx4AaABAg  20001003   \n",
       "\n",
       "      Genre  topiccode  Platform  ...  dislikeCount_video likeCount_video  \\\n",
       "0         0          0         1  ...                 NaN             NaN   \n",
       "1         1          2         1  ...               195.0          3817.0   \n",
       "2         2          4         2  ...                 NaN             NaN   \n",
       "3         0          0         1  ...                 NaN             NaN   \n",
       "4         0          0         1  ...                 NaN             NaN   \n",
       "...     ...        ...       ...  ...                 ...             ...   \n",
       "3857      0          4         2  ...                 NaN             NaN   \n",
       "3858      1          4         2  ...                 NaN             NaN   \n",
       "3859      0          3         1  ...              2820.0         12475.0   \n",
       "3860      2          3         1  ...               118.0         31761.0   \n",
       "3861      0          3         1  ...              1358.0          5740.0   \n",
       "\n",
       "      date_difference  commentCount_video  replyCount_comment    topic  \\\n",
       "0                 NaN                 NaN                 NaN      NaN   \n",
       "1               743.0              1748.0                 NaN  economy   \n",
       "2                 NaN                 NaN                 NaN      NaN   \n",
       "3                 NaN                 NaN                 NaN      NaN   \n",
       "4                 NaN                 NaN                 NaN      NaN   \n",
       "...               ...                 ...                 ...      ...   \n",
       "3857              NaN                 NaN                 NaN      NaN   \n",
       "3858              NaN                 NaN                 NaN      NaN   \n",
       "3859           3803.0              4785.0                 NaN     east   \n",
       "3860           1531.0              2206.0                 0.0     east   \n",
       "3861           2276.0              2887.0                 1.0     east   \n",
       "\n",
       "      subscribers  HATELIST_FOCUSED_DUMMY  Time_comment_year Time_video_year  \n",
       "0             NaN                       0               2017          2017.0  \n",
       "1       3630000.0                       0               2019          2019.0  \n",
       "2             NaN                       0               2019             NaN  \n",
       "3             NaN                       0               2018          2018.0  \n",
       "4             NaN                       0               2018          2018.0  \n",
       "...           ...                     ...                ...             ...  \n",
       "3857          NaN                       0               2019             NaN  \n",
       "3858          NaN                       0               2019             NaN  \n",
       "3859    6740000.0                       0               2018          2010.0  \n",
       "3860    6800000.0                       0               2018          2015.0  \n",
       "3861     549000.0                       0               2018          2018.0  \n",
       "\n",
       "[3862 rows x 79 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset: pd.DataFrame = pd.read_csv('data/publicsphere/full_data.csv')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first just try pre-processing and a simple tf-ivf:\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(texts):\n",
    "    x_train = []\n",
    "    for sent in tqdm.tqdm(texts):\n",
    "        sent = re.sub(r'@[^ ]+', '', sent)  #remove all usernames\n",
    "        sent = re.sub(r'https?://[^ ]+', '', sent) #remove all hyperlinks\n",
    "        sent = re.sub(r'#', '', sent) #remove all hashtags\n",
    "        sent = re.sub(r'([A-Za-z])\\1{2,}', r'\\1', sent) #normalize language use by replacing duplicate letters by single letters\n",
    "        sent = re.sub(\"[^a-zA-Z ]\", \"\", sent) #remove all non-words\n",
    "        sent = sent.lower().split()\n",
    "        sent = [lemmatizer.lemmatize(word) for word in sent if word not in set(stop_words)]\n",
    "        sent = ' '.join(sent)\n",
    "        x_train.append(sent)\n",
    "    return x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3862/3862 [00:05<00:00, 654.85it/s] \n"
     ]
    }
   ],
   "source": [
    "X = preprocess(dataset[\"commentText\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(max_features=5000, analyzer='word', ngram_range=(1,2), stop_words='english')\n",
    "X_tfidf = tfidf.fit_transform(X).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tfidf[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['tfidf_embedding'] = [torch.tensor(X_tfidf[i], dtype=torch.float32).flatten().tolist() for i in range(X_tfidf.shape[0])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['tfidf_embedding']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 1.        , 0.        , ..., 0.0110684 , 0.06654491,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 1.        , ..., 0.08009685, 0.        ,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.        , 0.0110684 , 0.08009685, ..., 1.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.06654491, 0.        , ..., 0.        , 1.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        1.        ]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarities = cosine_similarity(X_tfidf)\n",
    "cosine_similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_documents = len(cosine_similarities)\n",
    "\n",
    "# Initialize a list to store the most similar document pairs\n",
    "top_similar_pairs = []\n",
    "\n",
    "# Iterate through all document pairs\n",
    "for i in range(num_documents):\n",
    "    for j in range(i + 1, num_documents):\n",
    "        similarity = cosine_similarities[i][j]\n",
    "        \n",
    "        # Check if the similarity is NaN or zero\n",
    "        if not np.isnan(similarity) and similarity >= 1:\n",
    "            pair = (i, j)\n",
    "            top_similar_pairs.append((similarity, pair))\n",
    "            \n",
    "# Sort the list based on similarity in descending order\n",
    "top_similar_pairs.sort(key=lambda x: x[0], reverse=True)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1.0000000000000002, (139, 1990)),\n",
       " (1.0000000000000002, (246, 1179)),\n",
       " (1.0000000000000002, (362, 736)),\n",
       " (1.0000000000000002, (722, 739)),\n",
       " (1.0, (21, 1856)),\n",
       " (1.0, (75, 366)),\n",
       " (1.0, (75, 1022)),\n",
       " (1.0, (75, 2122)),\n",
       " (1.0, (75, 2421)),\n",
       " (1.0, (75, 2946)),\n",
       " (1.0, (101, 850)),\n",
       " (1.0, (133, 1057)),\n",
       " (1.0, (141, 3838)),\n",
       " (1.0, (164, 1240)),\n",
       " (1.0, (164, 2182)),\n",
       " (1.0, (198, 3213)),\n",
       " (1.0, (242, 2779)),\n",
       " (1.0, (242, 3278)),\n",
       " (1.0, (259, 490)),\n",
       " (1.0, (313, 2607)),\n",
       " (1.0, (341, 352)),\n",
       " (1.0, (366, 1022)),\n",
       " (1.0, (366, 2122)),\n",
       " (1.0, (366, 2421)),\n",
       " (1.0, (366, 2946)),\n",
       " (1.0, (395, 3340)),\n",
       " (1.0, (400, 585)),\n",
       " (1.0, (400, 877)),\n",
       " (1.0, (400, 962)),\n",
       " (1.0, (400, 2099)),\n",
       " (1.0, (400, 2848)),\n",
       " (1.0, (400, 3752)),\n",
       " (1.0, (436, 3211)),\n",
       " (1.0, (487, 936)),\n",
       " (1.0, (487, 2484)),\n",
       " (1.0, (546, 911)),\n",
       " (1.0, (559, 3708)),\n",
       " (1.0, (585, 877)),\n",
       " (1.0, (585, 962)),\n",
       " (1.0, (585, 2099)),\n",
       " (1.0, (585, 2848)),\n",
       " (1.0, (585, 3752)),\n",
       " (1.0, (643, 1401)),\n",
       " (1.0, (673, 1743)),\n",
       " (1.0, (678, 2244)),\n",
       " (1.0, (678, 2481)),\n",
       " (1.0, (678, 2843)),\n",
       " (1.0, (678, 2856)),\n",
       " (1.0, (678, 2971)),\n",
       " (1.0, (702, 1392)),\n",
       " (1.0, (709, 1800)),\n",
       " (1.0, (753, 930)),\n",
       " (1.0, (753, 1564)),\n",
       " (1.0, (753, 1887)),\n",
       " (1.0, (753, 2728)),\n",
       " (1.0, (793, 2935)),\n",
       " (1.0, (813, 1872)),\n",
       " (1.0, (816, 1088)),\n",
       " (1.0, (877, 962)),\n",
       " (1.0, (877, 2099)),\n",
       " (1.0, (877, 2848)),\n",
       " (1.0, (877, 3752)),\n",
       " (1.0, (894, 1060)),\n",
       " (1.0, (914, 2707)),\n",
       " (1.0, (914, 2914)),\n",
       " (1.0, (919, 3578)),\n",
       " (1.0, (930, 1564)),\n",
       " (1.0, (930, 1887)),\n",
       " (1.0, (930, 2728)),\n",
       " (1.0, (936, 2484)),\n",
       " (1.0, (937, 1005)),\n",
       " (1.0, (937, 1192)),\n",
       " (1.0, (955, 1404)),\n",
       " (1.0, (962, 2099)),\n",
       " (1.0, (962, 2848)),\n",
       " (1.0, (962, 3752)),\n",
       " (1.0, (969, 1196)),\n",
       " (1.0, (969, 1449)),\n",
       " (1.0, (969, 1690)),\n",
       " (1.0, (969, 2245)),\n",
       " (1.0, (969, 2591)),\n",
       " (1.0, (969, 2708)),\n",
       " (1.0, (969, 2840)),\n",
       " (1.0, (969, 3368)),\n",
       " (1.0, (969, 3405)),\n",
       " (1.0, (969, 3703)),\n",
       " (1.0, (1005, 1192)),\n",
       " (1.0, (1022, 2122)),\n",
       " (1.0, (1022, 2421)),\n",
       " (1.0, (1022, 2946)),\n",
       " (1.0, (1028, 1311)),\n",
       " (1.0, (1029, 2658)),\n",
       " (1.0, (1030, 2518)),\n",
       " (1.0, (1030, 2740)),\n",
       " (1.0, (1049, 2407)),\n",
       " (1.0, (1055, 1348)),\n",
       " (1.0, (1084, 1244)),\n",
       " (1.0, (1084, 1262)),\n",
       " (1.0, (1145, 2380)),\n",
       " (1.0, (1166, 1304)),\n",
       " (1.0, (1166, 1626)),\n",
       " (1.0, (1166, 1918)),\n",
       " (1.0, (1166, 2634)),\n",
       " (1.0, (1166, 2835)),\n",
       " (1.0, (1166, 3725)),\n",
       " (1.0, (1174, 3564)),\n",
       " (1.0, (1196, 1449)),\n",
       " (1.0, (1196, 1690)),\n",
       " (1.0, (1196, 2245)),\n",
       " (1.0, (1196, 2591)),\n",
       " (1.0, (1196, 2708)),\n",
       " (1.0, (1196, 2840)),\n",
       " (1.0, (1196, 3368)),\n",
       " (1.0, (1196, 3405)),\n",
       " (1.0, (1196, 3703)),\n",
       " (1.0, (1197, 1560)),\n",
       " (1.0, (1206, 3432)),\n",
       " (1.0, (1240, 2182)),\n",
       " (1.0, (1244, 1262)),\n",
       " (1.0, (1260, 3628)),\n",
       " (1.0, (1304, 1626)),\n",
       " (1.0, (1304, 1918)),\n",
       " (1.0, (1304, 2634)),\n",
       " (1.0, (1304, 2835)),\n",
       " (1.0, (1304, 3725)),\n",
       " (1.0, (1449, 1690)),\n",
       " (1.0, (1449, 2245)),\n",
       " (1.0, (1449, 2591)),\n",
       " (1.0, (1449, 2708)),\n",
       " (1.0, (1449, 2840)),\n",
       " (1.0, (1449, 3368)),\n",
       " (1.0, (1449, 3405)),\n",
       " (1.0, (1449, 3703)),\n",
       " (1.0, (1564, 1887)),\n",
       " (1.0, (1564, 2728)),\n",
       " (1.0, (1626, 1918)),\n",
       " (1.0, (1626, 2634)),\n",
       " (1.0, (1626, 2835)),\n",
       " (1.0, (1626, 3725)),\n",
       " (1.0, (1674, 1903)),\n",
       " (1.0, (1690, 2245)),\n",
       " (1.0, (1690, 2591)),\n",
       " (1.0, (1690, 2708)),\n",
       " (1.0, (1690, 2840)),\n",
       " (1.0, (1690, 3368)),\n",
       " (1.0, (1690, 3405)),\n",
       " (1.0, (1690, 3703)),\n",
       " (1.0, (1695, 2082)),\n",
       " (1.0, (1887, 2728)),\n",
       " (1.0, (1918, 2634)),\n",
       " (1.0, (1918, 2835)),\n",
       " (1.0, (1918, 3725)),\n",
       " (1.0, (2060, 2908)),\n",
       " (1.0, (2090, 2233)),\n",
       " (1.0, (2099, 2848)),\n",
       " (1.0, (2099, 3752)),\n",
       " (1.0, (2122, 2421)),\n",
       " (1.0, (2122, 2946)),\n",
       " (1.0, (2222, 3697)),\n",
       " (1.0, (2244, 2481)),\n",
       " (1.0, (2244, 2843)),\n",
       " (1.0, (2244, 2856)),\n",
       " (1.0, (2244, 2971)),\n",
       " (1.0, (2245, 2591)),\n",
       " (1.0, (2245, 2708)),\n",
       " (1.0, (2245, 2840)),\n",
       " (1.0, (2245, 3368)),\n",
       " (1.0, (2245, 3405)),\n",
       " (1.0, (2245, 3703)),\n",
       " (1.0, (2421, 2946)),\n",
       " (1.0, (2481, 2843)),\n",
       " (1.0, (2481, 2856)),\n",
       " (1.0, (2481, 2971)),\n",
       " (1.0, (2518, 2740)),\n",
       " (1.0, (2591, 2708)),\n",
       " (1.0, (2591, 2840)),\n",
       " (1.0, (2591, 3368)),\n",
       " (1.0, (2591, 3405)),\n",
       " (1.0, (2591, 3703)),\n",
       " (1.0, (2634, 2835)),\n",
       " (1.0, (2634, 3725)),\n",
       " (1.0, (2707, 2914)),\n",
       " (1.0, (2708, 2840)),\n",
       " (1.0, (2708, 3368)),\n",
       " (1.0, (2708, 3405)),\n",
       " (1.0, (2708, 3703)),\n",
       " (1.0, (2779, 3278)),\n",
       " (1.0, (2835, 3725)),\n",
       " (1.0, (2840, 3368)),\n",
       " (1.0, (2840, 3405)),\n",
       " (1.0, (2840, 3703)),\n",
       " (1.0, (2843, 2856)),\n",
       " (1.0, (2843, 2971)),\n",
       " (1.0, (2848, 3752)),\n",
       " (1.0, (2856, 2971)),\n",
       " (1.0, (2972, 3289)),\n",
       " (1.0, (3235, 3475)),\n",
       " (1.0, (3235, 3766)),\n",
       " (1.0, (3368, 3405)),\n",
       " (1.0, (3368, 3703)),\n",
       " (1.0, (3405, 3703)),\n",
       " (1.0, (3475, 3766)),\n",
       " (1.0, (3537, 3572))]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_similar_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(139, 1990), (246, 1179), (362, 736), (722, 739), (21, 1856), (75, 366), (75, 1022), (75, 2122), (75, 2421), (75, 2946), (101, 850), (133, 1057), (141, 3838), (164, 1240), (164, 2182), (198, 3213), (242, 2779), (242, 3278), (259, 490), (313, 2607), (341, 352), (366, 1022), (366, 2122), (366, 2421), (366, 2946), (395, 3340), (400, 585), (400, 877), (400, 962), (400, 2099), (400, 2848), (400, 3752), (436, 3211), (487, 936), (487, 2484), (546, 911), (559, 3708), (585, 877), (585, 962), (585, 2099), (585, 2848), (585, 3752), (643, 1401), (673, 1743), (678, 2244), (678, 2481), (678, 2843), (678, 2856), (678, 2971), (702, 1392), (709, 1800), (753, 930), (753, 1564), (753, 1887), (753, 2728), (793, 2935), (813, 1872), (816, 1088), (877, 962), (877, 2099), (877, 2848), (877, 3752), (894, 1060), (914, 2707), (914, 2914), (919, 3578), (930, 1564), (930, 1887), (930, 2728), (936, 2484), (937, 1005), (937, 1192), (955, 1404), (962, 2099), (962, 2848), (962, 3752), (969, 1196), (969, 1449), (969, 1690), (969, 2245), (969, 2591), (969, 2708), (969, 2840), (969, 3368), (969, 3405), (969, 3703), (1005, 1192), (1022, 2122), (1022, 2421), (1022, 2946), (1028, 1311), (1029, 2658), (1030, 2518), (1030, 2740), (1049, 2407), (1055, 1348), (1084, 1244), (1084, 1262), (1145, 2380), (1166, 1304), (1166, 1626), (1166, 1918), (1166, 2634), (1166, 2835), (1166, 3725), (1174, 3564), (1196, 1449), (1196, 1690), (1196, 2245), (1196, 2591), (1196, 2708), (1196, 2840), (1196, 3368), (1196, 3405), (1196, 3703), (1197, 1560), (1206, 3432), (1240, 2182), (1244, 1262), (1260, 3628), (1304, 1626), (1304, 1918), (1304, 2634), (1304, 2835), (1304, 3725), (1449, 1690), (1449, 2245), (1449, 2591), (1449, 2708), (1449, 2840), (1449, 3368), (1449, 3405), (1449, 3703), (1564, 1887), (1564, 2728), (1626, 1918), (1626, 2634), (1626, 2835), (1626, 3725), (1674, 1903), (1690, 2245), (1690, 2591), (1690, 2708), (1690, 2840), (1690, 3368), (1690, 3405), (1690, 3703), (1695, 2082), (1887, 2728), (1918, 2634), (1918, 2835), (1918, 3725), (2060, 2908), (2090, 2233), (2099, 2848), (2099, 3752), (2122, 2421), (2122, 2946), (2222, 3697), (2244, 2481), (2244, 2843), (2244, 2856), (2244, 2971), (2245, 2591), (2245, 2708), (2245, 2840), (2245, 3368), (2245, 3405), (2245, 3703), (2421, 2946), (2481, 2843), (2481, 2856), (2481, 2971), (2518, 2740), (2591, 2708), (2591, 2840), (2591, 3368), (2591, 3405), (2591, 3703), (2634, 2835), (2634, 3725), (2707, 2914), (2708, 2840), (2708, 3368), (2708, 3405), (2708, 3703), (2779, 3278), (2835, 3725), (2840, 3368), (2840, 3405), (2840, 3703), (2843, 2856), (2843, 2971), (2848, 3752), (2856, 2971), (2972, 3289), (3235, 3475), (3235, 3766), (3368, 3405), (3368, 3703), (3405, 3703), (3475, 3766), (3537, 3572)]\n"
     ]
    }
   ],
   "source": [
    "result = [pair for value, pair in top_similar_pairs if value >= 1]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "139          vote trump\n",
      "1990    Vote Trump 2020\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "246                        hell yeah fam\n",
      "1179    @TheDailyShow Hells to the Yeah.\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "362                @James Persinger \\n apple and orange.\n",
      "736    @hardball @MSNBC Just a few apples and oranges...\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "722             @hardball @KenSalazar Good grief!\n",
      "739    @NBCNews My opinion......lol.  Good grief.\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "21                     Mika is low IQ\n",
      "1856    @Rusty you still very low IQ.\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "75                                               I agree\n",
      "366    Kai Watson So you agree that Haiti is a Shithole?\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "75                                 I agree\n",
      "1022    @RealTimers @SteveSchmidtSES Agree\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "75                 I agree\n",
      "2122    Rtb boone i agree.\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "75                  I agree\n",
      "2421    Braulio V.  I agree\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "75                            I agree\n",
      "2946    at least we can agree on that\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "101                      Starts at 39:20\n",
      "850    @MeetThePress Can't start til Jan\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "133                                    Go Bernie 2016!\n",
      "1057    @FullFrontalSamB @Ilhan It's BERNIE, OKUURRRT!\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "141     Enough is enough, Diane. shut the fudge up!\n",
      "3838                       \"Woops\". \"Shut it down\".\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "164                             Exactly\n",
      "1240    @LateNightSeth ???????? exactly\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "164       Exactly\n",
      "2182    EXACTLY ?\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "198     this is so heartbreaking :(\n",
      "3213    This is so heartbreaking...\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "242                  Napoleon had help\n",
      "2779    You nedd help!\\nSeroius help!!\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "242     Napoleon had help\n",
      "3278                 Help\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "259    1st\n",
      "490    1st\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "313                      The US executes Traitors.\n",
      "2607    Libby was a traitor pardoned by a traitor.\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "341     SAVAGEEEEE\n",
      "352    Savage bill\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "366     Kai Watson So you agree that Haiti is a Shithole?\n",
      "1022                   @RealTimers @SteveSchmidtSES Agree\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "366     Kai Watson So you agree that Haiti is a Shithole?\n",
      "2122                                   Rtb boone i agree.\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "366     Kai Watson So you agree that Haiti is a Shithole?\n",
      "2421                                  Braulio V.  I agree\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "366     Kai Watson So you agree that Haiti is a Shithole?\n",
      "2946                        at least we can agree on that\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "395     @killalthedon21Â  get a life\n",
      "3340            Dion9646 get a life!\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "400    lineflyer1 Rasict trump stan ?\n",
      "585                       Trump 2020.\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "400    lineflyer1 Rasict trump stan ?\n",
      "877     @60Minutes Do Trump next Amal\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "400                       lineflyer1 Rasict trump stan ?\n",
      "962    @FaceTheNation @RepAdamSchiff Why is Trump des...\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "400                        lineflyer1 Rasict trump stan ?\n",
      "2099    \"Scientifically, not just words.\"\\n\\nPresident...\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "400     lineflyer1 Rasict trump stan ?\n",
      "2848                        Trump 2020\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "400     lineflyer1 Rasict trump stan ?\n",
      "3752       Why is Trump so cringy? >_<\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "436     @FullFrontalSamB @Huck_a_bot so very boring......\n",
      "3211                                         Too funny!!!\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "487                                    Who is this guy\n",
      "936    @TuckerCarlson @FoxNews These guys are awsome..\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "487           Who is this guy\n",
      "2484    You guys are so salty\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "546           This is no surprise at all...\n",
      "911    @FaceTheNation @RoyBlunt Surprise!!!\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "559                             Truth\n",
      "3708    Otravez 39 .....Friggin truth\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "585                      Trump 2020.\n",
      "877    @60Minutes Do Trump next Amal\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "585                                          Trump 2020.\n",
      "962    @FaceTheNation @RepAdamSchiff Why is Trump des...\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "585                                           Trump 2020.\n",
      "2099    \"Scientifically, not just words.\"\\n\\nPresident...\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "585     Trump 2020.\n",
      "2848     Trump 2020\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "585                     Trump 2020.\n",
      "3752    Why is Trump so cringy? >_<\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "643                                         learn engwish\n",
      "1401    @CBSEveningNews @CBSLARachel Something everyon...\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "673                            stupid ass\n",
      "1743    He is such a stupid unfunny prick\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "678               yes twoset\n",
      "2244    Yess Tervor is back♥\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "678     yes twoset\n",
      "2481           Yes\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "678                          yes twoset\n",
      "2843    @Richard Skipper Yes, that too.\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "678                    yes twoset\n",
      "2856    @Pat Marcy  - Yes I have.\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "678                           yes twoset\n",
      "2971    kimberly s Yes, that one. Blech.\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "702     @FullFrontalSamB @SmithDryGoods @kristencheeks...\n",
      "1392                              @ABCWorldNews Awesome ?\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "709     @FullFrontalSamB @Ilhan No thanks\n",
      "1800            Jennifer Webb Thanks Sis?\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "753    @11thHour @MSNBC @Eugene_Robinson Idiot\n",
      "930      @hardball @MSNBC @HardballChris Idiot\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "753     @11thHour @MSNBC @Eugene_Robinson Idiot\n",
      "1564                               Hes an idiot\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "753     @11thHour @MSNBC @Eugene_Robinson Idiot\n",
      "1887                                      Idiot\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "753     @11thHour @MSNBC @Eugene_Robinson Idiot\n",
      "2728                                     IDIOTS\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "793     @TheDailyShow Yeeep\n",
      "2935                YEP !!!\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "813           @TuckerCarlson Will never happen\n",
      "1872    BowelMovement...won’t happen, dipshit.\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "816     @ABCWorldNews Wow\n",
      "1088         @NBCNews Wow\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "877                        @60Minutes Do Trump next Amal\n",
      "962    @FaceTheNation @RepAdamSchiff Why is Trump des...\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "877                         @60Minutes Do Trump next Amal\n",
      "2099    \"Scientifically, not just words.\"\\n\\nPresident...\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "877     @60Minutes Do Trump next Amal\n",
      "2848                       Trump 2020\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "877     @60Minutes Do Trump next Amal\n",
      "3752      Why is Trump so cringy? >_<\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "894     @NBCNews That’s, Pierre Delecto (doofus), to you!\n",
      "1060    @colbertlateshow That’s what you get for being...\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "914     @11thHour @MSNBC MAGA 2020 https://t.co/fO5yme...\n",
      "2707                                                #MAGA\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "914     @11thHour @MSNBC MAGA 2020 https://t.co/fO5yme...\n",
      "2914                                                 MAGA\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "919         @FullFrontalSamB Need as ringtone\n",
      "3578    BRAH, BRAH..\\n\\n\\nI need u in my lief\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "930     @hardball @MSNBC @HardballChris Idiot\n",
      "1564                             Hes an idiot\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "930     @hardball @MSNBC @HardballChris Idiot\n",
      "1887                                    Idiot\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "930     @hardball @MSNBC @HardballChris Idiot\n",
      "2728                                   IDIOTS\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "936     @TuckerCarlson @FoxNews These guys are awsome..\n",
      "2484                              You guys are so salty\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "937     @LateNightSeth @Lesdoggg Loved this @LateNight...\n",
      "1005                          @LateNightSeth Loved this!!\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "937     @LateNightSeth @Lesdoggg Loved this @LateNight...\n",
      "1192                           @60Minutes I LOVED THIS!!!\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "955     @Nightline @byronpitts He’s the one ❤️ #Beto20...\n",
      "1404    @Nightline @markwayne143 @byronpitts #Beto2020...\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "962     @FaceTheNation @RepAdamSchiff Why is Trump des...\n",
      "2099    \"Scientifically, not just words.\"\\n\\nPresident...\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "962     @FaceTheNation @RepAdamSchiff Why is Trump des...\n",
      "2848                                           Trump 2020\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "962     @FaceTheNation @RepAdamSchiff Why is Trump des...\n",
      "3752                          Why is Trump so cringy? >_<\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "969       @LateNightSeth Lol!!\n",
      "1196    @colbertlateshow Lol ?\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "969            @LateNightSeth Lol!!\n",
      "1449    “And neither have you!” LOL\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "969     @LateNightSeth Lol!!\n",
      "1690        Roni Sanjaya lol\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "969     @LateNightSeth Lol!!\n",
      "2245                Same lol\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "969     @LateNightSeth Lol!!\n",
      "2591                     LoL\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "969     @LateNightSeth Lol!!\n",
      "2708                     lol\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "969     @LateNightSeth Lol!!\n",
      "2840                     Lol\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "969     @LateNightSeth Lol!!\n",
      "3368                     lol\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "969       @LateNightSeth Lol!!\n",
      "3405    @SuperBullaMan \\nLol ?\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "969     @LateNightSeth Lol!!\n",
      "3703                     lol\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "1005    @LateNightSeth Loved this!!\n",
      "1192     @60Minutes I LOVED THIS!!!\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "1022    @RealTimers @SteveSchmidtSES Agree\n",
      "2122                    Rtb boone i agree.\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "1022    @RealTimers @SteveSchmidtSES Agree\n",
      "2421                   Braulio V.  I agree\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "1022    @RealTimers @SteveSchmidtSES Agree\n",
      "2946         at least we can agree on that\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "1028    @TuckerCarlson @mikeroweworks Why? Serious que...\n",
      "1311     @ABCWorldNews How about the same question to you\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "1029    @patriotact @hahaaaamid @odotr22 lmaoooooo lan...\n",
      "2658                                                 Lmao\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "1030    @AC360 @brianstelter Nobody cares.\n",
      "2518                     Cares about WHAT!\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "1030    @AC360 @brianstelter Nobody cares.\n",
      "2740    Riddle me this. Why should I care?\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "1049    @hardball @MSNBC @NicolleDWallace Losers unite\n",
      "2407        WHAT ELSE IS NEW???\\nTHE DOTARD IS A LOSER\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "1055    @hardball @ericswalwell Ps... you Don’t no Any...\n",
      "1348             @NewsHour @nytdavidbrooks No they don’t.\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "1084    @LastWeekTonight This is the best one. Also #r...\n",
      "1244    @FullFrontalSamB @halcyonperson @at_howard @Ma...\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "1084    @LastWeekTonight This is the best one. Also #r...\n",
      "1262              @RealTimers @billmaher You are the best\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "1145    @LastWeekTonight brilliant!\n",
      "2380                    Brilliant ?\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "1166       @colbertlateshow Love me some #JoltinJoe\n",
      "1304    @LateNightSeth I Love You @sethmeyers ? !!!\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "1166    @colbertlateshow Love me some #JoltinJoe\n",
      "1626                                  Love this.\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "1166    @colbertlateshow Love me some #JoltinJoe\n",
      "1918                                   Love her!\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "1166    @colbertlateshow Love me some #JoltinJoe\n",
      "2634                           I love this show!\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "1166    @colbertlateshow Love me some #JoltinJoe\n",
      "2835        I love ?that name....New Bethlehem !\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "1166    @colbertlateshow Love me some #JoltinJoe\n",
      "3725                   I LOVE YOUUUUUUUUUUUUUUUU\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "1174    @RealTimers @WhitfordBradley @cbellantoni @Rea...\n",
      "3564     I  dont  think  we  will  ......................\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "1196         @colbertlateshow Lol ?\n",
      "1449    “And neither have you!” LOL\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "1196    @colbertlateshow Lol ?\n",
      "1690          Roni Sanjaya lol\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "1196    @colbertlateshow Lol ?\n",
      "2245                  Same lol\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "1196    @colbertlateshow Lol ?\n",
      "2591                       LoL\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "1196    @colbertlateshow Lol ?\n",
      "2708                       lol\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "1196    @colbertlateshow Lol ?\n",
      "2840                       Lol\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "1196    @colbertlateshow Lol ?\n",
      "3368                       lol\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "1196    @colbertlateshow Lol ?\n",
      "3405    @SuperBullaMan \\nLol ?\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "1196    @colbertlateshow Lol ?\n",
      "3703                       lol\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "1197     @AC360 GREAT INTERVIEW\n",
      "1560    What a great interview!\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "1206    @AC360 @andersoncooper Perfect.\n",
      "3432                            Perfect\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "1240    @LateNightSeth ???????? exactly\n",
      "2182                          EXACTLY ?\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "1244    @FullFrontalSamB @halcyonperson @at_howard @Ma...\n",
      "1262              @RealTimers @billmaher You are the best\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "1260    @CBSEveningNews @whyfund @weijia He’s sunk and...\n",
      "3628                  +Pasu suel do you know this icon ??\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "1304    @LateNightSeth I Love You @sethmeyers ? !!!\n",
      "1626                                     Love this.\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "1304    @LateNightSeth I Love You @sethmeyers ? !!!\n",
      "1918                                      Love her!\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "1304    @LateNightSeth I Love You @sethmeyers ? !!!\n",
      "2634                              I love this show!\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "1304    @LateNightSeth I Love You @sethmeyers ? !!!\n",
      "2835           I love ?that name....New Bethlehem !\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "1304    @LateNightSeth I Love You @sethmeyers ? !!!\n",
      "3725                      I LOVE YOUUUUUUUUUUUUUUUU\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "1449    “And neither have you!” LOL\n",
      "1690               Roni Sanjaya lol\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "1449    “And neither have you!” LOL\n",
      "2245                       Same lol\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "1449    “And neither have you!” LOL\n",
      "2591                            LoL\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "1449    “And neither have you!” LOL\n",
      "2708                            lol\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "1449    “And neither have you!” LOL\n",
      "2840                            Lol\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "1449    “And neither have you!” LOL\n",
      "3368                            lol\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "1449    “And neither have you!” LOL\n",
      "3405         @SuperBullaMan \\nLol ?\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "1449    “And neither have you!” LOL\n",
      "3703                            lol\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "1564    Hes an idiot\n",
      "1887           Idiot\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "1564    Hes an idiot\n",
      "2728          IDIOTS\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "1626    Love this.\n",
      "1918     Love her!\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "1626           Love this.\n",
      "2634    I love this show!\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "1626                              Love this.\n",
      "2835    I love ?that name....New Bethlehem !\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "1626                   Love this.\n",
      "3725    I LOVE YOUUUUUUUUUUUUUUUU\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "1674    Kira Barsmith I did. The whole thing too.\n",
      "1903                        you can be two things\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "1690    Roni Sanjaya lol\n",
      "2245            Same lol\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "1690    Roni Sanjaya lol\n",
      "2591                 LoL\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "1690    Roni Sanjaya lol\n",
      "2708                 lol\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "1690    Roni Sanjaya lol\n",
      "2840                 Lol\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "1690    Roni Sanjaya lol\n",
      "3368                 lol\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "1690          Roni Sanjaya lol\n",
      "3405    @SuperBullaMan \\nLol ?\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "1690    Roni Sanjaya lol\n",
      "3703                 lol\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "1695    Let all the hamiltrash Congregate ?\n",
      "2082                      Grrrrr, let's go!\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "1887     Idiot\n",
      "2728    IDIOTS\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "1918            Love her!\n",
      "2634    I love this show!\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "1918                               Love her!\n",
      "2835    I love ?that name....New Bethlehem !\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "1918                    Love her!\n",
      "3725    I LOVE YOUUUUUUUUUUUUUUUU\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "2060    Tiring leftist dogma\n",
      "2908         Leftist assh*le\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "2090    You are fake news\n",
      "2233       Very fake news\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "2099    \"Scientifically, not just words.\"\\n\\nPresident...\n",
      "2848                                           Trump 2020\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "2099    \"Scientifically, not just words.\"\\n\\nPresident...\n",
      "3752                          Why is Trump so cringy? >_<\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "2122     Rtb boone i agree.\n",
      "2421    Braulio V.  I agree\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "2122               Rtb boone i agree.\n",
      "2946    at least we can agree on that\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "2222                   Fucking clowns,\n",
      "3697    The bloke's a fucking clown ….\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "2244    Yess Tervor is back♥\n",
      "2481                     Yes\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "2244               Yess Tervor is back♥\n",
      "2843    @Richard Skipper Yes, that too.\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "2244         Yess Tervor is back♥\n",
      "2856    @Pat Marcy  - Yes I have.\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "2244                Yess Tervor is back♥\n",
      "2971    kimberly s Yes, that one. Blech.\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "2245    Same lol\n",
      "2591         LoL\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "2245    Same lol\n",
      "2708         lol\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "2245    Same lol\n",
      "2840         Lol\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "2245    Same lol\n",
      "3368         lol\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "2245                  Same lol\n",
      "3405    @SuperBullaMan \\nLol ?\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "2245    Same lol\n",
      "3703         lol\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "2421              Braulio V.  I agree\n",
      "2946    at least we can agree on that\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "2481                                Yes\n",
      "2843    @Richard Skipper Yes, that too.\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "2481                          Yes\n",
      "2856    @Pat Marcy  - Yes I have.\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "2481                                 Yes\n",
      "2971    kimberly s Yes, that one. Blech.\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "2518                     Cares about WHAT!\n",
      "2740    Riddle me this. Why should I care?\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "2591    LoL\n",
      "2708    lol\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "2591    LoL\n",
      "2840    Lol\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "2591    LoL\n",
      "3368    lol\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "2591                       LoL\n",
      "3405    @SuperBullaMan \\nLol ?\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "2591    LoL\n",
      "3703    lol\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "2634                       I love this show!\n",
      "2835    I love ?that name....New Bethlehem !\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "2634            I love this show!\n",
      "3725    I LOVE YOUUUUUUUUUUUUUUUU\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "2707    #MAGA\n",
      "2914     MAGA\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "2708    lol\n",
      "2840    Lol\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "2708    lol\n",
      "3368    lol\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "2708                       lol\n",
      "3405    @SuperBullaMan \\nLol ?\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "2708    lol\n",
      "3703    lol\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "2779    You nedd help!\\nSeroius help!!\n",
      "3278                              Help\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "2835    I love ?that name....New Bethlehem !\n",
      "3725               I LOVE YOUUUUUUUUUUUUUUUU\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "2840    Lol\n",
      "3368    lol\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "2840                       Lol\n",
      "3405    @SuperBullaMan \\nLol ?\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "2840    Lol\n",
      "3703    lol\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "2843    @Richard Skipper Yes, that too.\n",
      "2856          @Pat Marcy  - Yes I have.\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "2843     @Richard Skipper Yes, that too.\n",
      "2971    kimberly s Yes, that one. Blech.\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "2848                     Trump 2020\n",
      "3752    Why is Trump so cringy? >_<\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "2856           @Pat Marcy  - Yes I have.\n",
      "2971    kimberly s Yes, that one. Blech.\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "2972    Suck it !!!!!!!!!\n",
      "3289             You suck\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "3235           lock hem up\n",
      "3475    lock  them all  up\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "3235            lock hem up\n",
      "3766    AND LOCK HER UP!!!1\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "3368                       lol\n",
      "3405    @SuperBullaMan \\nLol ?\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "3368    lol\n",
      "3703    lol\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "3405    @SuperBullaMan \\nLol ?\n",
      "3703                       lol\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "3475     lock  them all  up\n",
      "3766    AND LOCK HER UP!!!1\n",
      "Name: commentText, dtype: object\n",
      "\n",
      "3537    Liberals are incorrigible\n",
      "3572                still liberal\n",
      "Name: commentText, dtype: object\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for first, second in result:\n",
    "    print(dataset[\"commentText\"][[first, second]])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hell yeah fam\n",
      "hell yeah\n",
      "vote trump\n",
      "vote trump\n",
      "mika low iq\n",
      "still low iq\n",
      "good grief\n",
      "opinionlol good grief\n",
      "persinger apple orange\n",
      "apple orange\n"
     ]
    }
   ],
   "source": [
    "print(X[246])\n",
    "print(X[1179])\n",
    "print(X[139])\n",
    "print(X[1990])\n",
    "print(X[21])\n",
    "print(X[1856])\n",
    "print(X[722])\n",
    "print(X[739])\n",
    "print(X[362])\n",
    "print(X[736])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[74], line 16\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m      8\u001b[0m     (model_1, model_2) \u001b[38;5;129;01min\u001b[39;00m resultsplatform\u001b[38;5;241m.\u001b[39mkeys() \u001b[38;5;129;01mor\u001b[39;00m \n\u001b[1;32m      9\u001b[0m     (model_2, model_1) \u001b[38;5;129;01min\u001b[39;00m resultsplatform\u001b[38;5;241m.\u001b[39mkeys()\n\u001b[1;32m     10\u001b[0m ):\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m     13\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m([\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28msum\u001b[39m(dist(\n\u001b[1;32m     15\u001b[0m         torch\u001b[38;5;241m.\u001b[39mtensor(np\u001b[38;5;241m.\u001b[39marray(v_1)), \n\u001b[0;32m---> 16\u001b[0m         torch\u001b[38;5;241m.\u001b[39mtensor(np\u001b[38;5;241m.\u001b[39marray(c_2\u001b[38;5;241m.\u001b[39mtolist()))\n\u001b[1;32m     17\u001b[0m         )) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(c_2)\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m v_1 \u001b[38;5;129;01min\u001b[39;00m c_1\n\u001b[1;32m     19\u001b[0m ]) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(c_1)\n\u001b[1;32m     21\u001b[0m resultsplatform[(model_1, model_2)] \u001b[38;5;241m=\u001b[39m res\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_1\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_2\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mres\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "grouped_data = dataset.groupby(\"Platform\")\n",
    "dist = torch.nn.PairwiseDistance()\n",
    "resultsplatform: typing.Dict[typing.Tuple[str, str], float] = {}\n",
    "for model_1, c_1 in tqdm.tqdm(grouped_data['tfidf_embedding'], total=grouped_data.ngroups):\n",
    "    for model_2, c_2 in tqdm.tqdm(grouped_data['tfidf_embedding'], total=grouped_data.ngroups):\n",
    "\n",
    "        if (\n",
    "            (model_1, model_2) in resultsplatform.keys() or \n",
    "            (model_2, model_1) in resultsplatform.keys()\n",
    "        ):\n",
    "            continue\n",
    "\n",
    "        res = sum([\n",
    "            sum(dist(\n",
    "                torch.tensor(np.array(v_1)), \n",
    "                torch.tensor(np.array(c_2.tolist()))\n",
    "                )) / len(c_2)\n",
    "            for v_1 in c_1\n",
    "        ]) / len(c_1)\n",
    "\n",
    "        resultsplatform[(model_1, model_2)] = res\n",
    "\n",
    "        print(f'{model_1}:{model_2}:{res.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_data = dataset.groupby(\"topiccode\")\n",
    "dist = torch.nn.PairwiseDistance()\n",
    "resultstopic: typing.Dict[typing.Tuple[str, str], float] = {}\n",
    "for model_1, c_1 in tqdm.tqdm(grouped_data['tfidf_embedding'], total=grouped_data.ngroups):\n",
    "    for model_2, c_2 in tqdm.tqdm(grouped_data['tfidf_embedding'], total=grouped_data.ngroups):\n",
    "\n",
    "        if (\n",
    "            (model_1, model_2) in resultstopic.keys() or \n",
    "            (model_2, model_1) in resultstopic.keys()\n",
    "        ):\n",
    "            continue\n",
    "\n",
    "        res = sum([\n",
    "            sum(dist(\n",
    "                torch.tensor(np.array(v_1)), \n",
    "                torch.tensor(np.array(c_2.tolist()))\n",
    "                )) / len(c_2)\n",
    "            for v_1 in c_1\n",
    "        ]) / len(c_1)\n",
    "\n",
    "        resultstopic[(model_1, model_2)] = res\n",
    "\n",
    "        print(f'{model_1}:{model_2}:{res.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_data = dataset.groupby(\"Genre\")\n",
    "dist = torch.nn.PairwiseDistance()\n",
    "resultsgenre: typing.Dict[typing.Tuple[str, str], float] = {}\n",
    "for model_1, c_1 in tqdm.tqdm(grouped_data['tfidf_embedding'], total=grouped_data.ngroups):\n",
    "    for model_2, c_2 in tqdm.tqdm(grouped_data['tfidf_embedding'], total=grouped_data.ngroups):\n",
    "\n",
    "        if (\n",
    "            (model_1, model_2) in resultsgenre.keys() or \n",
    "            (model_2, model_1) in resultsgenre.keys()\n",
    "        ):\n",
    "            continue\n",
    "\n",
    "        res = sum([\n",
    "            sum(dist(\n",
    "                torch.tensor(np.array(v_1)), \n",
    "                torch.tensor(np.array(c_2.tolist()))\n",
    "                )) / len(c_2)\n",
    "            for v_1 in c_1\n",
    "        ]) / len(c_1)\n",
    "\n",
    "        resultsgenre[(model_1, model_2)] = res\n",
    "\n",
    "        print(f'{model_1}:{model_2}:{res.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL: str = 'mixtral:8x7b-instruct-v0.1-q6_K' # options: 'gemma:7b-instruct-q6_K', 'gemma2:27b-instruct-q6_K', 'llama3.1:8b-instruct-q6_K', 'llama3.1:70b-instruct-q6_K', 'mistral:7b-instruct-v0.3-q6_K', 'mistral-large:123b-instruct-2407-q6_K', 'mixtral:8x7b-instruct-v0.1-q6_K', 'mixtral:8x22b-instruct-v0.1-q6_K', 'phi3:14b-medium-128k-instruct-q6_K' or 'qwen2:72b-instruct-q6_K'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_MXBAI: typing.Dict[str, np.ndarray] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 123/3862 [04:58<2:31:24,  2.43s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index, row \u001b[38;5;129;01min\u001b[39;00m tqdm\u001b[38;5;241m.\u001b[39mtqdm(dataset[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcommentText\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mitems(), total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(dataset)):\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m: \n\u001b[0;32m----> 4\u001b[0m         embed \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(requests\u001b[38;5;241m.\u001b[39mpost(\n\u001b[1;32m      5\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://inf.cl.uni-trier.de/embed/\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      6\u001b[0m             json\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m: MODEL, \n\u001b[1;32m      7\u001b[0m                   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mYou help me get embeddings for a sentence. I provide you a with a context and a sentence and you reply only with that exact sentence. Context = \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m context \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m; Sentence: \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m row}\n\u001b[1;32m      8\u001b[0m             )\u001b[38;5;241m.\u001b[39mjson()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m _e:\n\u001b[1;32m     10\u001b[0m         logging\u001b[38;5;241m.\u001b[39mwarning(_e)\n",
      "File \u001b[0;32m/Users/SjoerdStolwijk/anaconda3/envs/DefiningDebateQuality/lib/python3.12/site-packages/requests/api.py:115\u001b[0m, in \u001b[0;36mpost\u001b[0;34m(url, data, json, **kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(url, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, json\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    104\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a POST request.\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \n\u001b[1;32m    106\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m request(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url, data\u001b[38;5;241m=\u001b[39mdata, json\u001b[38;5;241m=\u001b[39mjson, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/Users/SjoerdStolwijk/anaconda3/envs/DefiningDebateQuality/lib/python3.12/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m session\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/Users/SjoerdStolwijk/anaconda3/envs/DefiningDebateQuality/lib/python3.12/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(prep, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msend_kwargs)\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m/Users/SjoerdStolwijk/anaconda3/envs/DefiningDebateQuality/lib/python3.12/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m adapter\u001b[38;5;241m.\u001b[39msend(request, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m/Users/SjoerdStolwijk/anaconda3/envs/DefiningDebateQuality/lib/python3.12/site-packages/requests/adapters.py:589\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    586\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    588\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 589\u001b[0m     resp \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39murlopen(\n\u001b[1;32m    590\u001b[0m         method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[1;32m    591\u001b[0m         url\u001b[38;5;241m=\u001b[39murl,\n\u001b[1;32m    592\u001b[0m         body\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mbody,\n\u001b[1;32m    593\u001b[0m         headers\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m    594\u001b[0m         redirect\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    595\u001b[0m         assert_same_host\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    596\u001b[0m         preload_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    597\u001b[0m         decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    598\u001b[0m         retries\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_retries,\n\u001b[1;32m    599\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[1;32m    600\u001b[0m         chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[1;32m    601\u001b[0m     )\n\u001b[1;32m    603\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    604\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[0;32m/Users/SjoerdStolwijk/anaconda3/envs/DefiningDebateQuality/lib/python3.12/site-packages/urllib3/connectionpool.py:789\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    786\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    788\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 789\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(\n\u001b[1;32m    790\u001b[0m     conn,\n\u001b[1;32m    791\u001b[0m     method,\n\u001b[1;32m    792\u001b[0m     url,\n\u001b[1;32m    793\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout_obj,\n\u001b[1;32m    794\u001b[0m     body\u001b[38;5;241m=\u001b[39mbody,\n\u001b[1;32m    795\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[1;32m    796\u001b[0m     chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[1;32m    797\u001b[0m     retries\u001b[38;5;241m=\u001b[39mretries,\n\u001b[1;32m    798\u001b[0m     response_conn\u001b[38;5;241m=\u001b[39mresponse_conn,\n\u001b[1;32m    799\u001b[0m     preload_content\u001b[38;5;241m=\u001b[39mpreload_content,\n\u001b[1;32m    800\u001b[0m     decode_content\u001b[38;5;241m=\u001b[39mdecode_content,\n\u001b[1;32m    801\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kw,\n\u001b[1;32m    802\u001b[0m )\n\u001b[1;32m    804\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[1;32m    805\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/Users/SjoerdStolwijk/anaconda3/envs/DefiningDebateQuality/lib/python3.12/site-packages/urllib3/connectionpool.py:536\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    534\u001b[0m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[1;32m    535\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 536\u001b[0m     response \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39mgetresponse()\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    538\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[0;32m/Users/SjoerdStolwijk/anaconda3/envs/DefiningDebateQuality/lib/python3.12/site-packages/urllib3/connection.py:464\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mresponse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HTTPResponse\n\u001b[1;32m    463\u001b[0m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[0;32m--> 464\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mgetresponse()\n\u001b[1;32m    466\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    467\u001b[0m     assert_header_parsing(httplib_response\u001b[38;5;241m.\u001b[39mmsg)\n",
      "File \u001b[0;32m/Users/SjoerdStolwijk/anaconda3/envs/DefiningDebateQuality/lib/python3.12/http/client.py:1428\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1426\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1427\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1428\u001b[0m         response\u001b[38;5;241m.\u001b[39mbegin()\n\u001b[1;32m   1429\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[1;32m   1430\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/Users/SjoerdStolwijk/anaconda3/envs/DefiningDebateQuality/lib/python3.12/http/client.py:331\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 331\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_read_status()\n\u001b[1;32m    332\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[1;32m    333\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/Users/SjoerdStolwijk/anaconda3/envs/DefiningDebateQuality/lib/python3.12/http/client.py:292\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 292\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp\u001b[38;5;241m.\u001b[39mreadline(_MAXLINE \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[1;32m    294\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/Users/SjoerdStolwijk/anaconda3/envs/DefiningDebateQuality/lib/python3.12/socket.py:708\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    706\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 708\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39mrecv_into(b)\n\u001b[1;32m    709\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    710\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/Users/SjoerdStolwijk/anaconda3/envs/DefiningDebateQuality/lib/python3.12/ssl.py:1252\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1248\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1249\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1250\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1251\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1252\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread(nbytes, buffer)\n\u001b[1;32m   1253\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1254\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m/Users/SjoerdStolwijk/anaconda3/envs/DefiningDebateQuality/lib/python3.12/ssl.py:1104\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1102\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1103\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1104\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[1;32m   1105\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1106\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "context = 'social media replies to a news- or infotainment-post'\n",
    "for index, row in tqdm.tqdm(dataset[\"commentText\"].items(), total=len(dataset)):\n",
    "    try: \n",
    "        embed = np.array(requests.post(\n",
    "            'https://inf.cl.uni-trier.de/embed/',\n",
    "            json={'model': MODEL, \n",
    "                  'prompt': 'You help me get embeddings for a sentence. I provide you with a context and a sentence and you reply only with that exact sentence. Context = ' + context + '; Sentence: ' + row}\n",
    "            ).json()[\"response\"])\n",
    "    except Exception as _e:\n",
    "        logging.warning(_e)\n",
    "        embed = None\n",
    "    \n",
    "    embed_MXBAI[index] = embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_w_embeds = dataset.join(pd.Series(embed_MXBAI, name=\"embed_MXBAI\"))\n",
    "dataset_w_embeds.to_parquet(f'{CFG.report_dir}/pubsphere.MXBAIembeds.parquet')\n",
    "dataset_w_embeds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:3: SyntaxWarning: invalid escape sequence '\\p'\n",
      "<>:3: SyntaxWarning: invalid escape sequence '\\p'\n",
      "C:\\Users\\sstolwi\\AppData\\Local\\Temp\\ipykernel_13408\\795071030.py:3: SyntaxWarning: invalid escape sequence '\\p'\n",
      "  dataset_prompt = pd.read_parquet(f'{CFG.report_dir}\\publicsphere.cardiff_prompt_classify_s.parquet')\n"
     ]
    }
   ],
   "source": [
    "#load datasets:\n",
    "dataset_w_embeds = pd.read_parquet(f'{CFG.report_dir}/pubsphere.MXBAIembeds.parquet')\n",
    "dataset_prompt = pd.read_parquet(f'{CFG.report_dir}\\publicsphere.cardiff_prompt_classify_s.parquet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create BERT-base embeddings for each word:\n",
    "\n",
    "model_name = \"google-bert/bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create Twhin-BERT-base embeddings for each word:\n",
    "model_name = \"Twitter/twhin-bert-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/SjoerdStolwijk/anaconda3/envs/DefiningDebateQuality/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at voidism/diffcse-roberta-base-sts were not used when initializing RobertaModel: ['aux_bert.embeddings.LayerNorm.bias', 'aux_bert.embeddings.LayerNorm.weight', 'aux_bert.embeddings.position_embeddings.weight', 'aux_bert.embeddings.position_ids', 'aux_bert.embeddings.token_type_embeddings.weight', 'aux_bert.embeddings.word_embeddings.weight', 'aux_bert.encoder.layer.0.attention.output.LayerNorm.bias', 'aux_bert.encoder.layer.0.attention.output.LayerNorm.weight', 'aux_bert.encoder.layer.0.attention.output.dense.bias', 'aux_bert.encoder.layer.0.attention.output.dense.weight', 'aux_bert.encoder.layer.0.attention.self.key.bias', 'aux_bert.encoder.layer.0.attention.self.key.weight', 'aux_bert.encoder.layer.0.attention.self.query.bias', 'aux_bert.encoder.layer.0.attention.self.query.weight', 'aux_bert.encoder.layer.0.attention.self.value.bias', 'aux_bert.encoder.layer.0.attention.self.value.weight', 'aux_bert.encoder.layer.0.intermediate.dense.bias', 'aux_bert.encoder.layer.0.intermediate.dense.weight', 'aux_bert.encoder.layer.0.output.LayerNorm.bias', 'aux_bert.encoder.layer.0.output.LayerNorm.weight', 'aux_bert.encoder.layer.0.output.dense.bias', 'aux_bert.encoder.layer.0.output.dense.weight', 'aux_bert.encoder.layer.1.attention.output.LayerNorm.bias', 'aux_bert.encoder.layer.1.attention.output.LayerNorm.weight', 'aux_bert.encoder.layer.1.attention.output.dense.bias', 'aux_bert.encoder.layer.1.attention.output.dense.weight', 'aux_bert.encoder.layer.1.attention.self.key.bias', 'aux_bert.encoder.layer.1.attention.self.key.weight', 'aux_bert.encoder.layer.1.attention.self.query.bias', 'aux_bert.encoder.layer.1.attention.self.query.weight', 'aux_bert.encoder.layer.1.attention.self.value.bias', 'aux_bert.encoder.layer.1.attention.self.value.weight', 'aux_bert.encoder.layer.1.intermediate.dense.bias', 'aux_bert.encoder.layer.1.intermediate.dense.weight', 'aux_bert.encoder.layer.1.output.LayerNorm.bias', 'aux_bert.encoder.layer.1.output.LayerNorm.weight', 'aux_bert.encoder.layer.1.output.dense.bias', 'aux_bert.encoder.layer.1.output.dense.weight', 'aux_bert.encoder.layer.10.attention.output.LayerNorm.bias', 'aux_bert.encoder.layer.10.attention.output.LayerNorm.weight', 'aux_bert.encoder.layer.10.attention.output.dense.bias', 'aux_bert.encoder.layer.10.attention.output.dense.weight', 'aux_bert.encoder.layer.10.attention.self.key.bias', 'aux_bert.encoder.layer.10.attention.self.key.weight', 'aux_bert.encoder.layer.10.attention.self.query.bias', 'aux_bert.encoder.layer.10.attention.self.query.weight', 'aux_bert.encoder.layer.10.attention.self.value.bias', 'aux_bert.encoder.layer.10.attention.self.value.weight', 'aux_bert.encoder.layer.10.intermediate.dense.bias', 'aux_bert.encoder.layer.10.intermediate.dense.weight', 'aux_bert.encoder.layer.10.output.LayerNorm.bias', 'aux_bert.encoder.layer.10.output.LayerNorm.weight', 'aux_bert.encoder.layer.10.output.dense.bias', 'aux_bert.encoder.layer.10.output.dense.weight', 'aux_bert.encoder.layer.11.attention.output.LayerNorm.bias', 'aux_bert.encoder.layer.11.attention.output.LayerNorm.weight', 'aux_bert.encoder.layer.11.attention.output.dense.bias', 'aux_bert.encoder.layer.11.attention.output.dense.weight', 'aux_bert.encoder.layer.11.attention.self.key.bias', 'aux_bert.encoder.layer.11.attention.self.key.weight', 'aux_bert.encoder.layer.11.attention.self.query.bias', 'aux_bert.encoder.layer.11.attention.self.query.weight', 'aux_bert.encoder.layer.11.attention.self.value.bias', 'aux_bert.encoder.layer.11.attention.self.value.weight', 'aux_bert.encoder.layer.11.intermediate.dense.bias', 'aux_bert.encoder.layer.11.intermediate.dense.weight', 'aux_bert.encoder.layer.11.output.LayerNorm.bias', 'aux_bert.encoder.layer.11.output.LayerNorm.weight', 'aux_bert.encoder.layer.11.output.dense.bias', 'aux_bert.encoder.layer.11.output.dense.weight', 'aux_bert.encoder.layer.2.attention.output.LayerNorm.bias', 'aux_bert.encoder.layer.2.attention.output.LayerNorm.weight', 'aux_bert.encoder.layer.2.attention.output.dense.bias', 'aux_bert.encoder.layer.2.attention.output.dense.weight', 'aux_bert.encoder.layer.2.attention.self.key.bias', 'aux_bert.encoder.layer.2.attention.self.key.weight', 'aux_bert.encoder.layer.2.attention.self.query.bias', 'aux_bert.encoder.layer.2.attention.self.query.weight', 'aux_bert.encoder.layer.2.attention.self.value.bias', 'aux_bert.encoder.layer.2.attention.self.value.weight', 'aux_bert.encoder.layer.2.intermediate.dense.bias', 'aux_bert.encoder.layer.2.intermediate.dense.weight', 'aux_bert.encoder.layer.2.output.LayerNorm.bias', 'aux_bert.encoder.layer.2.output.LayerNorm.weight', 'aux_bert.encoder.layer.2.output.dense.bias', 'aux_bert.encoder.layer.2.output.dense.weight', 'aux_bert.encoder.layer.3.attention.output.LayerNorm.bias', 'aux_bert.encoder.layer.3.attention.output.LayerNorm.weight', 'aux_bert.encoder.layer.3.attention.output.dense.bias', 'aux_bert.encoder.layer.3.attention.output.dense.weight', 'aux_bert.encoder.layer.3.attention.self.key.bias', 'aux_bert.encoder.layer.3.attention.self.key.weight', 'aux_bert.encoder.layer.3.attention.self.query.bias', 'aux_bert.encoder.layer.3.attention.self.query.weight', 'aux_bert.encoder.layer.3.attention.self.value.bias', 'aux_bert.encoder.layer.3.attention.self.value.weight', 'aux_bert.encoder.layer.3.intermediate.dense.bias', 'aux_bert.encoder.layer.3.intermediate.dense.weight', 'aux_bert.encoder.layer.3.output.LayerNorm.bias', 'aux_bert.encoder.layer.3.output.LayerNorm.weight', 'aux_bert.encoder.layer.3.output.dense.bias', 'aux_bert.encoder.layer.3.output.dense.weight', 'aux_bert.encoder.layer.4.attention.output.LayerNorm.bias', 'aux_bert.encoder.layer.4.attention.output.LayerNorm.weight', 'aux_bert.encoder.layer.4.attention.output.dense.bias', 'aux_bert.encoder.layer.4.attention.output.dense.weight', 'aux_bert.encoder.layer.4.attention.self.key.bias', 'aux_bert.encoder.layer.4.attention.self.key.weight', 'aux_bert.encoder.layer.4.attention.self.query.bias', 'aux_bert.encoder.layer.4.attention.self.query.weight', 'aux_bert.encoder.layer.4.attention.self.value.bias', 'aux_bert.encoder.layer.4.attention.self.value.weight', 'aux_bert.encoder.layer.4.intermediate.dense.bias', 'aux_bert.encoder.layer.4.intermediate.dense.weight', 'aux_bert.encoder.layer.4.output.LayerNorm.bias', 'aux_bert.encoder.layer.4.output.LayerNorm.weight', 'aux_bert.encoder.layer.4.output.dense.bias', 'aux_bert.encoder.layer.4.output.dense.weight', 'aux_bert.encoder.layer.5.attention.output.LayerNorm.bias', 'aux_bert.encoder.layer.5.attention.output.LayerNorm.weight', 'aux_bert.encoder.layer.5.attention.output.dense.bias', 'aux_bert.encoder.layer.5.attention.output.dense.weight', 'aux_bert.encoder.layer.5.attention.self.key.bias', 'aux_bert.encoder.layer.5.attention.self.key.weight', 'aux_bert.encoder.layer.5.attention.self.query.bias', 'aux_bert.encoder.layer.5.attention.self.query.weight', 'aux_bert.encoder.layer.5.attention.self.value.bias', 'aux_bert.encoder.layer.5.attention.self.value.weight', 'aux_bert.encoder.layer.5.intermediate.dense.bias', 'aux_bert.encoder.layer.5.intermediate.dense.weight', 'aux_bert.encoder.layer.5.output.LayerNorm.bias', 'aux_bert.encoder.layer.5.output.LayerNorm.weight', 'aux_bert.encoder.layer.5.output.dense.bias', 'aux_bert.encoder.layer.5.output.dense.weight', 'aux_bert.encoder.layer.6.attention.output.LayerNorm.bias', 'aux_bert.encoder.layer.6.attention.output.LayerNorm.weight', 'aux_bert.encoder.layer.6.attention.output.dense.bias', 'aux_bert.encoder.layer.6.attention.output.dense.weight', 'aux_bert.encoder.layer.6.attention.self.key.bias', 'aux_bert.encoder.layer.6.attention.self.key.weight', 'aux_bert.encoder.layer.6.attention.self.query.bias', 'aux_bert.encoder.layer.6.attention.self.query.weight', 'aux_bert.encoder.layer.6.attention.self.value.bias', 'aux_bert.encoder.layer.6.attention.self.value.weight', 'aux_bert.encoder.layer.6.intermediate.dense.bias', 'aux_bert.encoder.layer.6.intermediate.dense.weight', 'aux_bert.encoder.layer.6.output.LayerNorm.bias', 'aux_bert.encoder.layer.6.output.LayerNorm.weight', 'aux_bert.encoder.layer.6.output.dense.bias', 'aux_bert.encoder.layer.6.output.dense.weight', 'aux_bert.encoder.layer.7.attention.output.LayerNorm.bias', 'aux_bert.encoder.layer.7.attention.output.LayerNorm.weight', 'aux_bert.encoder.layer.7.attention.output.dense.bias', 'aux_bert.encoder.layer.7.attention.output.dense.weight', 'aux_bert.encoder.layer.7.attention.self.key.bias', 'aux_bert.encoder.layer.7.attention.self.key.weight', 'aux_bert.encoder.layer.7.attention.self.query.bias', 'aux_bert.encoder.layer.7.attention.self.query.weight', 'aux_bert.encoder.layer.7.attention.self.value.bias', 'aux_bert.encoder.layer.7.attention.self.value.weight', 'aux_bert.encoder.layer.7.intermediate.dense.bias', 'aux_bert.encoder.layer.7.intermediate.dense.weight', 'aux_bert.encoder.layer.7.output.LayerNorm.bias', 'aux_bert.encoder.layer.7.output.LayerNorm.weight', 'aux_bert.encoder.layer.7.output.dense.bias', 'aux_bert.encoder.layer.7.output.dense.weight', 'aux_bert.encoder.layer.8.attention.output.LayerNorm.bias', 'aux_bert.encoder.layer.8.attention.output.LayerNorm.weight', 'aux_bert.encoder.layer.8.attention.output.dense.bias', 'aux_bert.encoder.layer.8.attention.output.dense.weight', 'aux_bert.encoder.layer.8.attention.self.key.bias', 'aux_bert.encoder.layer.8.attention.self.key.weight', 'aux_bert.encoder.layer.8.attention.self.query.bias', 'aux_bert.encoder.layer.8.attention.self.query.weight', 'aux_bert.encoder.layer.8.attention.self.value.bias', 'aux_bert.encoder.layer.8.attention.self.value.weight', 'aux_bert.encoder.layer.8.intermediate.dense.bias', 'aux_bert.encoder.layer.8.intermediate.dense.weight', 'aux_bert.encoder.layer.8.output.LayerNorm.bias', 'aux_bert.encoder.layer.8.output.LayerNorm.weight', 'aux_bert.encoder.layer.8.output.dense.bias', 'aux_bert.encoder.layer.8.output.dense.weight', 'aux_bert.encoder.layer.9.attention.output.LayerNorm.bias', 'aux_bert.encoder.layer.9.attention.output.LayerNorm.weight', 'aux_bert.encoder.layer.9.attention.output.dense.bias', 'aux_bert.encoder.layer.9.attention.output.dense.weight', 'aux_bert.encoder.layer.9.attention.self.key.bias', 'aux_bert.encoder.layer.9.attention.self.key.weight', 'aux_bert.encoder.layer.9.attention.self.query.bias', 'aux_bert.encoder.layer.9.attention.self.query.weight', 'aux_bert.encoder.layer.9.attention.self.value.bias', 'aux_bert.encoder.layer.9.attention.self.value.weight', 'aux_bert.encoder.layer.9.intermediate.dense.bias', 'aux_bert.encoder.layer.9.intermediate.dense.weight', 'aux_bert.encoder.layer.9.output.LayerNorm.bias', 'aux_bert.encoder.layer.9.output.LayerNorm.weight', 'aux_bert.encoder.layer.9.output.dense.bias', 'aux_bert.encoder.layer.9.output.dense.weight', 'generator.lm_head.bias', 'generator.lm_head.decoder.bias', 'generator.lm_head.decoder.weight', 'generator.lm_head.dense.bias', 'generator.lm_head.dense.weight', 'generator.lm_head.layer_norm.bias', 'generator.lm_head.layer_norm.weight', 'generator.roberta.embeddings.LayerNorm.bias', 'generator.roberta.embeddings.LayerNorm.weight', 'generator.roberta.embeddings.position_embeddings.weight', 'generator.roberta.embeddings.position_ids', 'generator.roberta.embeddings.token_type_embeddings.weight', 'generator.roberta.embeddings.word_embeddings.weight', 'generator.roberta.encoder.layer.0.attention.output.LayerNorm.bias', 'generator.roberta.encoder.layer.0.attention.output.LayerNorm.weight', 'generator.roberta.encoder.layer.0.attention.output.dense.bias', 'generator.roberta.encoder.layer.0.attention.output.dense.weight', 'generator.roberta.encoder.layer.0.attention.self.key.bias', 'generator.roberta.encoder.layer.0.attention.self.key.weight', 'generator.roberta.encoder.layer.0.attention.self.query.bias', 'generator.roberta.encoder.layer.0.attention.self.query.weight', 'generator.roberta.encoder.layer.0.attention.self.value.bias', 'generator.roberta.encoder.layer.0.attention.self.value.weight', 'generator.roberta.encoder.layer.0.intermediate.dense.bias', 'generator.roberta.encoder.layer.0.intermediate.dense.weight', 'generator.roberta.encoder.layer.0.output.LayerNorm.bias', 'generator.roberta.encoder.layer.0.output.LayerNorm.weight', 'generator.roberta.encoder.layer.0.output.dense.bias', 'generator.roberta.encoder.layer.0.output.dense.weight', 'generator.roberta.encoder.layer.1.attention.output.LayerNorm.bias', 'generator.roberta.encoder.layer.1.attention.output.LayerNorm.weight', 'generator.roberta.encoder.layer.1.attention.output.dense.bias', 'generator.roberta.encoder.layer.1.attention.output.dense.weight', 'generator.roberta.encoder.layer.1.attention.self.key.bias', 'generator.roberta.encoder.layer.1.attention.self.key.weight', 'generator.roberta.encoder.layer.1.attention.self.query.bias', 'generator.roberta.encoder.layer.1.attention.self.query.weight', 'generator.roberta.encoder.layer.1.attention.self.value.bias', 'generator.roberta.encoder.layer.1.attention.self.value.weight', 'generator.roberta.encoder.layer.1.intermediate.dense.bias', 'generator.roberta.encoder.layer.1.intermediate.dense.weight', 'generator.roberta.encoder.layer.1.output.LayerNorm.bias', 'generator.roberta.encoder.layer.1.output.LayerNorm.weight', 'generator.roberta.encoder.layer.1.output.dense.bias', 'generator.roberta.encoder.layer.1.output.dense.weight', 'generator.roberta.encoder.layer.2.attention.output.LayerNorm.bias', 'generator.roberta.encoder.layer.2.attention.output.LayerNorm.weight', 'generator.roberta.encoder.layer.2.attention.output.dense.bias', 'generator.roberta.encoder.layer.2.attention.output.dense.weight', 'generator.roberta.encoder.layer.2.attention.self.key.bias', 'generator.roberta.encoder.layer.2.attention.self.key.weight', 'generator.roberta.encoder.layer.2.attention.self.query.bias', 'generator.roberta.encoder.layer.2.attention.self.query.weight', 'generator.roberta.encoder.layer.2.attention.self.value.bias', 'generator.roberta.encoder.layer.2.attention.self.value.weight', 'generator.roberta.encoder.layer.2.intermediate.dense.bias', 'generator.roberta.encoder.layer.2.intermediate.dense.weight', 'generator.roberta.encoder.layer.2.output.LayerNorm.bias', 'generator.roberta.encoder.layer.2.output.LayerNorm.weight', 'generator.roberta.encoder.layer.2.output.dense.bias', 'generator.roberta.encoder.layer.2.output.dense.weight', 'generator.roberta.encoder.layer.3.attention.output.LayerNorm.bias', 'generator.roberta.encoder.layer.3.attention.output.LayerNorm.weight', 'generator.roberta.encoder.layer.3.attention.output.dense.bias', 'generator.roberta.encoder.layer.3.attention.output.dense.weight', 'generator.roberta.encoder.layer.3.attention.self.key.bias', 'generator.roberta.encoder.layer.3.attention.self.key.weight', 'generator.roberta.encoder.layer.3.attention.self.query.bias', 'generator.roberta.encoder.layer.3.attention.self.query.weight', 'generator.roberta.encoder.layer.3.attention.self.value.bias', 'generator.roberta.encoder.layer.3.attention.self.value.weight', 'generator.roberta.encoder.layer.3.intermediate.dense.bias', 'generator.roberta.encoder.layer.3.intermediate.dense.weight', 'generator.roberta.encoder.layer.3.output.LayerNorm.bias', 'generator.roberta.encoder.layer.3.output.LayerNorm.weight', 'generator.roberta.encoder.layer.3.output.dense.bias', 'generator.roberta.encoder.layer.3.output.dense.weight', 'generator.roberta.encoder.layer.4.attention.output.LayerNorm.bias', 'generator.roberta.encoder.layer.4.attention.output.LayerNorm.weight', 'generator.roberta.encoder.layer.4.attention.output.dense.bias', 'generator.roberta.encoder.layer.4.attention.output.dense.weight', 'generator.roberta.encoder.layer.4.attention.self.key.bias', 'generator.roberta.encoder.layer.4.attention.self.key.weight', 'generator.roberta.encoder.layer.4.attention.self.query.bias', 'generator.roberta.encoder.layer.4.attention.self.query.weight', 'generator.roberta.encoder.layer.4.attention.self.value.bias', 'generator.roberta.encoder.layer.4.attention.self.value.weight', 'generator.roberta.encoder.layer.4.intermediate.dense.bias', 'generator.roberta.encoder.layer.4.intermediate.dense.weight', 'generator.roberta.encoder.layer.4.output.LayerNorm.bias', 'generator.roberta.encoder.layer.4.output.LayerNorm.weight', 'generator.roberta.encoder.layer.4.output.dense.bias', 'generator.roberta.encoder.layer.4.output.dense.weight', 'generator.roberta.encoder.layer.5.attention.output.LayerNorm.bias', 'generator.roberta.encoder.layer.5.attention.output.LayerNorm.weight', 'generator.roberta.encoder.layer.5.attention.output.dense.bias', 'generator.roberta.encoder.layer.5.attention.output.dense.weight', 'generator.roberta.encoder.layer.5.attention.self.key.bias', 'generator.roberta.encoder.layer.5.attention.self.key.weight', 'generator.roberta.encoder.layer.5.attention.self.query.bias', 'generator.roberta.encoder.layer.5.attention.self.query.weight', 'generator.roberta.encoder.layer.5.attention.self.value.bias', 'generator.roberta.encoder.layer.5.attention.self.value.weight', 'generator.roberta.encoder.layer.5.intermediate.dense.bias', 'generator.roberta.encoder.layer.5.intermediate.dense.weight', 'generator.roberta.encoder.layer.5.output.LayerNorm.bias', 'generator.roberta.encoder.layer.5.output.LayerNorm.weight', 'generator.roberta.encoder.layer.5.output.dense.bias', 'generator.roberta.encoder.layer.5.output.dense.weight', 'lm_head.bias', 'lm_head.decoder.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'mlp.net.0.weight', 'mlp.net.1.bias', 'mlp.net.1.num_batches_tracked', 'mlp.net.1.running_mean', 'mlp.net.1.running_var', 'mlp.net.1.weight', 'mlp.net.3.weight', 'mlp.net.4.num_batches_tracked', 'mlp.net.4.running_mean', 'mlp.net.4.running_var']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at voidism/diffcse-roberta-base-sts and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "#calculate DiffCSE embeddings:\n",
    "model_name = \"voidism/diffcse-roberta-base-sts\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for encoding sentence embeddings based on the DiffCSE approach which is based on (and refers to) SimCSE/evaluation.py \n",
    "def encode_sentence(sentence):\n",
    "    input_ids = tokenizer.encode(sentence, add_special_tokens=True, truncation=True, max_length=512, padding='max_length')\n",
    "    input_ids_tensor = torch.tensor(input_ids).unsqueeze(0)  # Add batch dimension\n",
    "    attention_mask = torch.ones_like(input_ids_tensor)  # Creating attention mask \n",
    "    with torch.no_grad():\n",
    "            outputs = model(input_ids_tensor, output_hidden_states=True, return_dict=True, attention_mask=attention_mask)\n",
    "            sentence_embedding = outputs.last_hidden_state[:, 0].cpu()\n",
    "            return sentence_embedding.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for DiffCSE: Create embeddings for each argument and store them in a new column\n",
    "dataset.loc[:, 'DiffCSE_embedding'] = dataset[\"commentText\"].apply(encode_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#claim mine the publicsphere data\n",
    "#first add system prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_claim: str = \\\n",
    "    \"\"\"\n",
    "        Instruction:\n",
    "\n",
    "        You are a text annotation assitant. Analyze a collection of social media comments enclosed in chevrons <..>. Identify and list the claims within these comments. Claims can be related to events, issues, opinions or concerns in relation to the specified topic.\n",
    "        Claims are defined as the main assertion or conclusion of an argument.\n",
    "        You summarize each claim into a short simple sentence.\n",
    "\n",
    "        Response format:\n",
    "\n",
    "        You provide only the list of claims, separated by commas, without any additional text or explanations. If no claims can be identified, return an empty list [].\n",
    "\n",
    "        Response format template:\n",
    "        \n",
    "        [\"claim 1\", \"claim 2\", ... \"claim x\"]\n",
    "\t\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL: str = 'llama3.1:70b-instruct-q6_K'  # options: 'gemma:7b-instruct-q6_K', 'gemma2:27b-instruct-q6_K', 'llama3.1:8b-instruct-q6_K', 'llama3.1:70b-instruct-q6_K', 'mistral:7b-instruct-v0.3-q6_K', 'mistral-large:123b-instruct-2407-q6_K', 'mixtral:8x7b-instruct-v0.1-q6_K', 'mixtral:8x22b-instruct-v0.1-q6_K', 'phi3:14b-medium-128k-instruct-q6_K' or 'qwen2:72b-instruct-q6_K'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['StartDate', 'RecordedDate', 'IPAddress', 'Finished', 'Coder', 'ID',\n",
       "       'Mark_ID', 'Genre', 'topiccode', 'Platform', 'Anonymity',\n",
       "       'Anonymity_9_TEXT', 'codable', 'Interaction', 'Acknowledgement',\n",
       "       'TopicRelevance', 'Reasoning', 'BackgroundInfo', 'ExternalEvidence',\n",
       "       'ExternalEvidence_1_TEXT', 'Opinion', 'disagreement',\n",
       "       'Ideologicaldirection', 'Name_calling', 'Vulgarity',\n",
       "       'Attack_reputation', 'Question_Intelligenc', 'All_caps_function',\n",
       "       'Sarcasm_to_criticize', 'Individual_right', 'discrimination',\n",
       "       'Invoke_violence', 'Tone', 'INTERACTIVITY_DUMMY', 'RATIONALITY_DUMMY',\n",
       "       'HAS_OPINION_DUMMY', 'LIBERAL_NEUTRAL_CONSERVATIVE', 'LIBERAL_DUMMY',\n",
       "       'CONSERVATIVE_DUMMY', 'NAMECALLING_DUMMY', 'VULGAR_DUMMY',\n",
       "       'NAMECALLING_VULGAR_DUMMY', 'INCIVILITY_ORDINAL', 'INCIVILITY_DUMMY',\n",
       "       'INTOLERANCE_DUMMY', 'filter_$', 'IMPOLITENESS_DUMMY', 'commentText',\n",
       "       'showName', 'genre', 'Time_comment', 'likeCount_comment', 'entities',\n",
       "       'place', 'retweet_count', 'platform', 'retweeted', 'language', 'source',\n",
       "       'in_reply_to_status_id_str', 'in_reply_to_user_id_str',\n",
       "       'in_reply_to_screen_name', 'is_quote_status', 'videoTitle',\n",
       "       'description', 'Time_video', 'channelTitle', 'channelId', 'viewCount',\n",
       "       'dislikeCount_video', 'likeCount_video', 'date_difference',\n",
       "       'commentCount_video', 'replyCount_comment', 'topic', 'subscribers',\n",
       "       'HATELIST_FOCUSED_DUMMY', 'Time_comment_year', 'Time_video_year',\n",
       "       'tfidf_embedding', 'embed_MXP'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_w_embeds.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "topiccode staat voor:\n",
    "0 : YouTube general\n",
    "1 : YT Mueller/Comy investigation\n",
    "2 : YT: Economy\n",
    "3 : YT: Middle East\n",
    "4 : Twitter:general"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "topicdict = {0 : 'YTgen', \n",
    "1 : 'YT_investigation',\n",
    "2 : 'YT_Econ',\n",
    "3 : 'YT_ME',\n",
    "4 : 'Twitgen'}\n",
    "\n",
    "dataset_w_embeds.loc[:,'topiccodeSTR']= dataset_w_embeds.topiccode.map(topicdict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_prompt.loc[:,'topiccodeSTR']= dataset_prompt.topiccode.map(topicdict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "options_str = \"\"\"\n",
    "seed: 42\n",
    "temperature: 0.8\n",
    "num_predict: 1\n",
    "\"\"\"\n",
    "\n",
    "options = yaml.safe_load(options_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '196415a8-7027-11ef-8a8f-0d8e6fdd3d4b',\n",
       " 'timestamp': '2024-09-11T10:17:50.380071',\n",
       " 'model': 'llama3.1:70b-instruct-q6_K',\n",
       " 'prompt': [{'role': 'system',\n",
       "   'content': '\\n        Instruction:\\n\\n        You are a text annotation assitant. Analyze a collection of social media comments enclosed in chevrons <..>. Identify and list the claims within these comments. Claims can be related to events, issues, opinions or concerns in relation to the specified topic.\\n        Claims are defined as the main assertion or conclusion of an argument.\\n        You summarize each claim into a short simple sentence.\\n\\n        Response format:\\n\\n        You provide only the list of claims, separated by commas, without any additional text or explanations. If no claims can be identified, return an empty list [].\\n\\n        Response format template:\\n        \\n        [\"claim 1\", \"claim 2\", ... \"claim x\"]\\n\\t'},\n",
       "  {'role': 'user',\n",
       "   'content': 'The following set of social media posts are social media replies to a news- or infotainment-post. Check whether your answer only consists of a list of claims. \\n\"Posts\":\\n<[\\'sad\\', \"That\\'s a vicious insult!!! What did a box of rocks ever do to you that you would slander it like that? I represent the coalition for mineral rights. Minerals have rights to.\", \\'@colbertlateshow The question has always been if he was compromised. We never knew if he was aware of the trump tow… https://t.co/J7oTDPZcM9\\', \\'Goya Solidar.  So there are a few of us left.  Try reading the comments on both Fox News and The Daily Show clips.  The death of free and civil dialogue has never been painted so well.\\', \\'hello hello \\\\nNo-one else will hug him.\\']>'}],\n",
       " 'response': '[\"Minerals have rights\", \"The question has always been if he was compromised\", \"The death of free and civil dialogue has occurred\"]'}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "requests.post(\n",
    "                            'https://inf.cl.uni-trier.de/',\n",
    "                            json={\n",
    "                                'model': MODEL,\n",
    "                                'system': SYSTEM_claim,\n",
    "                                'prompt': f'The following set of social media posts are replies to a news- or infotainment-post. '\n",
    "                                        + f'Check whether your answer only consists of a list of claims. \\n\"Posts\":\\n<{dataset_w_embeds[\"commentText\"][:5].to_list()}>',\n",
    "                                'options': options\n",
    "                                }).json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3862it [2:05:28,  1.95s/it]\n"
     ]
    }
   ],
   "source": [
    "#get a list of claims per comment:\n",
    "chunked_result: typing.List[pd.DataFrame] = []\n",
    "for index, row in tqdm.tqdm(dataset_prompt.iterrows()):\n",
    "    try: \n",
    "        chunked_result.append(\n",
    "            pd.DataFrame(\n",
    "                data=[\n",
    "                    requests.post(\n",
    "                        'https://inf.cl.uni-trier.de/',\n",
    "                        json={\n",
    "                            'model': MODEL,\n",
    "                            'system': SYSTEM_claim,\n",
    "                            'prompt': f'The following set of social media posts are replies to a news- or infotainment-post. '\n",
    "                                    + f'Check whether your answer strictly adheres to the specified format. \\n\"Posts\":\\n<{row[\"commentText\"]}>',\n",
    "                            'options': options\n",
    "                            }).json()['response']                       \n",
    "                ],\n",
    "                columns=['claims']\n",
    "            )\n",
    "        )\n",
    "        \n",
    "    except json.JSONDecodeError:\n",
    "        print(\"invalid json response, skipping to next batch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:16, 16.55s/it]\n",
      "1it [00:02,  2.04s/it]\n",
      "3it [00:04,  1.40s/it]\n"
     ]
    }
   ],
   "source": [
    "#apply generalized claim mining to get a list of claims:\n",
    "chunked_result: typing.List[pd.DataFrame] = []\n",
    "for label, group in dataset_prompt[:5].groupby('topiccodeSTR'):\n",
    "    for index, row in tqdm.tqdm(group.iterrows()):\n",
    "        try: \n",
    "            chunked_result.append(\n",
    "                pd.DataFrame(\n",
    "                    data=[\n",
    "                        requests.post(\n",
    "                            'https://inf.cl.uni-trier.de/',\n",
    "                            json={\n",
    "                                'model': MODEL,\n",
    "                                'system': SYSTEM_claim,\n",
    "                                'prompt': f'The following set of social media posts are replies to a news- or infotainment-post. '\n",
    "                                        + f'Check whether your answer strictly adheres to the specified format. \\n\"Posts\":\\n<{row[\"commentText\"]}>'\n",
    "                                }).json()['response']                       \n",
    "                    ],\n",
    "                    columns=['claims']\n",
    "                )\n",
    "                .assign(label=label)\n",
    "            )\n",
    "            \n",
    "        except json.JSONDecodeError:\n",
    "            print(\"invalid json response, skipping to next batch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 claims\n",
      "0                                                    []\n",
      "1     [\"Minerals have rights too\", \"A box of rocks w...\n",
      "2     [\"There is a question about whether Trump was ...\n",
      "3     [\"The death of free and civil dialogue is evid...\n",
      "4                         [\"Nobody else will hug him.\"]\n",
      "...                                                 ...\n",
      "3857  [\"They can't afford chemical peels and facelif...\n",
      "3858                           [\"He will be confirmed\"]\n",
      "3859  [\"They knew all about the cameras.\", \"The susp...\n",
      "3860  [\"Troops are waiting in the fields\", \"Men will...\n",
      "3861  [\"Russia and the US are both against ISIS\", \"T...\n",
      "\n",
      "[3862 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "claim_df = pd.concat(chunked_result, ignore_index=True)\n",
    "print(claim_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[]',\n",
       " '[\"Minerals have rights too\", \"A box of rocks was slandered\"]',\n",
       " '[\"There is a question about whether Trump was compromised\", \"It is unknown whether Trump was aware of being compromised\"]',\n",
       " '[\"The death of free and civil dialogue is evident in online comments.\"]',\n",
       " '[\"Nobody else will hug him.\"]',\n",
       " '[\"The user will never be president\", \"The user is the daughter of Liawatha\"]',\n",
       " '[\"Trump is a traitor\"]',\n",
       " '[\"he would shoot someone on Fifth Avenue\"]',\n",
       " '[\"America is a country of idiots run by the NRA\", \"Guns are more valued than children\\'s lives in America\", \"The 2nd amendment should be abolished\", \"Getting rid of guns will save children\\'s lives\"]',\n",
       " '[\"They weren\\'t going to vote democratic anyway.\"]',\n",
       " '[\"Ivanka has high moral standards\"]',\n",
       " '[\"Hillary is a lesbian\"]',\n",
       " '[]',\n",
       " '[\"Admitting Russia back in is sensible\", \"Democrats have gone full neocon for the last three years\"]',\n",
       " '[\"You will lose the midterms\"]',\n",
       " '[\"A constitutional crisis is occurring\"]',\n",
       " '[\"DTJ thinks he is the best of the best\", \"DJT is a draft dodger who belittles real war heroes\", \"DJT is afraid of his own school records coming out\", \"DJT hides his tax documents\", \"Trump is not good at concocting real estate deals\", \"DJT skims millions from projects and lies on his taxes\"]',\n",
       " '[\"The poster agrees with someone or something\"]',\n",
       " '[\"Trump\\'s weekend trips to his resort cost taxpayers money\", \"Sen McCain and others should criticize Trump for his costly weekend trips\", \"The President can do work from the White House which is already paid for by taxes\"]',\n",
       " '[\"The young girl\\'s mother committed identity theft\", \"The young girl\\'s mother entered the country illegally\", \"The young girl is misguided\"]',\n",
       " '[\"The person in charge of the VA is not fit for the job\", \"The person lied about Trump\\'s height and weight\"]',\n",
       " '[\"Mika has a low IQ\"]',\n",
       " '[\"The President of the United States lacks emotional intelligence\", \"The President has no impulse control\"]',\n",
       " '[\"Gunner Eisenberg loves Hiroshima\"]',\n",
       " '[\"The Republican party is power-drunk\", \"A Democratic president would have faced impeachment hearings by now\", \"There are conflicts of interest with the current administration from day one\", \"There was Russian hacking involved\", \"NSA debauchery occurred\"]',\n",
       " '[\"Trump should be discarded and disrespected\"]',\n",
       " '[\"Mueller cast doubt on the accuracy of the Buzzfeed article\", \"Rudy Giuliani made a stupid comment\"]',\n",
       " '[\"The spying program started in the Bush era\", \"The spying program contributes to national security\", \"Obamacare has faults\", \"Obamacare\\'s merit is undeniable\"]',\n",
       " '[\"The author feels they are living in a surreal reality under Trump\\'s presidency\", \"The author believes their community has been taken over or is being run by someone referred to as \\'Orange Fatty\\'\"]',\n",
       " '[\"Democrats will not take any action\"]',\n",
       " '[\"Someone kidnapped people\\'s kids.\"]',\n",
       " '[\"Jared Kushner is like his father\"]',\n",
       " '[\"Kelly will be fired\", \"The government is being taken over by Communists\", \"Mueller is the only hope\"]',\n",
       " '[]',\n",
       " '[\"Zendaya is a great addition to the show\"]',\n",
       " '[\"The story takes place during Christmas\"]',\n",
       " '[\"A milkshake diet causes farts\"]',\n",
       " '[]',\n",
       " '[\"The country is likely to be in flames within the next 4 years\", \"People are desperate for the next 4 years to pass quickly\"]',\n",
       " '[\"Jonah Hill\\'s films are not propaganda\", \"Claiming Jonah Hill\\'s films are propaganda due to his past as a soldier is illogical\"]',\n",
       " '[\"Hillary Clinton was a bad presidential candidate\", \"Trump won the presidency because of Clinton\\'s candidacy\"]',\n",
       " '[]',\n",
       " '[\"Most students at top-end universities like Harvard attend for prestige rather than education.\"]',\n",
       " '[\"There will be a season finale.\"]',\n",
       " '[\"the US government no longer punishes treason with death\"]',\n",
       " '[\"The U.S. is overly dramatic\"]',\n",
       " '[\"Rambo 3 also contains something mentioned in the original post.\"]',\n",
       " '[\"Sneezing, coughing and vomiting are symptoms of the flu.\"]',\n",
       " '[\"Peanuts is better than balls\"]',\n",
       " '[\"Conspiracy theories can drive Republicans to madness\"]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[\"News about the monument collapse is fake\"]',\n",
       " '[\"She is a huge liar.\"]',\n",
       " '[\"The post \\'tha God\\' is an example of humour.\"]',\n",
       " '[]',\n",
       " '[\"Trump does not pay his contractors\", \"Trump paid off porn stars for unperformed work\"]',\n",
       " '[\"The person being referred to is extremely wealthy\", \"He can afford to take the rest of his life off\"]',\n",
       " '[\"the interview is boring\", \"Bernie was bumped for the interview\"]',\n",
       " '[]',\n",
       " '[\"You don\\'t know\"]',\n",
       " '[\"Hilary Clinton solicited money for the Clinton Foundation\", \"The source of fear-mongering is Reince Priebus, head of the RNC\"]',\n",
       " '[\"The person being referred to is adorable\", \"You can see her nerves as she sits in the spotlight\"]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[\"There should be more storytellers like her on late night\", \"Stephen enjoys her storytelling\"]',\n",
       " '[\"The person in question is a beautiful young man\", \"He will recover fully and soon\"]',\n",
       " '[\"Despair is a form of hybris\", \"Intelligence in humans and animals involves anticipating the future by understanding the past and presence\", \"Worrying about the future means caring for it\"]',\n",
       " '[]',\n",
       " '[\"She kept innocent people locked up for her ego.\"]',\n",
       " '[]',\n",
       " '[\"The person being referred to is nice\"]',\n",
       " '[\"Karma has been served\"]',\n",
       " '[\"Citizens of a certain country are fleeing and flooding Europe\"]',\n",
       " '[\"The user trusts Trevor\\'s opinion\", \"The user does not know who the person mentioned is\"]',\n",
       " '[]',\n",
       " '[\"Republicans are unaware or uninformed\", \"The Republican electorate will eventually lose patience\"]',\n",
       " '[\"Racists are motivated by terror\", \"Fear is the root cause of racism\"]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[\"North Korea, Arabian Muslims, and others want Trump to succeed\", \"Trump is being praised by these groups for the Nobel Prize\", \"Their support of Trump is a ploy to divide and destroy the USA\"]',\n",
       " '[\"the dress was actually blue and black\"]',\n",
       " '[\"He and his fans are trash.\"]',\n",
       " '[\"This comment is hateful.\"]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[\"Buzzfeed is a news outlet\", \"Buzzfeed posts humorous content\"]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[\"Trudeau is a savage\"]',\n",
       " '[\"People who get offended by comments from another race are likely racists\"]',\n",
       " '[\"An Albanian man claims he will not see something\"]',\n",
       " '[\"She meant facts that support their claim\"]',\n",
       " '[\"Putin made a statement in response to being joked about\"]',\n",
       " '[]',\n",
       " '[\"Dutch people are high in intelligence and height standings\", \"Trump voters have a survival instinct\", \"The Dutch electorate lacks a survival instinct\"]',\n",
       " '[\"Bodega babies is something noteworthy\"]',\n",
       " '[\"The person is being sarcastic about agreeing with Trump\", \"His new show is similar to The Colbert Report and uses sarcasm\", \"He is not being silenced because he is getting his own show\"]',\n",
       " '[\"It\\'s unnecessary to publicly share someone\\'s medical results\", \"The public sharing of medical information is shameful\"]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[\"Otto Warmbier will not be forgotten.\"]',\n",
       " '[\"She looks older than her age\", \"Being too skinny makes you look older after 50\"]',\n",
       " '[\"The woman\\'s attitude, mindset, and sense of fashion are admirable\", \"The woman dresses outside of traditional views of how older women should dress\"]',\n",
       " '[\"The author is being unfairly targeted by the deep state\", \"There has been a constant hatred towards Russia for 400 straight days\"]',\n",
       " '[\"Viewers should do more than just comment\", \"The creator should make a video or mirror an existing one from another channel\"]',\n",
       " '[\"Black Lives Matter is racist\", \"The commenter\\'s opinion on Black Lives Matter is always wrong\"]',\n",
       " '[]',\n",
       " '[\"Albert Einstein started a tradition in another country that people now follow\"]',\n",
       " '[\"Overhunting is one cause\", \"Pesticides are one cause\"]',\n",
       " '[\"Liberals will not win in 2020\", \"Liberal ideologies are anti-American, socialist and insane\", \"MSNBC is fake news\"]',\n",
       " '[\"Mexico will pay for a huge wall\"]',\n",
       " '[\"Murder should not have any consequences.\", \"Allowing murder would make America great.\"]',\n",
       " '[\"The writer believes in equal pay for equal work\", \"The writer thinks women should not expect special treatment just because they are female\"]',\n",
       " '[\"the statement is true\"]',\n",
       " '[\"No Russia collusion was found after 2 years and $40 million\"]',\n",
       " '[\"Colin Kaepernick does not stand for the national anthem\", \"He should not get paid in the USA\"]',\n",
       " '[\"The person in question is corrupt\", \"Veritas delivered incriminating information\"]',\n",
       " '[\"Russell O\\'Neal is very old\"]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[\"Dennis Vance\\'s list of presidents is incomplete\", \"George W. Bush should be included in Dennis Vance\\'s list\"]',\n",
       " '[\"Earth Day has already passed according to him\"]',\n",
       " '[\"This situation is insane.\"]',\n",
       " '[]',\n",
       " '[\"Trump has little command of the English language.\"]',\n",
       " '[\"There is a right time and place for everything\", \"Harry used to know when to act appropriately in front of the media during his party days\", \"William and Kate behave differently at home\"]',\n",
       " '[\"The Democrats are a waste of time\", \"Mr. Trump will win everything in 2020\"]',\n",
       " '[\"Democrats want Trump to end the shutdown\", \"Trump is responsible for the shutdown\"]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[\"The author thinks the previous commenter\\'s response was empty and lacking substance\"]',\n",
       " '[\"Bernie should be elected in 2016\"]',\n",
       " '[\"The conditions in Central America are a consequence of Reagan and Kissinger wars\", \"Thousands of people died due to the Iran Contras war\", \"Drug wars created gangs in Central America\", \"Climate change has obliterated land in Central America\", \"Asylum is legal for those fleeing violence and hunger\", \"The treatment of asylum seekers in the US is illegal under international law\"]',\n",
       " '[\"Mitch McConnell should retire\", \"Harry Reid is old\"]',\n",
       " '[\"Shaun Ellis should keep hoping\"]',\n",
       " '[\"Anyone aligned with Barack Obama is crooked\"]',\n",
       " '[\"Hezbollah cannot survive without Iran\\'s support\"]',\n",
       " '[\"Trump should be voted for\"]',\n",
       " '[]',\n",
       " '[Diane should stop speaking.]',\n",
       " '[]',\n",
       " '[\"The senator has great values\", \"We need more senators from the south with those values\"]',\n",
       " '[\"The person who broke the rules knew the rules beforehand\", \"Republicans tend to blame others when they don\\'t follow the rules\", \"Trump will likely blame Obama next\"]',\n",
       " '[\"Gun zealots are selfish\", \"Gun zealots have no evidence to support their claims\", \"Gun zealots are responsible for the deaths of thousands\"]',\n",
       " '[\"Tim Kaine will not vote for the person\", \"Kaine and Hillary lost an election\"]',\n",
       " '[]',\n",
       " '[\"There are Russian-speaking people in countries bordering Russia\", \"The situation is similar to that of Spanish speakers on the US-Mexico border\"]',\n",
       " '[\"The video claims that Iran is being set up for a violation of the deal\"]',\n",
       " '[\"Mueller should be fired\", \"Rosenstein should be fired\", \"Donald Trump will get impeached\"]',\n",
       " '[\"A sitting US President can only be impeached by Congress\", \"A sitting US President cannot be indicted\", \"Mueller and his team are acting like mobsters\"]',\n",
       " '[\"The person being replied to has not worked or provided a service for America\"]',\n",
       " '[\"Bernie is speaking reasonably\", \"Many people misunderstand reasonable speech\"]',\n",
       " '[\"Clinton\\'s campaign used propaganda\", \"Hillary Clinton does not have 3 million more votes than Bernie Sanders when including caucus votes\"]',\n",
       " '[\"The user will vote for Trump\"]',\n",
       " '[\"Republicans are Marxist liars\", \"Name calling and belittling labels makes one look small\", \"Extremists are doing harm to America\", \"The middle of politics is drivable while the extremes represent the gutter\"]',\n",
       " '[\"CBS lies to the nation on Sundays\"]',\n",
       " '[\"there is a huge gap between your legs\"]',\n",
       " '[]',\n",
       " '[\"There is an age limit of 5 years old in El Paso and 1 month old in Odessa for something, likely related to arms or gun control\"]',\n",
       " '[\"The user hates facts.\"]',\n",
       " '[\"The author thought Trump would behave well for the UN appearance\", \"Trump pulled out of P.A.\", \"The author will face their children and be asked why they sold out\", \"The author is getting good at covering like Spicer\", \"U.N. will say thanks but no thanks\"]',\n",
       " '[\"Facebook propaganda has no influence on people\", \"The US treats others poorly on a grand scale\", \"Russia can only influence US elections by hacking voting machines\", \"Facebook accounts cannot change election outcomes\"]',\n",
       " '[]',\n",
       " '[\"The author of the post is not taking parenting seriously\", \"Young people often do crazy things regardless of parental consent\"]',\n",
       " '[\"Women are not physically and mentally as strong as men\", \"Women cannot fit certain jobs as well as men\"]',\n",
       " '[\"White people are gullible and easily deceived\"]',\n",
       " '[\"People die every day\", \"Death is a normal part of life\"]',\n",
       " '[\"Obama allowed ICE Director John Morton to prohibit ICE officers from enforcing US immigration laws\", \"Obama provided amnesty to illegal immigrants\"]',\n",
       " '[\"A video titled \\'Face Transplant Recipient Meets Donor\\'s Mother\\' is available.\"]',\n",
       " '[]',\n",
       " '[\"A woman from The Little Couple TV show is assisting with a surgery\"]',\n",
       " '[\"The user listens to Carlos Santana\\'s CD every day\", \"Carlos Santana has the best performances ever\"]',\n",
       " '[\"He may have lost his sense of smell\", \"He may be unable to cry\"]',\n",
       " '[\"Part 2 is missing\"]',\n",
       " '[\"getting shot in the face has serious health ramifications\", \"people will need to wear bulletproof masks for protection\"]',\n",
       " '[\"Hiring a rapist is a solution\"]',\n",
       " '[\"She reminds me of Hillary Clinton and Amy Schumer.\"]',\n",
       " '[\"The person is not ugly\", \"The person is not fat\"]',\n",
       " '[\"The person has Trump Derangement Syndrome\"]',\n",
       " '[\"FBI is in Tatters\", \"Sacking Comey was a mistake\", \"Trump is responsible for the state of the FBI\"]',\n",
       " '[]',\n",
       " '[\"The photography in the documentary is amazing\", \"Some shots are mindblowing and seem impossible to plan\", \"The crew must have spent a long time waiting for certain natural events to occur\", \"The music is overblown\", \"The foley sound design is over the top\"]',\n",
       " '[\"The second installment did not meet the expectations set by the first one\"]',\n",
       " '[\"Some wannabe rich individuals are in legal trouble\", \"Some wannabe rich individuals are in financial trouble\"]',\n",
       " '[\"Mitt Romney should be supported for the 2012 election\"]',\n",
       " '[]',\n",
       " '[\"Men who kiss and tell are likely to hurt their partners\", \"A jealous man is not trustworthy\"]',\n",
       " '[]',\n",
       " '[\"The show is amazing\", \"You won\\'t regret seeing the show\", \"The sound clips of Reeve are a great addition\"]',\n",
       " '[\"You previously claimed to be 47 years old\", \"You previously claimed to be 40 years old\", \"Your current age claim of 41 is questionable\"]',\n",
       " '[\"The person/topic is purely evil\"]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[\"They should have lost Steve among all the other Steves\"]',\n",
       " '[\"The industry is worse\", \"Biodiversity is a fundamental pillar for all life on earth\", \"Cloned meat is a dead end\", \"Smallscale and local food is the right way to go\"]',\n",
       " '[\"the truth about something unspecified will be revealed in a few years\"]',\n",
       " '[\"the news is heartbreaking\"]',\n",
       " '[\"The speaker does not understand the banana joke\", \"The banana joke may be an old reference\"]',\n",
       " '[\"People should focus on complementing rather than competing with each other.\"]',\n",
       " '[]',\n",
       " '[\"Stress can have negative effects on the body\", \"Many veterans are on blood pressure medication\"]',\n",
       " '[\"The board behind Samantha is blurred\", \"The \\'UN\\' part on Sam\\'s shirt is covered by black tape\", \"Censoring the UN part would be a joke\", \"Covering the UN part with tape risks damaging the shirt\"]',\n",
       " '[\"There is something written on Tapper\\'s shirt that he tries to cover up.\"]',\n",
       " '[]',\n",
       " '[\"The show\\'s humor is based on mocking half of its potential viewer-base\", \"The show would succeed if it didn\\'t mock half its potential viewers\"]',\n",
       " '[]',\n",
       " '[\"The author was unfair to Bernie during the election\", \"A female president is desirable for many people\", \"Bernie could have beaten Trump\"]',\n",
       " '[\"I am going to stop watching this show and The Daily Show\"]',\n",
       " '[\"Sam should interview Meghan Markle\"]',\n",
       " '[]',\n",
       " '[\"Samantha\\'s birthday cake is made up of Ho-Ho\\'s\", \"The type of cake suits Samantha\"]',\n",
       " '[\"Full Frontal is not well understood\", \"The person being addressed cannot think for themselves\"]',\n",
       " '[\"the commenter finds the person in question attractive\", \"the commenter thinks the speaker would not criticize someone they find unattractive\"]',\n",
       " '[\"Women cannot say no on a cruise ship due to implications.\"]',\n",
       " '[\"Gorsuch is the best of Trump\\'s appointments\", \"Trump considered appointing a Republican senator from Alabama as a justice\", \"Nikki Haley is a good cabinet member\"]',\n",
       " '[\"That situation is completely wretched\"]',\n",
       " '[\"Trump supporters and some conservatives don\\'t see the difference\", \"Pointing out the truth is often met with accusations of being elitist\", \"Republicans have fed the anger of their base for 8 years\"]',\n",
       " '[\"Tom Martell is a four-time draft dodger\"]',\n",
       " '[\"She is a god.\"]',\n",
       " '[\"There are US politicians blocking bills designed to protect children from child marriage.\"]',\n",
       " '[\"Someone killed Jose C. Moya\\'s father\", \"Jose C. Moya wants revenge\"]',\n",
       " '[\"Donald Trump does not care about enabling\", \"There are people who have enabled Donald Trump\"]',\n",
       " '[]',\n",
       " '[\"The song did not try to appropriate the style\", \"The song made a good whole out of the style\"]',\n",
       " '[\"They\\'re working for Trump as long as he serves their donors\\' interests.\"]',\n",
       " '[\"The writer\\'s aunts did not take care of them when they were sweating.\"]',\n",
       " '[\"Seth Meyers is the true heir of Jon Stewart.\"]',\n",
       " '[\"Larry Wilmore has great content\"]',\n",
       " '[]',\n",
       " '[\"Sam does not understand Trump\\'s movement\"]',\n",
       " '[\"Trump is an authoritarian leader\", \"Unity is a code word for Trump demanding obedience\"]',\n",
       " '[]',\n",
       " '[\"the price of something is 250,000\"]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[\"Goldblum is old\"]',\n",
       " '[\"Videos like this one cause depression\"]',\n",
       " '[]',\n",
       " '[\"They wanted the book.\"]',\n",
       " '[\"Napoleon did not act alone\"]',\n",
       " '[]',\n",
       " '[\"I have never heard of you.\"]',\n",
       " '[\"Gerald Fraas misses the point of satire\", \"Gerald Fraas condescends to others by pasting a definition of non sequitur from Dictionary.com\", \"The Onion article posted by Gerald Fraas is pretending that Republican viewpoints are nuanced\"]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[\"Hunger Games is not available on Xbox\"]',\n",
       " '[]',\n",
       " '[\"Responding with \\'all lives matter\\' takes away from the Black Lives Matter movement\", \"Black Lives Matter is its own distinct cause\"]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[\"This person wants to subscribe to HBO because of one particular individual.\"]',\n",
       " '[]',\n",
       " '[\"Humans do not cause climate change\", \"Climate change is not a real thing\", \"NASA reported that the polar ice shelf has grown by 60% since 2012\", \"Government-backed scientists faked data on global warming in 2009 and 2013\", \"The concept of climate change is a scam to get money from people\"]',\n",
       " '[\"Last Week Tonight often criticizes HBO\\'s actions\"]',\n",
       " '[\"Paul Ryan is being compared unfavorably\"]',\n",
       " '[\"He was sending a message to his mistress.\"]',\n",
       " '[\"Payments from Medicare are about half of what private insurance typically pays\", \"Hospitals and doctors would see about a 40% reduction of revenues if paid at Medicare rates\", \"Only 18% of hospitals are for-profit\"]',\n",
       " '[\"Trump\\'s cabinet consists of criminals.\"]',\n",
       " '[\"Trump is getting arraigned\"]',\n",
       " '[\"Trump acts more like Hitler than any other US president\", \"Trump sells hate and fear by scapegoating\", \"Trump\\'s actions resemble those of a Fascist dictator\"]',\n",
       " '[\"The user is voting for Rosen.\"]',\n",
       " '[\"Matthews is a Clinton fanboy\", \"Hillary may need to take out a restraining order on Matthews\"]',\n",
       " '[\"Trump will have a high chance of winning the general election if Hillary wins the nomination\"]',\n",
       " '[\"Fox news runs stories without verifying them\", \"MSNBC tells viewers when a story hasn\\'t been independently verified\"]',\n",
       " '[\"A free story about Donnie is available for download from Adobe cloud\"]',\n",
       " '[]',\n",
       " '[\"The President is playing golf\", \"The President should be at the White House\"]',\n",
       " '[\"There was a study out of London about this topic that aired on NPR in the late nineties\", \"Exercising empathy is desirable\"]',\n",
       " '[\"He was paid to do that\"]',\n",
       " '[\"It\\'s not a popularity contest\"]',\n",
       " '[\"Ted panders through his teeth\", \"Ted acts like a spineless blob fish\", \"Seeing Ted\\'s face on TV causes nightmares\"]',\n",
       " '[\"Only 19% of Americans voted for Trump.\", \"More people voted for Hillary than Trump.\"]',\n",
       " '[\"Rush Limbaugh is associated with the Oxycotin in Broadcasting Network\"]',\n",
       " '[\"The post has zero impact\"]',\n",
       " '[\"This is fake news\"]',\n",
       " '[\"MSNBC handles certain Democrats with kid gloves\", \"The person being referred to is a corrupted corporate Democrat\"]',\n",
       " '[\"they are pathetic\"]',\n",
       " '[\"The lack of secrecy over time disproves a conspiracy\"]',\n",
       " '[\"Americans are stupid, gullible and easily distracted\", \"You will be distracted from getting answers\"]',\n",
       " '[\"Omarosa is being used by MSNBC\", \"MSNBC is spinning information\"]',\n",
       " '[\"Donny is crooked\"]',\n",
       " '[]',\n",
       " '[\"Someone might have told her to leak information\", \"She might be spying for a deep state\", \"Spying for a deep state would be treason\"]',\n",
       " '[\"Trump did something illegal\"]',\n",
       " '[\"Innocent people are protected by law from accusations\", \"The constitution provides protection for innocent people\"]',\n",
       " '[\"Trump committed a crime by pushing an altered map as real\", \"Trump can be fined and/or jailed for up to 90 days according to 18 U.S. Code 2074\"]',\n",
       " '[\"Forbes\\' estimates are not always accurate\", \"Wilbur Ross was a fake billionaire\"]',\n",
       " '[\"Dems lost an election\", \"the speaker is glad about the loss\"]',\n",
       " '[\"There was no exoneration\"]',\n",
       " '[]',\n",
       " '[\"Looking up and to the right indicates trying to remember something\", \"Looking up and to the left indicates making something up\"]',\n",
       " '[\"A large number of Americans elected Trump due to dissatisfaction with government corruption\", \"Liberals are trying to remove Trump out of fear\", \"Trump will not be removed from office\"]',\n",
       " '[\"The government should be equally outraged about abortion\", \"Federal funding of Planned Parenthood should be stopped\"]',\n",
       " '[\"Russia is involved in \\'Swampgate\\'\"]',\n",
       " '[\"Chris dominates conversations with guests\", \"The host should give guests a chance to speak\"]',\n",
       " '[\"Analogy in question is very accurate\"]',\n",
       " '[\"Having a female attorney question for the all-male panel will not change the Republican narrative\"]',\n",
       " '[\"The female candidate has had 35 years to prepare, while the male candidate has only had 35 minutes.\"]',\n",
       " '[]',\n",
       " '[\"Trump will make a deal with Putin\", \"Putin will fix the midterms for the Republicans\", \"Trump will loosen sanctions in exchange\", \"Trump will pull US troops out of Syria\"]',\n",
       " '[\"They own much of the Senate\", \"They own much of Congress\"]',\n",
       " '[\"Flake\\'s vote will not be influenced by lies\", \"They don\\'t care about lies or sexual assault\"]',\n",
       " '[\"Republican governors in Massachusetts, New Hampshire, and Vermont are not extremely conservative\", \"Democratic governors in Louisiana, Montana, and North Carolina are not extremely liberal\", \"Governors who can compromise are more likely to get elected and reelected\"]',\n",
       " '[\"Hillary is responsible\"]',\n",
       " '[\"Someone is trying to stall the investigation.\"]',\n",
       " '[\"The US executes traitors.\"]',\n",
       " '[\"Trump deceived people with false information\"]',\n",
       " '[\"This president exposed the fake news\", \"Those who can\\'t see it are stupid\"]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[\"They\\'re discussing the current president\"]',\n",
       " '[\"Close to 100% of Trump supporters believe strongly in having guns.\"]',\n",
       " '[\"Women running for President are a joke\"]',\n",
       " '[\"Republican governors may be repealing their states\\' death penalties\"]',\n",
       " '[\"Gandhi had a significant connection with salt\"]',\n",
       " '[\"Burning an American flag is a form of free speech\", \"A large percentage of Muslims follow the Koran word for word and believe it\\'s fine to kill non-followers\", \"Most people do not speak out against jihad because they are afraid\", \"The numbers Bill used in his argument about Islam are real\"]',\n",
       " '[\"@Tejas Raikar replied to the wrong person\"]',\n",
       " '[]',\n",
       " '[\"A civil war is desired by some individuals\", \"Some people wish harm upon those with opposing political views, specifically Democrats\"]',\n",
       " '[\"Bernie represents dignity\", \"Bernie represents honesty\"]',\n",
       " '[\"The audience is comprised of Clinton supporters\"]',\n",
       " '[\"Rebel XX has attractive breasts.\"]',\n",
       " '[\"The poster\\'s criteria for something is based solely on its attractiveness.\"]',\n",
       " '[\"She has a porn star vibe\", \"She looks like someone from Bang Bus\"]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[\"There are people who are extremely unintelligent\"]',\n",
       " '[\"Democrats have gained control of the Senate at some point in time\"]',\n",
       " '[\"He will never make it\", \"America is way too homophobic\"]',\n",
       " '[\"Intelligent people can believe in religious nonsense\", \"Religious beliefs are not intelligent\"]',\n",
       " '[\"Her brand of left-wing populist rhetoric is extremely unpopular on a national level\", \"Special business interests will work to keep her out of office\"]',\n",
       " '[\"The politician\\'s rhetoric about clean air energy sources is double speak for nuclear power\", \"The nuclear industry ignores public concerns and pushes risks and costs onto them\", \"Decades of protests were needed to get a moratorium on new nuclear plant construction\"]',\n",
       " '[\"Dale Swistun\\'s statement is just an opinion\"]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[\"The video is brilliant and funny\"]',\n",
       " '[\"The original poster should have said something nice.\"]',\n",
       " '[\"The investigators initially suggested the perpetrator was a black man\", \"Amanda Knox\\'s cell phone showed a message from her boss telling her she wasn\\'t needed at work\", \"Amanda Knox never actually confessed to the crime\"]',\n",
       " '[\"Obama hasn\\'t worked in the Midwest for a long time\", \"People often pry into someone\\'s religion\"]',\n",
       " '[\"dinosaurs, microscopic organisms, Neanderthals and other pre-historic humans are not mentioned in the Bible\", \"the Sun does not revolve around the Earth\", \"the Noah flood never occurred\"]',\n",
       " '[\"A wall should be built\"]',\n",
       " '[\"Bill has not tweeted since last night\", \"Someone should check if Bill is okay\"]',\n",
       " '[\"The Comey thing is still developing\"]',\n",
       " '[\"The US invaded over 20 countries after WWII\", \"The Middle East is in an ethnic and religious crisis similar to the Thirty Years War\", \"US infrastructure and education are suffering heavily\"]',\n",
       " '[\"Savage is a bill\"]',\n",
       " '[\"Donald Trump is pathetic\", \"Republicans are cry babies\"]',\n",
       " '[\"You are not a fan of free speech\", \"You are not a fan of a free press\"]',\n",
       " '[\"Russian bots should be ignored\"]',\n",
       " '[\"Trump should be impeached\", \"Russia should be bombed\", \"Churches should be taxed\", \"Republicans should be separated from their swastikas\"]',\n",
       " '[\"Toblerone\\'s rebranding is an example of deflection\"]',\n",
       " '[\"President Trump made 8,158 false or misleading claims in his first two years\", \"President Trump\\'s entire life, businesses, and deals have been a lie\"]',\n",
       " '[\"They changed their tag from news channel to comedy show\"]',\n",
       " '[]',\n",
       " '[\"Fox still controls the conservative news niche.\"]',\n",
       " '[]',\n",
       " '[\"Anderson Cooper is a great representative of the world\", \"Anderson Cooper\\'s mother is a queen\"]',\n",
       " '[\"CNN once showed up at an old woman\\'s home and accused her of colluding with the Russians\"]',\n",
       " '[]',\n",
       " '[\"Haiti is a shithole\"]',\n",
       " '[\"Donald Trump is a traitor to this country\", \"Trump\\'s supporters are traitors to this country\"]',\n",
       " '[\"Watching Fox is better\"]',\n",
       " '[\"Hillary Clinton masterminded a uranium deal\", \"The uranium deal was done for personal profit\"]',\n",
       " '[\"The video is old footage from Hurricane Hugo\", \"The video is meant to make Trump look weak\"]',\n",
       " '[\"He needs a lawyer to speak on his behalf\", \"He sucks at his job\"]',\n",
       " '[\"CNN did a story on the material of Paul Manafort\\'s jacket\"]',\n",
       " '[\"The writer\\'s response is ridiculous\", \"The only appropriate reason for the military and war is justice\"]',\n",
       " '[\"Obama was granted presidential privilege\", \"the author did not care about Obama\\'s presidential privilege\"]',\n",
       " '[\"A person with a history of drunken attempted rape is on the Supreme Court\"]',\n",
       " '[\"Cnn is hypocritical when discussing dishonesty\", \"Anderson Cooper is dishonest\"]',\n",
       " '[\"Cooper\\'s words have negatively impacted the commenter\\'s perception of him\"]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[\"Russia bombed\", \"the bombing made medical aid unnecessary\"]',\n",
       " '[\"Donald Trump frequently tells lies.\"]',\n",
       " '[\"Trump has a fragile brain\", \"Trump cannot handle tough discussions and complex situations\"]',\n",
       " '[\"In Korea, \\'Fox\\' is spelled as \\'CNN\\'.\"]',\n",
       " '[\"No lies were told\"]',\n",
       " '[\"He should have used his car\"]',\n",
       " '[\"John Podesta is a child rapist\", \"Bill Clinton is a rapist\", \"Weiner is a pedophile\"]',\n",
       " '[\"Anderson Vanderbilt Cooper will face a negative consequence\"]',\n",
       " '[\"Trump is fixing America\", \"Obama is Satan\"]',\n",
       " '[\"Some people repeat \\'fake news\\' without providing evidence\"]',\n",
       " '[\"Gus Dupree is not taking a salary.\"]',\n",
       " '[\"The feet in the image belong to white people.\"]',\n",
       " '[\"The future of children depends on truth, morality, and facts\", \"Adults are allowing blaming, name-calling, mocking, and slandering\"]',\n",
       " '[]',\n",
       " '[\"Anderson Cooper\\'s report was insignificant\", \"CNN is no longer credible\"]',\n",
       " '[]',\n",
       " '[\"Many Irish people were distinguished from whites on the basis of ethnicity in history\", \"Irish people often referred to themselves as white but part of a separate \\'Irish\\' ethnic group\"]',\n",
       " '[\"CNN has been lying to its viewers for 3 years\"]',\n",
       " '[\"She resembles coal.\"]',\n",
       " '[\"He is not man enough to have served\", \"John McCain was a better man than him\"]',\n",
       " '[\"trump is racist\"]',\n",
       " '[\"Donald Trump is a bad person\"]',\n",
       " '[\"There are people who believe in superstitious nonsense.\"]',\n",
       " '[\"The person being referred to does not like her husband\"]',\n",
       " '[\"The world has something wrong with it\", \"These comments are disgraceful\", \"She lost her children, husband, and her family\"]',\n",
       " '[\"Military assets are being moved.\"]',\n",
       " '[\"Bernie is the most consistent progressive\", \"Bernie is the most trustworthy progressive\"]',\n",
       " '[\"Five people have died\"]',\n",
       " '[]',\n",
       " '[\"There are lots of sheep.\"]',\n",
       " '[\"The Catholic rosary is loved by the author\", \"The drug war is causing an overtaking of South American countries\", \"Taxpayer dollars are wasted on police, prisons, and rehabs in the United States\", \"Some people are passively suicidal\", \"People will only get sober when their life gets bad enough\", \"Long-term sobriety comes from fear of returning to addiction\", \"Many addicts will simply die\", \"The minute you\\'re born you\\'re in the process of dying\"]',\n",
       " '[\"Oakland is like the wild west\", \"The author pays $750 for a bedroom in a ghetto area of Oakland and shares an apartment with a roommate\"]',\n",
       " '[\"A shoplifter killed a grandmother\"]',\n",
       " '[\"Some women were allegedly forced to sleep with certain individuals\"]',\n",
       " '[]',\n",
       " '[\"The US is the true enemy of Venezuela\", \"US sanctions cause hyperinflation and starvation in Venezuela\"]',\n",
       " '[\"The American people have a reputation for being working alcoholics\", \"Third world countries are war zones where governments cannot protect their citizens\\' safety and food needs\", \"Immigrants from third world countries do not adopt American tradition, culture, and religion after being accepted into the US\", \"Immigrants take advantage of the US system without paying into it\"]',\n",
       " '[\"CBS continues to promote war for the benefit of the military industrial complex\", \"US troops should be brought home\"]',\n",
       " '[\"the government should return found items to their discoverers\", \"people who find valuable items are treated unfairly\"]',\n",
       " '[\"the current president is the dumbest one\"]',\n",
       " '[\"Trillions of dollars are leaving the stock market\", \"The tariffs wars won\\'t end well with a slowing global economy\"]',\n",
       " '[\"The person is glad to see free kids\"]',\n",
       " '[\"Gun owners are responsible for the deaths of 58 people\", \"The right to life is the first right mentioned in the Declaration of Independence\"]',\n",
       " '[\"Zionists are trying to harm Palestinians\", \"Palestinians are poor\"]',\n",
       " '[\"Jeffrey Epstein is leaving the country\", \"Michael Bloomberg is personally escorting Jeffrey Epstein\"]',\n",
       " '[\"Customs only have computers\"]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[\"He saved 3 girls\", \"God is in the clouds and cannot save anyone\"]',\n",
       " '[\"Paying a settlement for a rape charge implies guilt\", \"An innocent person would fight for their innocence rather than pay a settlement\", \"Wealth and resources should be used to defend against false accusations\"]',\n",
       " '[\"Islam is a religion not a race\"]',\n",
       " '[]',\n",
       " '[\"this guy is a hero\"]',\n",
       " '[]',\n",
       " '[\"God has excellent strength\", \"There is an excellent legacy\"]',\n",
       " '[\"We are all in this together\"]',\n",
       " '[\"The post was boring\", \"The post could have been funny\"]',\n",
       " '[\"A great man was killed\"]',\n",
       " '[\"The person in question has parents\"]',\n",
       " '[\"the audio is terrible\"]',\n",
       " '[\"Israel is bullying other countries in the region\", \"Israel has nuclear weapons\"]',\n",
       " '[\"the person has no relevant qualifications\"]',\n",
       " '[\"Marijuana is good for ADD\"]',\n",
       " '[\"Nomi spies on users for the government\", \"Nomi can listen and has a camera\", \"People are too used to government intrusion\"]',\n",
       " '[\"Priority Boarding guarantees a B card and nearly always gets you an A card\", \"The earlier you buy a ticket with Priority Boarding, the greater the chance of getting an A card\"]',\n",
       " '[\"Shaquille Griffin broke the record for the 40 yard dash for linebackers\", \"The bond between Shaquille and Sha\\'keem has strengthened their team\"]',\n",
       " '[\"Lewis Hamilton visited Brazil for the first time in 2009\", \"Hamilton did not visit Senna\\'s grave in 2007 because he wanted to avoid crowds\"]',\n",
       " '[\"California requires a 60-hour certified course and passing a test to get licensed\", \"Conservatives think California has too many regulations\", \"More fraud occurs in unregulated Republican states than in regulated California\"]',\n",
       " '[\"Elon Musk is innovative and creates thousands of jobs\", \"The government lacks innovation\", \"People like Elon Musk make it exciting to be alive\"]',\n",
       " '[\"The lyrics in the song Brainless don\\'t need to rhyme perfectly\"]',\n",
       " '[\"1:37 is replacing 60 minutes\"]',\n",
       " '[\"We need some of that.\"]',\n",
       " '[\"The answer is no\"]',\n",
       " '[\"The Inverse Square Law does not apply to extended sources of radiation\", \"Geiger counters and other radiation detection monitors are calibrated using the Inverse Square Law\", \"Measurements taken by Geiger counters do not represent the true amount of ionizing events or radionuclides in biota and bodies\"]',\n",
       " '[\"The government is aware of something that journalists do not.\"]',\n",
       " '[]',\n",
       " '[\"The original 60 minutes episode is being sold on Amazon for $79.99 USD\"]',\n",
       " '[\"Nancy\\'s words lack meaning\", \"Nancy\\'s words are not humorous\"]',\n",
       " '[\"The ACA was modeled for people who didn\\'t have access to health care\", \"Single Payer plans provide better quality of care and coverage\"]',\n",
       " '[\"People should not stop traveling internationally\", \"Precautions can be taken by travelers\", \"Individuals with a compromised immune system need special guidance\"]',\n",
       " '[\"No bullying occurred\"]',\n",
       " '[\"There is nothing floating in the Pacific Ocean from Japan\"]',\n",
       " '[\"The information is documented in news articles from 2016\"]',\n",
       " '[\"Someone other than Trump would have been a better choice for president\"]',\n",
       " '[\"They arrested Mike\"]',\n",
       " '[\"Beek did not apologize\"]',\n",
       " '[]',\n",
       " '[\"He/they were not pervy enough to imagine the concept.\"]',\n",
       " '[\"The American public gets its reflection from its president\", \"The American public gets its reflection from its television\"]',\n",
       " '[\"he won\\'t last 4 years\"]',\n",
       " '[\"The individuals involved are comedians\"]',\n",
       " '[\"The informal definition of \\'knocker\\' is a persistent and carping critic or faultfinder.\"]',\n",
       " '[]',\n",
       " '[\"Fox Crane is somehow involved\"]',\n",
       " '[\"A widening perspective can be achieved with money\", \"Young people should take note of the relationship between a widening perspective and wealth\"]',\n",
       " '[\"Racism, sexism, homophobia and ignorance are not acceptable positions or arguments\", \"Ignorance and a lack of education should not be celebrated\", \"Most of Trump\\'s economic policies have been dismissed as unrealistic or prohibitively expensive by policy analysts\"]',\n",
       " '[\"The post contained only one good joke\", \"Explaining the joke was unnecessary\"]',\n",
       " '[]',\n",
       " '[\"There was a conjugal visit between Donald Trump and Ivanka on September 1 before his impeachment.\"]',\n",
       " '[\"Seth Meyers always looks like the hero\\'s most awkward friend\"]',\n",
       " '[\"He is not funny in this show\"]',\n",
       " '[]',\n",
       " '[\"Trump called George Washington stupid\", \"Trump\\'s birth certificate is questionable\"]',\n",
       " '[\"Apple had exploding batteries in 2009\", \"Samsung currently produces iPhone batteries\"]',\n",
       " '[\"iPhone batteries are explosive\"]',\n",
       " '[\"She is a lovely lady\"]',\n",
       " '[\"the event is a complete disaster\", \"expensive truffle went to waste\"]',\n",
       " '[]',\n",
       " '[\"Annie is pretty young\", \"we should not sexualize Annie\"]',\n",
       " '[\"AFD is a fascist party\", \"there are other parties that could be considered fascist\"]',\n",
       " '[]',\n",
       " '[\"Seth\\'s subscriber count is below 400,000\", \"Having Ben Affleck, Henry Cavill, Jessie Eisenberg, or Amy Adams on his show will increase Seth\\'s subscriber count\", \"Fallon and Kimmel typically have better guests than Seth\"]',\n",
       " '[\"The Secretary of Defense was honest about appealing to young people.\", \"Being clear about intentions is sufficient for acceptance.\"]',\n",
       " '[\"Trump views women as objects for his own benefit\", \"Some men view women as property due to patriarchal attitudes\"]',\n",
       " '[\"Mike Pence is a ghost\"]',\n",
       " '[\"Red Dawn is related to the topic\"]',\n",
       " '[\"Lindsey Graham and Susan Collins acted like Republicans\"]',\n",
       " '[\"More information about the Hannity-Trump relationship will be revealed\", \"The person has no problems and is doing well\", \"The person doesn\\'t tolerate foolish people\"]',\n",
       " '[\"the person in the picture/video is hugging a pole\"]',\n",
       " '[\"A war may start\", \"Many Koreans, North and/or South, will be killed in a war\"]',\n",
       " '[\"Mueller has a shady lead\"]',\n",
       " '[\"Stormy Daniels should be the President in 2020\"]',\n",
       " '[\"Graham is right\", \"Kavanaugh should be re-nominated if the vote fails\"]',\n",
       " '[\"President Trump is correct in saying that US citizens also have dreams.\"]',\n",
       " '[\"People are born as either male or female sex\", \"Gender refers to the sex of an individual\", \"Gender is not an assignment to either sex\", \"Gender is not a choice\"]',\n",
       " '[\"An armed officer was present at the school during the shooting\", \"Having an armed officer present does not prevent shootings\"]',\n",
       " '[]',\n",
       " '[\"Trump should fire Kushner\", \"Kushner might become a snitch\", \"Hillary should be impeached\"]',\n",
       " '[\"Houston, Texas is part of North Eastern Mexico\", \"Southern California is part of North Western Mexico\"]',\n",
       " '[]',\n",
       " '[\"Trump is a traitor to his country\", \"Trump has committed treason against America\"]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[\"Many people besides McCain voted no\"]',\n",
       " '[\"The Clintons are behind Mueller\\'s hiring\", \"The Russian Probe did not investigate the Clintons\", \"The Clintons were deeply involved with the Russians\"]',\n",
       " '[\"Jesus Christ has returned\"]',\n",
       " '[\"Difficult employees can be transferred to other locations\", \"Adak Alaska has job openings\"]',\n",
       " '[\"the author agrees with the content of the post\"]',\n",
       " '[\"Pelosi has lost control\"]',\n",
       " '[\"Democrats will give in to some of Trump\\'s demands\", \"Trump will claim victory despite not getting everything he wants\", \"The border wall will never be built\"]',\n",
       " '[\"Their vote to not defend the nation counts as treason\"]',\n",
       " '[]',\n",
       " '[\"Hillary, Obama and Comey should be put to death for treason\"]',\n",
       " '[\"Liberal Hollywood is responsible for over 95% of America\\'s pedophiles\", \"Liberal Hollywood is responsible for over 95% of America\\'s rapists\", \"Liberal Hollywood is responsible for over 95% of America\\'s racists\"]',\n",
       " '[\"He lives in America\", \"He is an American national\"]',\n",
       " '[\"Omarosa is superior to Trump\"]',\n",
       " '[\"Trump can use a certain pathway to build his wall.\"]',\n",
       " '[\"Hannity should start doing movie reviews.\"]',\n",
       " '[\"@The Real World is upsetting Vlad\", \"Increasing the mic volume will improve audio quality\"]',\n",
       " '[\"This post is for entertainment purposes.\"]',\n",
       " '[\"Youngsters are vulnerable to evil influences and actions\", \"Decent parents being absent from their lives can make youngsters more vulnerable\", \"A male who fails as a father can occasion victimization and trauma in his daughter\\'s life\", \"Hypocritical promiscuous males produce females emotionally deprived of paternal care and affection, making them vulnerable to promiscuity\", \"No man or woman should abandon their children, regardless of the other parent being an absolute mess and headache\", \"A person is responsible for doing their best to still treat the other parent with care and respect, even after a split\", \"Children are not responsible for their parents',\n",
       " '[]',\n",
       " '[\"Pat Tillman was an American hero\"]',\n",
       " '[\"People who dislike Trump do so simply because they hate him.\"]',\n",
       " '[\"Hillary Clinton is not trustworthy\", \"Hillary Clinton lies\"]',\n",
       " '[\"Hannity has a low bar\", \"Hannity is a moron\"]',\n",
       " '[\"Most people find this situation infuriating\"]',\n",
       " '[\"Trump supporters have become racist\", \"Fox can no longer align themselves with Trump supporters\"]',\n",
       " '[\"Comey\\'s family values are being questioned\"]',\n",
       " '[\"Tyler is going to hell\"]',\n",
       " '[\"God\\'s protection is requested for someone and their family\"]',\n",
       " '[\"Someone needs to be punished severely for this\"]',\n",
       " '[\"Trump will win again in 2020\"]',\n",
       " '[\"Corrine Brown enriched her family and friends\"]',\n",
       " '[\"Hunter Biden is involved in the Ukraine oil business\"]',\n",
       " '[\"The outcome was expected\"]',\n",
       " '[\"Democrats are hypocrites\"]',\n",
       " '[\"Donald Trump is a loser\"]',\n",
       " '[\"President Trump is the fairest\"]',\n",
       " '[\"Trump is stupid\"]',\n",
       " '[]',\n",
       " '[\"The person in question is not a racist\", \"Killing an innocent black person was unjustified\"]',\n",
       " '[\"People died because they were stupid\", \"The vehicle was unsafe\"]',\n",
       " '[\"The statement is true\"]',\n",
       " '[\"The police made a honest mistake\", \"The homeowner should not have had his gun out if the burglars were already gone\", \"The dispatcher failed to inform the homeowner that the police were on scene\"]',\n",
       " '[\"The speaker\\'s favorite president is not bad\", \"The person being addressed, along with the speaker\\'s enemies, are bad\"]',\n",
       " '[]',\n",
       " '[\"The speaker dropped by with only a few hours\\' notice\", \"The room was not packed\"]',\n",
       " '[\"There is truth\"]',\n",
       " '[\"She survived miraculously\", \"Puerto Rico is America\\'s colony\"]',\n",
       " '[\"Evil things can happen in the writer\\'s home country\"]',\n",
       " '[\"Living things on Earth have gone through evolution\", \"Living things on Earth are currently going through evolution\"]',\n",
       " '[\"There was a news crew filming at the location of an incident but everything appeared normal.\"]',\n",
       " '[\"The writer is receiving tax dollars\", \"The writer views others as slaves\"]',\n",
       " '[]',\n",
       " '[\"People are rooting for the alligator.\"]',\n",
       " '[\"He should be punished severely\"]',\n",
       " '[\"A random act of kindness is a beautiful thing.\", \"ABC posts some amazing videos.\"]',\n",
       " '[\"There has been another school shooting\"]',\n",
       " '[\"Gang stalking will destroy the United States\", \"God commands people to love their neighbors\", \"Gang stalking involves spying on, stealing from, and trying to kill one\\'s neighbors without leaving evidence\"]',\n",
       " '[\"The government saved companies and people\\'s jobs with taxpayer money\", \"Taxpayers are entitled to a refund of their tax money used for saving companies\"]',\n",
       " '[\"The economy is doing well\"]',\n",
       " '[\"People should do their own research\"]',\n",
       " '[\"A Whopper Jr costs $1.39\", \"Completing a 3-minute survey on the receipt gives a code for a free Whopper\"]',\n",
       " '[\"the person who chose to shoot a gun is solely responsible for the incident\"]',\n",
       " '[\"There will be hysteria from the left when Ginsburg leaves office\", \"Another conservative judge will get nominated after Ginsburg\\'s departure\", \"The next 30 years will be marked by conservative rule\"]',\n",
       " '[\"It is important to have travel insurance when traveling.\"]',\n",
       " '[\"The speaker wants to continue using a certain method to communicate with someone\", \"David Muir can be seen at the Walmart grocery store in Orlando or the Subway restaurant in Walmart\"]',\n",
       " '[\"Money is the main motivator\", \"Racism is involved\"]',\n",
       " '[]',\n",
       " '[\"There are groups fighting each other over the belief in a deity\", \"The deity mentioned is being mocked and disrespected by someone claiming to be it\"]',\n",
       " '[\"The universe\\'s origins imply the existence of God\", \"God created the universe and everything in it out of matter\", \"The big bang should have produced equal amounts of matter and anti-matter, but didn\\'t\", \"God is eternal, all-powerful, and has no origin\", \"God created time and is above time\", \"God created a world that appeared older than it actually was\", \"Random subatomic particles cannot form life\", \"There can only be one supreme God\"]',\n",
       " '[\"The statement is false\", \"Repeating a statement does not make it true\"]',\n",
       " '[\"A new study is unclear about whether to use \\'woman\\' or \\'women\\'\"]',\n",
       " '[\"Trump should be re-elected in 2020\"]',\n",
       " '[\"America does not negotiate with terrorists\"]',\n",
       " '[\"Democrats promote an inhumane and animalistic way of living\", \"the current situation is inhumane and should be stopped by ending the lives of those affected\"]',\n",
       " '[\"The government won\\'t grant citizenship to many Hispanic families who have applied\", \"Republicans and Democrats are working together to eliminate freedom of speech\", \"Net neutrality is being dismantled as part of a larger conspiracy\"]',\n",
       " '[\"There is something wrong with protecting our borders\", \"Laws regarding border protection have been in place for years\"]',\n",
       " '[\"This isn\\'t happening\"]',\n",
       " '[\"97% of the homeless in downtown LA are unemployable due to illness\"]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[\"Dems getting their way will make America like Mumbai\", \"Mumbai will become like America\"]',\n",
       " '[\"the person\\'s mind is gone\"]',\n",
       " '[\"Mueller should be fired\"]',\n",
       " '[\"The investigation is a political hit-job\", \"The investigation is a joke\"]',\n",
       " '[\"Absolute power corrupts absolutely\"]',\n",
       " '[\"Things are not looking good for Trump legally\"]',\n",
       " '[\"Black people make racial remarks about white people\", \"Making such remarks is acceptable for black people or liberals\"]',\n",
       " '[\"Donald Trump should be fired\"]',\n",
       " '[\"Obama is responsible for something\"]',\n",
       " '[]',\n",
       " '[\"Sales are up.\"]',\n",
       " '[\"Zinke is corrupt\"]',\n",
       " '[\"There are only two genders\", \"Sexual orientation should be kept private and not used for leverage\"]',\n",
       " '[\"Hillary being at the museum ruins the experience\"]',\n",
       " '[\"Trump\\'s benefits should be exterminated\", \"Lazas benefits should be resolved forever\"]',\n",
       " '[]',\n",
       " '[\"Fox has been reporting more facts about this ongoing story than other networks\"]',\n",
       " '[\"Fox News should not be taken as a credible source.\"]',\n",
       " '[\"The rich and powerful are above the law\", \"The rich and powerful commit crimes that go unpunished\"]',\n",
       " '[\"England has lost its democratic values and common sense\", \"England is comparable to a \\'shit hole\\' Muslim country\"]',\n",
       " '[\"Tucker said that Trump supporters are racist and do not want to debate about immigration.\"]',\n",
       " '[\"This is the smartest thing he has ever said\"]',\n",
       " '[]',\n",
       " '[\"YouTube users should read the terms and services before complaining\", \"Conservatives are hypocritical about their stance on free market\"]',\n",
       " '[\"the deal fell through\"]',\n",
       " '[\"Liberals/Democrats will cause America\\'s downfall\"]',\n",
       " '[\"Wray\\'s resignation reports were false\"]',\n",
       " '[\"North Korea is not honoring the agreement\", \"North Korea is still testing ballistic missiles\"]',\n",
       " '[\"Trump has not been found guilty\"]',\n",
       " '[\"Cloud seeding cannot control weather in a large area like 100 square miles\"]',\n",
       " '[\"You give someone views by watching them\"]',\n",
       " '[]',\n",
       " '[\"CNN has not uploaded an update on this topic\"]',\n",
       " '[\"Progressives are fakers following a false ideology\", \"Many progressives are willing to give up their rights to the corporate machine\", \"The democratic system is morally bankrupt\", \"The media is brainwashing people\", \"Most of the government is corrupt\"]',\n",
       " '[\"Dailystormer was the first website to be banned\"]',\n",
       " '[\"California has declined\"]',\n",
       " '[\"The writer screwed up the USA\", \"The writer got away with molesting Catherine O\\'Brien\"]',\n",
       " '[\"Al Capone tried to intimidate witnesses\", \"Manafort tried to intimidate witnesses\"]',\n",
       " '[\"The president should be forcibly removed from office immediately.\"]',\n",
       " '[\"Nunes should be tried for treason\", \"The House investigation is a joke\"]',\n",
       " '[\"The intel on Saddam Hussein\\'s WMD program was accurate.\"]',\n",
       " '[\"America should prioritize its own interests above all else\", \"Jewish people are a problem\"]',\n",
       " '[\"People are involved in seditious activities\", \"The situation is very serious\"]',\n",
       " '[\"Republican politicians throw their own under the bus\", \"Democrats circle the wagons to protect their own\"]',\n",
       " '[\"Democrats claim President Trump committed treason by answering a hypothetical question\"]',\n",
       " '[\"Tucker is hinting that he is under investigation\", \"The US has become a banana republic if Tucker is investigated\"]',\n",
       " '[]',\n",
       " '[\"Abortion is equivalent to playing God or making a deal with the devil.\"]',\n",
       " '[\"Liberal people are communists\"]',\n",
       " '[\"Learn English\"]',\n",
       " '[\"America cannot have a dialogue with leftists because they are too violent and nuts.\"]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[\"Amitha Alex\\'s performance at 1030 was worse than her performance at 1310\"]',\n",
       " '[\"Their opinion on her is not wanted\", \"The topic is important\"]',\n",
       " '[]',\n",
       " '[\"He is attractive\"]',\n",
       " '[\"Government disarmament can lead to tyranny\", \"Tyranny occurred in Nazi Germany due to government disarmament\", \"Tyranny occurred in modern-day Venezuela due to government disarmament\", \"Tyranny occurred in the Soviet Union due to government disarmament\", \"Tyranny occurred in Cuba due to government disarmament\"]',\n",
       " '[\"Hasan can actually pull it off.\"]',\n",
       " '[\"the speaker misses the show\"]',\n",
       " '[\"HBO promotes its shows on The Tonight Show\", \"Netflix promotes its shows on Last Week Tonight\"]',\n",
       " '[\"The posted content is incomplete, missing the intro.\"]',\n",
       " '[\"Politics and business are connected\", \"Not liking someone\\'s politics can lead to a loss of business\"]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[\"Hassan deserves an Emmy\"]',\n",
       " '[]',\n",
       " '[\"He is stupid\", \"I love him\"]',\n",
       " '[\"Imran Khan cannot bring change\", \"Pakistan is a hopeless country\"]',\n",
       " '[]',\n",
       " '[\"his wife asked for a divorce\"]',\n",
       " '[\"The commenter\\'s like is the eighteenth.\"]',\n",
       " '[]',\n",
       " '[\"It\\'s not a good idea\", \"The idea has negative effects that can be felt in India\"]',\n",
       " '[\"His eyes are huge\"]',\n",
       " '[\"Mohammed loves someone\"]',\n",
       " '[\"Someone is being referred to as a Trumptard\"]',\n",
       " '[\"Non-dry turkey is rare\", \"Eating large amounts of chicken may not be desirable\"]',\n",
       " '[\"Almost everything can be found on YouTube\", \"Netflix has less content compared to YouTube\"]',\n",
       " '[]',\n",
       " '[\"You are sharing love, happiness, community and unity\", \"You are a beautiful being\"]',\n",
       " '[\"The user eats unusual types of mangoes\"]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[\"McStain had a tumor\"]',\n",
       " '[\"Trump speaks on behalf of Putin\", \"Putin wants Greenland for its resources or location\"]',\n",
       " '[\"You can\\'t trust a man who doesn\\'t know how to grill.\"]',\n",
       " '[\"Betsy DeVos has no clear stance on issues\", \"Betsy DeVos was given her position due to politics rather than merit\", \"Betsy DeVos is a disgrace\"]',\n",
       " '[\"@SpeakerPelosi, @JerryNadler, @SenSchumer, and many other Democrats have changed their stance on an unspecified issue\"]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[\"The current time is the worst period in history\", \"The event mentioned in the article is a source of great shame\"]',\n",
       " '[\"The Late Show with Stephen Colbert is frustrating to watch\", \"Late night lineups used to be fun to watch\"]',\n",
       " '[\"The current administration is unfit\", \"There will be a Freedom Day with justice\"]',\n",
       " '[\"CBS will not cover the Andrew McCabe story\", \"The Andrew McCabe story is real and being ignored by CBS\"]',\n",
       " '[\"The current President is unqualified\"]',\n",
       " '[\"Tucker Carlson is backpedaling.\"]',\n",
       " '[]',\n",
       " '[\"A country, alongside Australia, should investigate the dossier and its impact on lives.\"]',\n",
       " '[\"Professors are secretly taking a new unknown drug\", \"Professors are ignorant\"]',\n",
       " '[\"Jesus will return soon\"]',\n",
       " '[\"He should be thrown out\"]',\n",
       " '[]',\n",
       " '[\"The show \\'Last Week Tonight\\' has lasted\", \"The show\\'s lasting is unsurprising given its name\"]',\n",
       " '[\"Seth Meyers and Stephen Colbert did not get Trump elected\"]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[\"The author is accusing NewsHour of sounding like they are with the DNC\", \"Stating facts is being misinterpreted as biased\"]',\n",
       " '[\"Stop using One Direction references\"]',\n",
       " '[\"POTUS and VP were in NC the night before the election\", \"They went to NC to support a special election\"]',\n",
       " '[\"Storm cellars exist\"]',\n",
       " '[\"Bill Maher is willing to speak his mind and tell the truth\", \"Bill Maher makes off-color comments\"]',\n",
       " '[\"The person does not want anything to do with Ilhan\"]',\n",
       " '[\"TRUMP IS A CRIMINAL\"]',\n",
       " '[\"The authors\\' work was discussed on Anderson LIVE\", \"Residents of Newtown were subjected to many cons after Sandy Hook\"]',\n",
       " '[\"The sequence of events surrounding an issue cannot be spun\", \"There is something significant about the timeline that started in or after 2017\"]',\n",
       " '[\"He\\'s going to regret his action\"]',\n",
       " '[\"Bill Maher is a total ass\"]',\n",
       " '[\"Trump doesn\\'t have the guts to show his tax returns\", \"Trump doesn\\'t have the guts to face Congress and Mueller\"]',\n",
       " '[\"The American people are disgusted\"]',\n",
       " '[]',\n",
       " '[\"The next Olympics will give out participation medals\"]',\n",
       " '[\"Democrats have hawks\", \"Republicans have a pizza squad that makes asses out of themselves\", \"The Republican pizza squad cannot stand up to Trump\"]',\n",
       " '[]',\n",
       " '[\"John Oliver\\'s show is praiseworthy\"]',\n",
       " '[]',\n",
       " '[\"The behavior of @hardball, @ChrisMurphyCT, @SenWhitehouse, @RepKarenBass, and @SenMarkey is embarrassing\"]',\n",
       " '[\"They haven\\'t banned them, only announced regulations\"]',\n",
       " '[\"Anonymous sources claim Tucker Carlson has 7 Saudi wives.\"]',\n",
       " '[\"He is a sociopath\"]',\n",
       " '[\"Katie Hopkins is wise\", \"Katie Hopkins is smart\"]',\n",
       " '[\"The US still shames women for telling the truth\", \"Politics is prioritized over rightness\"]',\n",
       " '[\"Trump would compromise US intelligence if left alone with Putin\"]',\n",
       " '[\"the tweet implicated him in his criminal actions by giving motive\"]',\n",
       " '[\"@60Minutes and others are harming millions of chronic pain patients\", \"The mentioned individuals do not care about the harm they are causing\"]',\n",
       " '[@MichelleIsAWolf is the desired type of comedian.]',\n",
       " '[\"We don\\'t deserve animals\"]',\n",
       " '[]',\n",
       " '[\"The news is fake\", \"This is a witch hunt\"]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[\"The episode will be exciting\"]',\n",
       " '[\"The content is ridiculous\"]',\n",
       " '[\"There is no excuse for unequal pay\"]',\n",
       " '[\"@FullFrontalSamB is the same age as me.\"]',\n",
       " '[\"PBS and CNN\\'s journalism is purely partisan\"]',\n",
       " '[\"The president is dishonest\"]',\n",
       " '[\"Joe Biden thinks he\\'s more important than others regarding his comments on abortion\"]',\n",
       " '[\"John Oliver\\'s show Last Week Tonight has achieved great success\"]',\n",
       " '[\"@RealTimers is claiming that Bill Maher\\'s reference is about Barack Obama\"]',\n",
       " '[\"Words matter\"]',\n",
       " '[\"He lies about lying.\"]',\n",
       " '[\"The author missed Last Week Tonight\"]',\n",
       " '[]',\n",
       " '[\"AG Sessions should take this case to SCOTUS\"]',\n",
       " '[\"People\\'s hatred for Oprah contributed to her becoming a billionaire multiple times\"]',\n",
       " '[\"Eugene Robinson is an idiot\"]',\n",
       " '[\"@patriotact agrees with what @mypoohonked said\"]',\n",
       " '[\"The user will watch Last Week Tonight\"]',\n",
       " '[\"Nothing has been done about the issue\", \"The issue continues and will continue\"]',\n",
       " '[\"Environmental activists are not doing enough\"]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[\"The solution is not permanent.\"]',\n",
       " '[]',\n",
       " '[\"Something is definitely needed\"]',\n",
       " '[\"The commenter plans to travel to New York to take Sam B\\'s blazer\"]',\n",
       " '[\"The show Last Week Tonight was at risk of being cancelled\", \"Last Week Tonight will return\"]',\n",
       " '[\"The conversation with the NRA guest was bad\", \"The host was gun shy\"]',\n",
       " '[\"Lindsey Graham tried to defend McCabe\", \"Lindsey Graham was shut down by Margaret Brennan\"]',\n",
       " '[\"Anderson Cooper\\'s reporting is fake news\", \"The truth about a certain issue is emerging\"]',\n",
       " '[@AC360 is feeling hungry and thirsty]',\n",
       " '[\"Their vote shouldn\\'t depend on whether POTUS will sign or not.\"]',\n",
       " '[\"Washington Nationals fans booed President Trump\", \"The writer hopes Houston wins the series\"]',\n",
       " '[\"Trump supporters will be more motivated after this event\"]',\n",
       " '[\"Tucker Carlson knows how to ask great questions\"]',\n",
       " '[]',\n",
       " '[\"Maxine Waters is loved by the commenter\"]',\n",
       " '[]',\n",
       " '[\"There should be no war with Iran\"]',\n",
       " '[\"Republicans worship a different Jesus\"]',\n",
       " '[\"It is not a good idea\"]',\n",
       " '[]',\n",
       " '[\"Those are the ugliest shoes I have ever seen\"]',\n",
       " '[\"Trump\\'s downfall was predictable\", \"Trump made his own downfall easy\"]',\n",
       " '[]',\n",
       " '[\"Nothing tops Judy Woodruff\\'s betrayal to the brave men in Benghazi.\"]',\n",
       " '[\"Hasan Minhaj uses hand gestures to distract audiences from listening to him\"]',\n",
       " \"[@Nightline had the user on their show, The user's experience on Nightline was fun despite serious circumstances]\",\n",
       " '[\"@MeetThePress flirted with Chuck Todd\"]',\n",
       " '[\"Trump described Michael Cohen\\'s bust as an attack on America\"]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[\"@RepMikeQuigley has a cough\"]',\n",
       " '[\"The situation described in the post is evil\", \"Calling the civil authorities was the moral thing to do\"]',\n",
       " '[\"I am no longer interested in him.\"]',\n",
       " '[\"The speaker agrees with The Daily Show\"]',\n",
       " '[\"The post is brilliant\", \"The post is necessary\"]',\n",
       " '[]',\n",
       " '[\"He must go\", \"Making Americans sick\"]',\n",
       " \"[@LateNightSeth and @HeadCountOrg's YouTube stream is stuck]\",\n",
       " '[]',\n",
       " '[\"@AC360 @davidcicilline They are\"]',\n",
       " '[\"John Oliver is a legend\"]',\n",
       " '[\"An officer changed a woman\\'s tire on the highway\", \"Police officers are essential to the country\"]',\n",
       " '[]',\n",
       " '[\"He got a conviction\", \"Hillary walked away free\"]',\n",
       " '[\"Women have been depicted nude in media for years\", \"It\\'s time for men to be depicted nude as well\"]',\n",
       " '[\"Inhumane Chinese bots exist\", \"There has been an increase in illogical bots recently\"]',\n",
       " '[]',\n",
       " '[\"NYT wasn\\'t the only news agency to report on Trump campaign and Russia\", \"Clapper\\'s testimony is worth watching\"]',\n",
       " '[\"The dog has a small tail\"]',\n",
       " '[\"A stenographer taking down the phone call would have produced an accurate transcript\"]',\n",
       " '[\"Trump is an immature man\", \"Trump craves attention and adoration\"]',\n",
       " '[\"There are many American women who hate their country\"]',\n",
       " '[\"Bill Clinton is relevant to the conversation\"]',\n",
       " '[\"Something will not happen\"]',\n",
       " '[\"Major news media poses an imminent danger to America\"]',\n",
       " '[\"There was a protest\"]',\n",
       " '[]',\n",
       " '[\"The woman is stupid\", \"The man must be crazy for wanting to be with her\"]',\n",
       " '[\"Paul Ryan quit for a reason other than family\"]',\n",
       " '[\"Democrats should continue their current actions\"]',\n",
       " '[\"Hillary Clinton is president\", \"Nobody believes NBC\\'s polling data\"]',\n",
       " '[\"Trump\\'s actions hurt Americans\", \"Trump\\'s goal is to appease Ann Coulter\"]',\n",
       " '[\"Neanderthals crossbred with humans\", \"One in five humans have partial Neanderthal DNA\"]',\n",
       " '[\"the user did not have sex with that woman\"]',\n",
       " '[\"Trump\\'s tweets do not prove the media is irrelevant\", \"The media loves to cover Trump\\'s tweets\"]',\n",
       " '[\"The only thing Americans care about is paying the lowest taxes possible\", \"Americans do not care about building coastal protections\"]',\n",
       " '[\"Guests on Tucker Carlson\\'s show cannot answer his questions.\"]',\n",
       " '[\"Every terrorist has a mental problem\"]',\n",
       " '[@AndersonCooper is not a real journalist.]',\n",
       " '[\"The linked content is not acceptable\"]',\n",
       " '[\"Money can\\'t buy good looks\", \"Money can\\'t buy honesty\"]',\n",
       " '[\"The author loves Cynthia Alksne\", \"People scream at their TVs while watching Cynthia Alksne\"]',\n",
       " '[\"There should be a sequel to The Big Lebowski with an environmental theme.\"]',\n",
       " '[\"The phone call was not perfect\", \"The released transcript is not real\"]',\n",
       " '[\"Trump will watch Cohen\\'s testimony\", \"Cohen should reveal everything about Trump\"]',\n",
       " '[\"Rep Adam Schiff did not stand by his convictions\"]',\n",
       " '[\"The US involvement in Syria was not primarily about the Kurds or ISIS\", \"The US involvement in Syria was primarily about regime change\"]',\n",
       " '[\"CNN\\'s AC360 is a sore loser\", \"The public does not want CNN\\'s AC360\"]',\n",
       " '[@FaceTheNation interviewed Presidential Candidate U.S. Sen. Michael Bennet]',\n",
       " '[\"No one is talking about impeachment except for the Republican Party and possibly Maxine Waters\"]',\n",
       " '[\"Meet the Press is a worse program than it was when Tim Russert hosted\"]',\n",
       " '[\"The Tucker Carlson show is one of the best to watch\"]',\n",
       " '[\"ABC posted the Nightline episode 12 hours too late\"]',\n",
       " '[\"Ames has more guts and is a better man than US senators or the President\"]',\n",
       " '[\"Something is wrong with some people\"]',\n",
       " '[\"She is a credit to her gender\", \"She is a tough lady\"]',\n",
       " '[\"@SenatorCollins will lose their election\"]',\n",
       " '[\"that would be obstruction of justice\"]',\n",
       " '[\"Chris Matthews interrupts his guests\"]',\n",
       " '[\"People of all colors voted for Trump\"]',\n",
       " '[\"The president\\'s term can\\'t start until January\"]',\n",
       " '[\"The person in question is untrustworthy\", \"The person in question is creepy\"]',\n",
       " '[\"Hassan bhai will educate people in the upcoming episode\", \"India is not on the right path\"]',\n",
       " '[\"Someone should record their thoughts digitally rather than using another method\"]',\n",
       " '[\"Obama, the FBI, and Clinton colluded with each other\"]',\n",
       " '[]',\n",
       " '[Rick Santorum is an idiot]',\n",
       " '[\"A federally funded assault rifle buyback program is needed.\"]',\n",
       " '[\"Careers in software development and computer programming are one of the best fields for women in tech.\"]',\n",
       " '[\"@hasanminhaj inspired me to wear more sweatshirts\"]',\n",
       " '[\"The author\\'s arms got tired watching a video or performance\"]',\n",
       " '[\"Chris Hayes cried all night\"]',\n",
       " '[\"Trevor should discuss the KFC proposal in South Africa on The Daily Show\"]',\n",
       " '[\"Mark Judge should testify.\"]',\n",
       " '[\"The US motto is E Pluribus Unum\", \"Government control can be countered with an independent press\"]',\n",
       " '[\"GOT7 is a funny group\"]',\n",
       " '[\"Some people don\\'t know how to behave in a classy manner at parties\"]',\n",
       " '[\"A new comedy sitcom is coming up\"]',\n",
       " '[\"Tucker Carlson was shocked\", \"People should not have been shocked\"]',\n",
       " '[\"Tucker Carlson is a hypocrite\"]',\n",
       " '[\"Adam Schitt made up information\"]',\n",
       " '[]',\n",
       " '[\"The person being referred to is of very poor character.\"]',\n",
       " '[\"Allowing and protecting hatred will intensify it\"]',\n",
       " '[\"The photo is not a damning picture of Chris Matthews\"]',\n",
       " '[\"Don Jr is like his father\", \"Don Jr and Don Sr are weasels\"]',\n",
       " '[\"The user is tired of One Direction references\", \"One Direction references were sufficient in the first era\"]',\n",
       " '[]',\n",
       " '[\"Going to class is a waste of money.\"]',\n",
       " '[\"The US has no common ground with California\", \"California should secede from the US\"]',\n",
       " '[]',\n",
       " '[\"The people who criticize Biden\\'s age are similar to Trump supporters.\"]',\n",
       " '[\"A fancy word is used to describe Obstruction of Justice\"]',\n",
       " '[]',\n",
       " '[\"Matt Walsh has no chance of impacting Trump or his supporters\"]',\n",
       " '[\"Jeffrey referred to some people as Turks\"]',\n",
       " '[\"Brian was involved in a helicopter crash\"]',\n",
       " '[\"Jerry Springer is comparable to someone who became president\"]',\n",
       " '[\"A tape-recorded conversation relevant to the impeachment exists and should be requested\"]',\n",
       " '[\"Republicans are responsible for a particular issue or event\", \"The speaker believes Republicans should be held accountable by being called out by name\"]',\n",
       " '[\"Switzerland has one of the most liberal gun laws\"]',\n",
       " '[]',\n",
       " '[@BillKristol has not had a recent win]',\n",
       " '[\"US should invade Iraq\"]',\n",
       " '[\"Pierre Delecto is also known as Mitt Romney\"]',\n",
       " '[\"Nunes and Jim Jordan destroyed this sham\"]',\n",
       " '[\"@AC360 and Anderson Cooper is their favorite\"]',\n",
       " '[\"The post by @AC360 is ignorant.\"]',\n",
       " '[]',\n",
       " '[\"Trump lied\", \"Trump used poor people to get elected\"]',\n",
       " '[@HowardKurtz might inform Hannity]',\n",
       " '[\"Someone will be pardoned\"]',\n",
       " '[\"Mexico does more to protect America than The Democrats\"]',\n",
       " '[\"The person in question has good intentions\", \"The person is being used by dishonest people\"]',\n",
       " '[\"John Oliver is not as tall in person as expected\"]',\n",
       " '[\"Tim Morrison testified that there was nothing wrong with the call he listened in on.\"]',\n",
       " '[]',\n",
       " '[\"Trevor Noah and The Daily Show are excellent\"]',\n",
       " '[\"The age is actually 20\"]',\n",
       " '[\"M&M\\'s chocolates are being referred to as poor little bits in relation to Jurassic World\"]',\n",
       " '[\"The author enjoyed the 20-year run of Hardball\", \"The author thought the last episode was great\"]',\n",
       " '[\"Senator Roy Blunt is surprising\"]',\n",
       " '[\"The GOP won\\'t read or attend depositions\", \"The GOP might ignore evidence\"]',\n",
       " '[\"He was with Voldemort\\'s son.\"]',\n",
       " '[]',\n",
       " '[\"CNN\\'s AC360 needs to be shut down by the government\"]',\n",
       " '[\"Someone should have taken action sooner\"]',\n",
       " '[\"John Stewart is missed by some fans of The Daily Show\"]',\n",
       " '[\"@LateNightSeth\\'s content has decreased in quality over time\"]',\n",
       " '[]',\n",
       " '[@patriotact is from Northern California, Journey was frequently played on every radio station there]',\n",
       " '[\"Supporting plant-based meat substitutes can help preserve rainforests by reducing the need for cattle.\"]',\n",
       " '[]',\n",
       " '[\"She is now the same as all other CBS hosts\"]',\n",
       " '[\"Praise at her book signings is negatively affecting her\"]',\n",
       " '[\"@NewsHour\\'s reporting was strange\", \"@NewsHour is otherwise excellent\"]',\n",
       " '[\"There should be more content from Linsey Davis and less from David Muir.\"]',\n",
       " '[\"All US presidents before Trump handled classified information properly\"]',\n",
       " '[\"The person is asking if this is a game of ping pong\"]',\n",
       " '[\"Worth every penny\"]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[\"The concept of hope is audacious.\"]',\n",
       " '[\"Transgender men retain physical characteristics of their assigned sex at birth\"]',\n",
       " '[\"There should be a massive lawsuit against The Daily Show and Eva Longoria for being irresponsible\"]',\n",
       " '[\"Stephen Colbert is no longer funny\"]',\n",
       " '[\"Tucker Carlson and Fox News are awesome\"]',\n",
       " '[]',\n",
       " '[\"Jay-Z thinks kneeling is no longer an effective form of protest\"]',\n",
       " '[\"Stacey Abrams was overlooked by some due to sleeping\"]',\n",
       " '[\"Rebellion against tyrants is obedience to God.\"]',\n",
       " '[\"What Tucker Carlson said is true\"]',\n",
       " '[]',\n",
       " '[\"Tucker Carlson uses Trump as a smoke screen\", \"The guest on Tucker Carlson\\'s show tried to explain something to him but was interrupted\"]',\n",
       " '[\"Betsy Devos bought the education position\", \"Student debt levels are high\"]',\n",
       " '[\"It\\'s currently August\"]',\n",
       " '[\"The White Helmets is a fake NGO\", \"The White Helmets produced and directed propaganda\"]',\n",
       " '[\"@billpeduto should have informed @andersoncooper about his previous retweet\"]',\n",
       " '[\"There is video evidence available\"]',\n",
       " '[]',\n",
       " '[\"Ady Barkan is a hero\"]',\n",
       " '[\"A person should be found and prosecuted\"]',\n",
       " '[\"Chuck Todd\\'s show has pizzazz\"]',\n",
       " '[\"The lady in the video is awesome\"]',\n",
       " '[\"Election of an authoritarian leader in Turkey is undesirable.\"]',\n",
       " '[\"Beto is loved\"]',\n",
       " '[\"Democrats will win in 2020\", \"Mica should be destroyed now\"]',\n",
       " '[\"A recording exists and should be released\"]',\n",
       " '[\"Someone\\'s behavior was due to dementia kicking in.\"]',\n",
       " '[\"Red states are prone to natural disasters\"]',\n",
       " '[\"Trump saying \\'I don\\'t know anything\\' is his most believable statement in weeks.\"]',\n",
       " '[\"John Oliver has a lot to cover on Last Week Tonight\"]',\n",
       " '[\"Trump is destroying the US\"]',\n",
       " '[\"Gulab Jamun wearing glasses is epic\"]',\n",
       " '[\"South Sudan was formed in 2011.\"]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[\"Sackler family should be executed\"]',\n",
       " '[\"Flush immediately\"]',\n",
       " '[\"Germans and Japanese may not think Americans are the \\'Good Guys\\'\", \"Perspective plays a role in determining who is considered good\"]',\n",
       " '[\"Aid agencies are providing a free shuttle service for migrants\"]',\n",
       " '[\"ABC World News only shares personal feelings rather than actual news\"]',\n",
       " '[\"The VP\\'s mother was present\", \"There were other women present who made the VP need his mother\\'s protection\"]',\n",
       " '[\"Someone is accused of accepting Russian money\"]',\n",
       " '[\"The world is laughing at Donald Trump\"]',\n",
       " '[\"Blumenthal is a liar\"]',\n",
       " '[\"Canada does not have democratic socialism like Bernie talks about\", \"Canada does not have free Pharmacare\"]',\n",
       " '[]',\n",
       " '[\"Colbert is the worst night show host\"]',\n",
       " '[\"He believes the person in question is a Republican\"]',\n",
       " '[\"There is no system in place to qualify someone to legally check internal credentials.\"]',\n",
       " '[]',\n",
       " '[\"John Oliver looks better in a suit than in casual clothes.\"]',\n",
       " '[\"Susan Collins\\' husband has Russian connections\", \"Susan Collins is a fraud\"]',\n",
       " '[\"CNN is becoming similar to FOX\"]',\n",
       " '[\"Tucker Carlson\\'s actions are an attempt to boost ratings\"]',\n",
       " '[\"It does not make sense\"]',\n",
       " '[\"Betsy DeVos is unqualified for her job\", \"Trump only hires the best people\"]',\n",
       " '[\"The media is biased in its questioning\", \"Rand Paul is not pressing hard\"]',\n",
       " '[\"Something is unconscionable\"]',\n",
       " '[\"John Oliver is needed to address something on Donald Trump\\'s Twitter feed\"]',\n",
       " '[\"Christopher Columbus discovered Iowa.\"]',\n",
       " '[\"There have been many recalls lately\"]',\n",
       " '[\"The show was entertaining\", \"It\\'s good to have Stephen Colbert back\"]',\n",
       " '[\"The Last Week Tonight show is a reason for keeping an HBO subscription\"]',\n",
       " '[]',\n",
       " '[]',\n",
       " ...]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "claim_df.claims.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "955                                                   []\n",
       "956    [\"Democrats support Dan in 2020\", \"Someone nam...\n",
       "957    [\"There is a recording that has not been relea...\n",
       "958                                  [\"He had dementia\"]\n",
       "959    [\"Only Red States are prone to natural disaste...\n",
       "Name: claim_run1, dtype: object"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_prompt.claim_run1[:960].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change options to low temperature (0,1) and single prediction and compare result:\n",
    "options_low = \"\"\"\n",
    "seed: 42\n",
    "temperature: 0.1\n",
    "num_predict: 1\n",
    "\"\"\"\n",
    "\n",
    "options_low = yaml.safe_load(options_low)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:05,  1.18s/it]\n"
     ]
    }
   ],
   "source": [
    "#test low temperature claims per comment:\n",
    "chunked_result: typing.List[pd.DataFrame] = []\n",
    "for index, row in tqdm.tqdm(dataset_prompt[:5].iterrows()):\n",
    "    try: \n",
    "        chunked_result.append(\n",
    "            pd.DataFrame(\n",
    "                data=[\n",
    "                    requests.post(\n",
    "                        'https://inf.cl.uni-trier.de/',\n",
    "                        json={\n",
    "                            'model': MODEL,\n",
    "                            'system': SYSTEM_claim,\n",
    "                            'prompt': f'The following set of social media posts are replies to a news- or infotainment-post. '\n",
    "                                    + f'Check whether your answer strictly adheres to the specified format. \\n\"Posts\":\\n<{row[\"commentText\"]}>',\n",
    "                            'options': options\n",
    "                            }).json()['response']                       \n",
    "                ],\n",
    "                columns=['claims']\n",
    "            )\n",
    "        )\n",
    "        \n",
    "    except json.JSONDecodeError:\n",
    "        print(\"invalid json response, skipping to next batch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              claims\n",
      "0                                                 []\n",
      "1  [\"Minerals have rights too\", \"A box of rocks w...\n",
      "2                [\"Trump may have been compromised\"]\n",
      "3  [\"The death of free and civil dialogue is evid...\n",
      "4                      [\"Nobody else will hug him.\"]                                                  claims\n",
      "0                                                    []\n",
      "1     [\"Minerals have rights too\", \"A box of rocks w...\n",
      "2     [\"There is a question about whether Trump was ...\n",
      "3     [\"The death of free and civil dialogue is evid...\n",
      "4                         [\"Nobody else will hug him.\"]\n",
      "...                                                 ...\n",
      "3857  [\"They can't afford chemical peels and facelif...\n",
      "3858                           [\"He will be confirmed\"]\n",
      "3859  [\"They knew all about the cameras.\", \"The susp...\n",
      "3860  [\"Troops are waiting in the fields\", \"Men will...\n",
      "3861  [\"Russia and the US are both against ISIS\", \"T...\n",
      "\n",
      "[3862 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "claim_low = pd.concat(chunked_result, ignore_index=True)\n",
    "print(claim_low, claim_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 claims  original_index\n",
      "0                                                    []               0\n",
      "1                                [\"Minerals have rights               1\n",
      "2      \"The post is a vicious insult to boxes of roc...               1\n",
      "3                     [\"Trump may have been compromised               2\n",
      "4      \"The extent of Trump's awareness about his ow...               2\n",
      "...                                                 ...             ...\n",
      "1408                    [\"Democrats support Dan in 2020             956\n",
      "1409          \"Someone named Mica should be destroyed\"]             956\n",
      "1410  [\"There is a recording that has not been relea...             957\n",
      "1411                                [\"He had dementia\"]             958\n",
      "1412  [\"Only Red States are prone to natural disaste...             959\n",
      "\n",
      "[1413 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "#list the claims in a way that links them to the original post, so we can group the claims according to any grouper, while still being able to find unique embeddings per claim: \n",
    "claim_df['original_index'] = claim_df.index\n",
    "claim_df['claims'] = claim_df['claims'].str.split('\",')\n",
    "claim_df = claim_df.explode('claims')\n",
    "claim_df = claim_df.reset_index(drop=True)\n",
    "claim_df = claim_df.drop_duplicates()\n",
    "print(claim_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_prompt.loc[:, \"claim_optdef\"]=claim_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_prompt.to_parquet(f'{CFG.report_dir}/pubsphere.claim.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data:\n",
    "dataset_prompt = pd.read_parquet(f'{CFG.report_dir}/pubsphere.claim.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>StartDate</th>\n",
       "      <th>RecordedDate</th>\n",
       "      <th>IPAddress</th>\n",
       "      <th>Finished</th>\n",
       "      <th>Coder</th>\n",
       "      <th>ID</th>\n",
       "      <th>Mark_ID</th>\n",
       "      <th>Genre</th>\n",
       "      <th>topiccode</th>\n",
       "      <th>Platform</th>\n",
       "      <th>...</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>offensive</th>\n",
       "      <th>topics</th>\n",
       "      <th>emotions</th>\n",
       "      <th>irony</th>\n",
       "      <th>hate</th>\n",
       "      <th>topiccodeSTR</th>\n",
       "      <th>claim_run1</th>\n",
       "      <th>claim_optdef</th>\n",
       "      <th>claim_optdef_embed_MXBAI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5/30/2021 13:03:17</td>\n",
       "      <td>5/30/2021 13:04:17</td>\n",
       "      <td>62.194.51.29</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>UgyPHwv8G0cDE6-wEgl4AaABAg.8_0ZjJKSJty8_0kXGkAd2U</td>\n",
       "      <td>119</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>b'[\"negative\",\"neutral\"]'</td>\n",
       "      <td>{'offensive', 'non-offensive'}</td>\n",
       "      <td>set()</td>\n",
       "      <td>{'sadness', 'pessimism'}</td>\n",
       "      <td>{'non_irony', 'irony'}</td>\n",
       "      <td>{'NOT-HATE'}</td>\n",
       "      <td>YTgen</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[-0.5064548254013062, -0.11242958903312683, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10/11/2021 10:34:05</td>\n",
       "      <td>10/11/2021 10:36:46</td>\n",
       "      <td>213.127.109.191</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>Ugx2WXq9UdV8mPPjejJ4AaABAg.8yHCKV0Boe58yYRxEQEF45</td>\n",
       "      <td>282</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>b'[\"negative\",\"neutral\"]'</td>\n",
       "      <td>{'offensive', 'non-offensive'}</td>\n",
       "      <td>{'news_&amp;_social_concern'}</td>\n",
       "      <td>{'disgust', 'anger'}</td>\n",
       "      <td>{'non_irony'}</td>\n",
       "      <td>{'NOT-HATE'}</td>\n",
       "      <td>YT_Econ</td>\n",
       "      <td>[\"Minerals have rights\", \"The post is a viciou...</td>\n",
       "      <td>[\"Minerals have rights too\", \"A box of rocks w...</td>\n",
       "      <td>[0.07165694236755371, 0.056573204696178436, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9/9/2021 18:49:48</td>\n",
       "      <td>9/9/2021 18:51:32</td>\n",
       "      <td>213.127.110.0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1110578710648890000</td>\n",
       "      <td>372</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>b'[\"negative\",\"neutral\"]'</td>\n",
       "      <td>{'non-offensive'}</td>\n",
       "      <td>{'news_&amp;_social_concern'}</td>\n",
       "      <td>{'anticipation', 'disgust', 'anger'}</td>\n",
       "      <td>{'non_irony'}</td>\n",
       "      <td>{'NOT-HATE'}</td>\n",
       "      <td>Twitgen</td>\n",
       "      <td>[\"Trump may have been compromised\", \"The exten...</td>\n",
       "      <td>[\"There is a question about whether Trump was ...</td>\n",
       "      <td>[0.7152484059333801, -0.363998144865036, -0.46...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6/6/2021 16:12:46</td>\n",
       "      <td>6/6/2021 16:16:16</td>\n",
       "      <td>213.127.76.145</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>UgwUPFScjJ0MCeaP2F54AaABAg.8lvp3fc9Euf8lvvgsUgEgV</td>\n",
       "      <td>769</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>b'[\"negative\",\"neutral\"]'</td>\n",
       "      <td>{'non-offensive'}</td>\n",
       "      <td>{'news_&amp;_social_concern'}</td>\n",
       "      <td>{'disgust'}</td>\n",
       "      <td>{'irony'}</td>\n",
       "      <td>{'NOT-HATE'}</td>\n",
       "      <td>YTgen</td>\n",
       "      <td>[\"There are only a few people left who value c...</td>\n",
       "      <td>[\"The death of free and civil dialogue is evid...</td>\n",
       "      <td>[-0.19108964502811432, -0.04326176643371582, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6/13/2021 13:25:49</td>\n",
       "      <td>6/13/2021 13:27:28</td>\n",
       "      <td>213.127.82.232</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>UgwWKCWtSJdFvjGHvTp4AaABAg.8kUC5dGrQ2H8kUDRihE2f3</td>\n",
       "      <td>1206</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>b'[\"positive\",\"neutral\"]'</td>\n",
       "      <td>{'non-offensive'}</td>\n",
       "      <td>{'diaries_&amp;_daily_life'}</td>\n",
       "      <td>{'sadness'}</td>\n",
       "      <td>{'non_irony', 'irony'}</td>\n",
       "      <td>{'NOT-HATE'}</td>\n",
       "      <td>YTgen</td>\n",
       "      <td>[\"Nobody wants to hug him\"]</td>\n",
       "      <td>[\"Nobody else will hug him.\"]</td>\n",
       "      <td>[0.4981515407562256, -0.1532106101512909, 0.31...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 96 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             StartDate         RecordedDate        IPAddress  Finished  Coder  \\\n",
       "0   5/30/2021 13:03:17   5/30/2021 13:04:17     62.194.51.29         1      6   \n",
       "1  10/11/2021 10:34:05  10/11/2021 10:36:46  213.127.109.191         1      6   \n",
       "2    9/9/2021 18:49:48    9/9/2021 18:51:32    213.127.110.0         1      6   \n",
       "3    6/6/2021 16:12:46    6/6/2021 16:16:16   213.127.76.145         1      6   \n",
       "4   6/13/2021 13:25:49   6/13/2021 13:27:28   213.127.82.232         1      6   \n",
       "\n",
       "                                                  ID  Mark_ID  Genre  \\\n",
       "0  UgyPHwv8G0cDE6-wEgl4AaABAg.8_0ZjJKSJty8_0kXGkAd2U      119      0   \n",
       "1  Ugx2WXq9UdV8mPPjejJ4AaABAg.8yHCKV0Boe58yYRxEQEF45      282      1   \n",
       "2                                1110578710648890000      372      2   \n",
       "3  UgwUPFScjJ0MCeaP2F54AaABAg.8lvp3fc9Euf8lvvgsUgEgV      769      0   \n",
       "4  UgwWKCWtSJdFvjGHvTp4AaABAg.8kUC5dGrQ2H8kUDRihE2f3     1206      0   \n",
       "\n",
       "   topiccode  Platform  ...                  sentiment  \\\n",
       "0          0         1  ...  b'[\"negative\",\"neutral\"]'   \n",
       "1          2         1  ...  b'[\"negative\",\"neutral\"]'   \n",
       "2          4         2  ...  b'[\"negative\",\"neutral\"]'   \n",
       "3          0         1  ...  b'[\"negative\",\"neutral\"]'   \n",
       "4          0         1  ...  b'[\"positive\",\"neutral\"]'   \n",
       "\n",
       "                        offensive                     topics  \\\n",
       "0  {'offensive', 'non-offensive'}                      set()   \n",
       "1  {'offensive', 'non-offensive'}  {'news_&_social_concern'}   \n",
       "2               {'non-offensive'}  {'news_&_social_concern'}   \n",
       "3               {'non-offensive'}  {'news_&_social_concern'}   \n",
       "4               {'non-offensive'}   {'diaries_&_daily_life'}   \n",
       "\n",
       "                               emotions                   irony          hate  \\\n",
       "0              {'sadness', 'pessimism'}  {'non_irony', 'irony'}  {'NOT-HATE'}   \n",
       "1                  {'disgust', 'anger'}           {'non_irony'}  {'NOT-HATE'}   \n",
       "2  {'anticipation', 'disgust', 'anger'}           {'non_irony'}  {'NOT-HATE'}   \n",
       "3                           {'disgust'}               {'irony'}  {'NOT-HATE'}   \n",
       "4                           {'sadness'}  {'non_irony', 'irony'}  {'NOT-HATE'}   \n",
       "\n",
       "   topiccodeSTR                                         claim_run1  \\\n",
       "0         YTgen                                                 []   \n",
       "1       YT_Econ  [\"Minerals have rights\", \"The post is a viciou...   \n",
       "2       Twitgen  [\"Trump may have been compromised\", \"The exten...   \n",
       "3         YTgen  [\"There are only a few people left who value c...   \n",
       "4         YTgen                        [\"Nobody wants to hug him\"]   \n",
       "\n",
       "                                        claim_optdef  \\\n",
       "0                                                 []   \n",
       "1  [\"Minerals have rights too\", \"A box of rocks w...   \n",
       "2  [\"There is a question about whether Trump was ...   \n",
       "3  [\"The death of free and civil dialogue is evid...   \n",
       "4                      [\"Nobody else will hug him.\"]   \n",
       "\n",
       "                            claim_optdef_embed_MXBAI  \n",
       "0  [-0.5064548254013062, -0.11242958903312683, 0....  \n",
       "1  [0.07165694236755371, 0.056573204696178436, 0....  \n",
       "2  [0.7152484059333801, -0.363998144865036, -0.46...  \n",
       "3  [-0.19108964502811432, -0.04326176643371582, -...  \n",
       "4  [0.4981515407562256, -0.1532106101512909, 0.31...  \n",
       "\n",
       "[5 rows x 96 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_prompt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3862/3862 [14:04<00:00,  4.57it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0       [0.3085225224494934, 0.0025453902781009674, -0...\n",
       "1       [0.3428829610347748, 0.03502892702817917, -0.5...\n",
       "2       [0.5012139081954956, -0.1079118549823761, -0.6...\n",
       "3       [0.21832342445850372, -0.13364076614379883, -0...\n",
       "4       [0.49281346797943115, -0.11275702714920044, -0...\n",
       "                              ...                        \n",
       "3857    [0.5127694606781006, -0.30040407180786133, -0....\n",
       "3858    [0.5046452283859253, -0.13685280084609985, -0....\n",
       "3859    [0.5013481378555298, -0.34943655133247375, -0....\n",
       "3860    [0.4138392210006714, -0.20430096983909607, -0....\n",
       "3861    [0.3057856261730194, -0.03770238161087036, -0....\n",
       "Name: claim_optdef_embed_MXBAI, Length: 3862, dtype: object"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get MXBAI embeddings for the claims, but ordered per post, so one embedding for the set of claims for each post:\n",
    "context = 'claims made in social media replies to a news- or infotainment-post'\n",
    "\n",
    "claim_optdef_embed_MXBAI: typing.Dict[str, np.ndarray] = (\n",
    "    pd.Series(src.SentenceEmbedder()(dataset_prompt[\"claim_optdef\"], prefix='You help me get embeddings for a sentence. I provide you with a context and a sentence and you reply only with that exact sentence. Context = ' + context + '; Sentence: ' ), name=\"claim_optdef_embed_MXBAI\")\n",
    "    .rename_axis(index=dataset_prompt.index.names)\n",
    ")\n",
    "claim_optdef_embed_MXBAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset_prompt.loc[:,'claim_optdef_embed_MXBAI'] = claim_optdef_embed_MXBAI\n",
    "dataset_prompt.to_parquet(f'{CFG.report_dir}/pubsphere.claim_embed.parquet')\n",
    "#dataset_prompt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [-0.5064548254013062, -0.11242958903312683, 0....\n",
       "1       [0.07165694236755371, 0.056573204696178436, 0....\n",
       "2       [0.7152484059333801, -0.363998144865036, -0.46...\n",
       "3       [-0.19108964502811432, -0.04326176643371582, -...\n",
       "4       [0.4981515407562256, -0.1532106101512909, 0.31...\n",
       "                              ...                        \n",
       "3857    [0.5195960402488708, -0.3683280944824219, -0.4...\n",
       "3858    [0.24329741299152374, -0.7627171874046326, 0.2...\n",
       "3859    [0.6211866736412048, -0.27806738018989563, -0....\n",
       "3860    [-0.26269903779029846, -0.3088279068470001, 0....\n",
       "3861    [-0.16621819138526917, -0.10346470773220062, -...\n",
       "Name: claim_optdef_embed_MXBAI, Length: 3862, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "claim_optdef_embed_MXBAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>StartDate</th>\n",
       "      <th>RecordedDate</th>\n",
       "      <th>IPAddress</th>\n",
       "      <th>Finished</th>\n",
       "      <th>Coder</th>\n",
       "      <th>ID</th>\n",
       "      <th>Mark_ID</th>\n",
       "      <th>Genre</th>\n",
       "      <th>topiccode</th>\n",
       "      <th>Platform</th>\n",
       "      <th>...</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>offensive</th>\n",
       "      <th>topics</th>\n",
       "      <th>emotions</th>\n",
       "      <th>irony</th>\n",
       "      <th>hate</th>\n",
       "      <th>topiccodeSTR</th>\n",
       "      <th>claim_run1</th>\n",
       "      <th>claim_optdef</th>\n",
       "      <th>claim_optdef_embed_MXBAI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5/30/2021 13:03:17</td>\n",
       "      <td>5/30/2021 13:04:17</td>\n",
       "      <td>62.194.51.29</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>UgyPHwv8G0cDE6-wEgl4AaABAg.8_0ZjJKSJty8_0kXGkAd2U</td>\n",
       "      <td>119</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>b'[\"negative\",\"neutral\"]'</td>\n",
       "      <td>{'offensive', 'non-offensive'}</td>\n",
       "      <td>set()</td>\n",
       "      <td>{'sadness', 'pessimism'}</td>\n",
       "      <td>{'non_irony', 'irony'}</td>\n",
       "      <td>{'NOT-HATE'}</td>\n",
       "      <td>YTgen</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[-0.5064548254013062, -0.11242958903312683, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10/11/2021 10:34:05</td>\n",
       "      <td>10/11/2021 10:36:46</td>\n",
       "      <td>213.127.109.191</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>Ugx2WXq9UdV8mPPjejJ4AaABAg.8yHCKV0Boe58yYRxEQEF45</td>\n",
       "      <td>282</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>b'[\"negative\",\"neutral\"]'</td>\n",
       "      <td>{'offensive', 'non-offensive'}</td>\n",
       "      <td>{'news_&amp;_social_concern'}</td>\n",
       "      <td>{'disgust', 'anger'}</td>\n",
       "      <td>{'non_irony'}</td>\n",
       "      <td>{'NOT-HATE'}</td>\n",
       "      <td>YT_Econ</td>\n",
       "      <td>[\"Minerals have rights\", \"The post is a viciou...</td>\n",
       "      <td>[\"Minerals have rights too\", \"A box of rocks w...</td>\n",
       "      <td>[0.07165694236755371, 0.056573204696178436, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9/9/2021 18:49:48</td>\n",
       "      <td>9/9/2021 18:51:32</td>\n",
       "      <td>213.127.110.0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1110578710648890000</td>\n",
       "      <td>372</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>b'[\"negative\",\"neutral\"]'</td>\n",
       "      <td>{'non-offensive'}</td>\n",
       "      <td>{'news_&amp;_social_concern'}</td>\n",
       "      <td>{'anticipation', 'disgust', 'anger'}</td>\n",
       "      <td>{'non_irony'}</td>\n",
       "      <td>{'NOT-HATE'}</td>\n",
       "      <td>Twitgen</td>\n",
       "      <td>[\"Trump may have been compromised\", \"The exten...</td>\n",
       "      <td>[\"There is a question about whether Trump was ...</td>\n",
       "      <td>[0.7152484059333801, -0.363998144865036, -0.46...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6/6/2021 16:12:46</td>\n",
       "      <td>6/6/2021 16:16:16</td>\n",
       "      <td>213.127.76.145</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>UgwUPFScjJ0MCeaP2F54AaABAg.8lvp3fc9Euf8lvvgsUgEgV</td>\n",
       "      <td>769</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>b'[\"negative\",\"neutral\"]'</td>\n",
       "      <td>{'non-offensive'}</td>\n",
       "      <td>{'news_&amp;_social_concern'}</td>\n",
       "      <td>{'disgust'}</td>\n",
       "      <td>{'irony'}</td>\n",
       "      <td>{'NOT-HATE'}</td>\n",
       "      <td>YTgen</td>\n",
       "      <td>[\"There are only a few people left who value c...</td>\n",
       "      <td>[\"The death of free and civil dialogue is evid...</td>\n",
       "      <td>[-0.19108964502811432, -0.04326176643371582, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6/13/2021 13:25:49</td>\n",
       "      <td>6/13/2021 13:27:28</td>\n",
       "      <td>213.127.82.232</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>UgwWKCWtSJdFvjGHvTp4AaABAg.8kUC5dGrQ2H8kUDRihE2f3</td>\n",
       "      <td>1206</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>b'[\"positive\",\"neutral\"]'</td>\n",
       "      <td>{'non-offensive'}</td>\n",
       "      <td>{'diaries_&amp;_daily_life'}</td>\n",
       "      <td>{'sadness'}</td>\n",
       "      <td>{'non_irony', 'irony'}</td>\n",
       "      <td>{'NOT-HATE'}</td>\n",
       "      <td>YTgen</td>\n",
       "      <td>[\"Nobody wants to hug him\"]</td>\n",
       "      <td>[\"Nobody else will hug him.\"]</td>\n",
       "      <td>[0.4981515407562256, -0.1532106101512909, 0.31...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 96 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             StartDate         RecordedDate        IPAddress  Finished  Coder  \\\n",
       "0   5/30/2021 13:03:17   5/30/2021 13:04:17     62.194.51.29         1      6   \n",
       "1  10/11/2021 10:34:05  10/11/2021 10:36:46  213.127.109.191         1      6   \n",
       "2    9/9/2021 18:49:48    9/9/2021 18:51:32    213.127.110.0         1      6   \n",
       "3    6/6/2021 16:12:46    6/6/2021 16:16:16   213.127.76.145         1      6   \n",
       "4   6/13/2021 13:25:49   6/13/2021 13:27:28   213.127.82.232         1      6   \n",
       "\n",
       "                                                  ID  Mark_ID  Genre  \\\n",
       "0  UgyPHwv8G0cDE6-wEgl4AaABAg.8_0ZjJKSJty8_0kXGkAd2U      119      0   \n",
       "1  Ugx2WXq9UdV8mPPjejJ4AaABAg.8yHCKV0Boe58yYRxEQEF45      282      1   \n",
       "2                                1110578710648890000      372      2   \n",
       "3  UgwUPFScjJ0MCeaP2F54AaABAg.8lvp3fc9Euf8lvvgsUgEgV      769      0   \n",
       "4  UgwWKCWtSJdFvjGHvTp4AaABAg.8kUC5dGrQ2H8kUDRihE2f3     1206      0   \n",
       "\n",
       "   topiccode  Platform  ...                  sentiment  \\\n",
       "0          0         1  ...  b'[\"negative\",\"neutral\"]'   \n",
       "1          2         1  ...  b'[\"negative\",\"neutral\"]'   \n",
       "2          4         2  ...  b'[\"negative\",\"neutral\"]'   \n",
       "3          0         1  ...  b'[\"negative\",\"neutral\"]'   \n",
       "4          0         1  ...  b'[\"positive\",\"neutral\"]'   \n",
       "\n",
       "                        offensive                     topics  \\\n",
       "0  {'offensive', 'non-offensive'}                      set()   \n",
       "1  {'offensive', 'non-offensive'}  {'news_&_social_concern'}   \n",
       "2               {'non-offensive'}  {'news_&_social_concern'}   \n",
       "3               {'non-offensive'}  {'news_&_social_concern'}   \n",
       "4               {'non-offensive'}   {'diaries_&_daily_life'}   \n",
       "\n",
       "                               emotions                   irony          hate  \\\n",
       "0              {'sadness', 'pessimism'}  {'non_irony', 'irony'}  {'NOT-HATE'}   \n",
       "1                  {'disgust', 'anger'}           {'non_irony'}  {'NOT-HATE'}   \n",
       "2  {'anticipation', 'disgust', 'anger'}           {'non_irony'}  {'NOT-HATE'}   \n",
       "3                           {'disgust'}               {'irony'}  {'NOT-HATE'}   \n",
       "4                           {'sadness'}  {'non_irony', 'irony'}  {'NOT-HATE'}   \n",
       "\n",
       "   topiccodeSTR                                         claim_run1  \\\n",
       "0         YTgen                                                 []   \n",
       "1       YT_Econ  [\"Minerals have rights\", \"The post is a viciou...   \n",
       "2       Twitgen  [\"Trump may have been compromised\", \"The exten...   \n",
       "3         YTgen  [\"There are only a few people left who value c...   \n",
       "4         YTgen                        [\"Nobody wants to hug him\"]   \n",
       "\n",
       "                                        claim_optdef  \\\n",
       "0                                                 []   \n",
       "1  [\"Minerals have rights too\", \"A box of rocks w...   \n",
       "2  [\"There is a question about whether Trump was ...   \n",
       "3  [\"The death of free and civil dialogue is evid...   \n",
       "4                      [\"Nobody else will hug him.\"]   \n",
       "\n",
       "                            claim_optdef_embed_MXBAI  \n",
       "0  [-0.5064548254013062, -0.11242958903312683, 0....  \n",
       "1  [0.07165694236755371, 0.056573204696178436, 0....  \n",
       "2  [0.7152484059333801, -0.363998144865036, -0.46...  \n",
       "3  [-0.19108964502811432, -0.04326176643371582, -...  \n",
       "4  [0.4981515407562256, -0.1532106101512909, 0.31...  \n",
       "\n",
       "[5 rows x 96 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_prompt = dataset_prompt.join(claim_optdef_embed_MXBAI)\n",
    "dataset_prompt.to_parquet(f'{CFG.report_dir}/pubsphere.claim_embed.parquet')\n",
    "dataset_prompt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>StartDate</th>\n",
       "      <th>RecordedDate</th>\n",
       "      <th>IPAddress</th>\n",
       "      <th>Finished</th>\n",
       "      <th>Coder</th>\n",
       "      <th>ID</th>\n",
       "      <th>Mark_ID</th>\n",
       "      <th>Genre</th>\n",
       "      <th>topiccode</th>\n",
       "      <th>Platform</th>\n",
       "      <th>...</th>\n",
       "      <th>irony</th>\n",
       "      <th>hate</th>\n",
       "      <th>topiccodeSTR</th>\n",
       "      <th>claim_run1</th>\n",
       "      <th>claim_optdef</th>\n",
       "      <th>claim_optdef_embed_MXBAI</th>\n",
       "      <th>claim_optlow</th>\n",
       "      <th>claim_optlow_MXBAI</th>\n",
       "      <th>tfidf_embed_post</th>\n",
       "      <th>embed_MXBAI_post</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5/30/2021 13:03:17</td>\n",
       "      <td>5/30/2021 13:04:17</td>\n",
       "      <td>62.194.51.29</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>UgyPHwv8G0cDE6-wEgl4AaABAg.8_0ZjJKSJty8_0kXGkAd2U</td>\n",
       "      <td>119</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>{'non_irony', 'irony'}</td>\n",
       "      <td>{'NOT-HATE'}</td>\n",
       "      <td>YTgen</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[0.3085225224494934, 0.0025453902781009674, -0...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[0.3085225224494934, 0.0025453902781009674, -0...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.40566757321357727, -0.032418690621852875, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10/11/2021 10:34:05</td>\n",
       "      <td>10/11/2021 10:36:46</td>\n",
       "      <td>213.127.109.191</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>Ugx2WXq9UdV8mPPjejJ4AaABAg.8yHCKV0Boe58yYRxEQEF45</td>\n",
       "      <td>282</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>{'non_irony'}</td>\n",
       "      <td>{'NOT-HATE'}</td>\n",
       "      <td>YT_Econ</td>\n",
       "      <td>[\"Minerals have rights\", \"The post is a viciou...</td>\n",
       "      <td>[\"Minerals have rights too\", \"A box of rocks w...</td>\n",
       "      <td>[0.3428829610347748, 0.03502892702817917, -0.5...</td>\n",
       "      <td>[\"Minerals have rights too.\"]</td>\n",
       "      <td>[0.34888750314712524, -0.01314709335565567, -0...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.49942252039909363, -0.22240903973579407, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9/9/2021 18:49:48</td>\n",
       "      <td>9/9/2021 18:51:32</td>\n",
       "      <td>213.127.110.0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1110578710648890000</td>\n",
       "      <td>372</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>{'non_irony'}</td>\n",
       "      <td>{'NOT-HATE'}</td>\n",
       "      <td>Twitgen</td>\n",
       "      <td>[\"Trump may have been compromised\", \"The exten...</td>\n",
       "      <td>[\"There is a question about whether Trump was ...</td>\n",
       "      <td>[0.5012139081954956, -0.1079118549823761, -0.6...</td>\n",
       "      <td>[\"The question has always been if Trump was co...</td>\n",
       "      <td>[0.4851573407649994, -0.06400042772293091, -0....</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.6132397651672363, -0.3024018704891205, -0.6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6/6/2021 16:12:46</td>\n",
       "      <td>6/6/2021 16:16:16</td>\n",
       "      <td>213.127.76.145</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>UgwUPFScjJ0MCeaP2F54AaABAg.8lvp3fc9Euf8lvvgsUgEgV</td>\n",
       "      <td>769</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>{'irony'}</td>\n",
       "      <td>{'NOT-HATE'}</td>\n",
       "      <td>YTgen</td>\n",
       "      <td>[\"There are only a few people left who value c...</td>\n",
       "      <td>[\"The death of free and civil dialogue is evid...</td>\n",
       "      <td>[0.21832342445850372, -0.13364076614379883, -0...</td>\n",
       "      <td>[\"There are only a few people left who value f...</td>\n",
       "      <td>[0.31287914514541626, -0.15600241720676422, -0...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.27884048223495483, -0.3008716404438019, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6/13/2021 13:25:49</td>\n",
       "      <td>6/13/2021 13:27:28</td>\n",
       "      <td>213.127.82.232</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>UgwWKCWtSJdFvjGHvTp4AaABAg.8kUC5dGrQ2H8kUDRihE2f3</td>\n",
       "      <td>1206</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>{'non_irony', 'irony'}</td>\n",
       "      <td>{'NOT-HATE'}</td>\n",
       "      <td>YTgen</td>\n",
       "      <td>[\"Nobody wants to hug him\"]</td>\n",
       "      <td>[\"Nobody else will hug him.\"]</td>\n",
       "      <td>[0.49281346797943115, -0.11275702714920044, -0...</td>\n",
       "      <td>[\"No one will hug him\"]</td>\n",
       "      <td>[0.5331375598907471, -0.04385572671890259, -0....</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.5210463404655457, 0.03200243413448334, -0.3...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             StartDate         RecordedDate        IPAddress  Finished  Coder  \\\n",
       "0   5/30/2021 13:03:17   5/30/2021 13:04:17     62.194.51.29         1      6   \n",
       "1  10/11/2021 10:34:05  10/11/2021 10:36:46  213.127.109.191         1      6   \n",
       "2    9/9/2021 18:49:48    9/9/2021 18:51:32    213.127.110.0         1      6   \n",
       "3    6/6/2021 16:12:46    6/6/2021 16:16:16   213.127.76.145         1      6   \n",
       "4   6/13/2021 13:25:49   6/13/2021 13:27:28   213.127.82.232         1      6   \n",
       "\n",
       "                                                  ID  Mark_ID  Genre  \\\n",
       "0  UgyPHwv8G0cDE6-wEgl4AaABAg.8_0ZjJKSJty8_0kXGkAd2U      119      0   \n",
       "1  Ugx2WXq9UdV8mPPjejJ4AaABAg.8yHCKV0Boe58yYRxEQEF45      282      1   \n",
       "2                                1110578710648890000      372      2   \n",
       "3  UgwUPFScjJ0MCeaP2F54AaABAg.8lvp3fc9Euf8lvvgsUgEgV      769      0   \n",
       "4  UgwWKCWtSJdFvjGHvTp4AaABAg.8kUC5dGrQ2H8kUDRihE2f3     1206      0   \n",
       "\n",
       "   topiccode  Platform  ...                   irony          hate  \\\n",
       "0          0         1  ...  {'non_irony', 'irony'}  {'NOT-HATE'}   \n",
       "1          2         1  ...           {'non_irony'}  {'NOT-HATE'}   \n",
       "2          4         2  ...           {'non_irony'}  {'NOT-HATE'}   \n",
       "3          0         1  ...               {'irony'}  {'NOT-HATE'}   \n",
       "4          0         1  ...  {'non_irony', 'irony'}  {'NOT-HATE'}   \n",
       "\n",
       "   topiccodeSTR                                         claim_run1  \\\n",
       "0         YTgen                                                 []   \n",
       "1       YT_Econ  [\"Minerals have rights\", \"The post is a viciou...   \n",
       "2       Twitgen  [\"Trump may have been compromised\", \"The exten...   \n",
       "3         YTgen  [\"There are only a few people left who value c...   \n",
       "4         YTgen                        [\"Nobody wants to hug him\"]   \n",
       "\n",
       "                                        claim_optdef  \\\n",
       "0                                                 []   \n",
       "1  [\"Minerals have rights too\", \"A box of rocks w...   \n",
       "2  [\"There is a question about whether Trump was ...   \n",
       "3  [\"The death of free and civil dialogue is evid...   \n",
       "4                      [\"Nobody else will hug him.\"]   \n",
       "\n",
       "                            claim_optdef_embed_MXBAI  \\\n",
       "0  [0.3085225224494934, 0.0025453902781009674, -0...   \n",
       "1  [0.3428829610347748, 0.03502892702817917, -0.5...   \n",
       "2  [0.5012139081954956, -0.1079118549823761, -0.6...   \n",
       "3  [0.21832342445850372, -0.13364076614379883, -0...   \n",
       "4  [0.49281346797943115, -0.11275702714920044, -0...   \n",
       "\n",
       "                                        claim_optlow  \\\n",
       "0                                                 []   \n",
       "1                      [\"Minerals have rights too.\"]   \n",
       "2  [\"The question has always been if Trump was co...   \n",
       "3  [\"There are only a few people left who value f...   \n",
       "4                            [\"No one will hug him\"]   \n",
       "\n",
       "                                  claim_optlow_MXBAI  \\\n",
       "0  [0.3085225224494934, 0.0025453902781009674, -0...   \n",
       "1  [0.34888750314712524, -0.01314709335565567, -0...   \n",
       "2  [0.4851573407649994, -0.06400042772293091, -0....   \n",
       "3  [0.31287914514541626, -0.15600241720676422, -0...   \n",
       "4  [0.5331375598907471, -0.04385572671890259, -0....   \n",
       "\n",
       "                                    tfidf_embed_post  \\\n",
       "0  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "1  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "2  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "4  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "\n",
       "                                    embed_MXBAI_post  \n",
       "0  [0.40566757321357727, -0.032418690621852875, -...  \n",
       "1  [0.49942252039909363, -0.22240903973579407, -0...  \n",
       "2  [0.6132397651672363, -0.3024018704891205, -0.6...  \n",
       "3  [0.27884048223495483, -0.3008716404438019, -0....  \n",
       "4  [0.5210463404655457, 0.03200243413448334, -0.3...  \n",
       "\n",
       "[5 rows x 100 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load data:\n",
    "dataset_prompt = pd.read_parquet(f'{CFG.report_dir}/pubsphere.claim_embed.parquet')\n",
    "dataset_prompt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [02:10<?, ?it/s]\n",
      "  0%|          | 0/2 [02:10<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 17\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m      9\u001b[0m     (model_1, model_2) \u001b[38;5;129;01min\u001b[39;00m resultsplatform\u001b[38;5;241m.\u001b[39mkeys() \u001b[38;5;129;01mor\u001b[39;00m \n\u001b[0;32m     10\u001b[0m     (model_2, model_1) \u001b[38;5;129;01min\u001b[39;00m resultsplatform\u001b[38;5;241m.\u001b[39mkeys()\n\u001b[0;32m     11\u001b[0m ):\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m     14\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m([\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28msum\u001b[39m(dist(\n\u001b[0;32m     16\u001b[0m         torch\u001b[38;5;241m.\u001b[39mtensor(np\u001b[38;5;241m.\u001b[39marray(v_1)), \n\u001b[1;32m---> 17\u001b[0m         \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mc_2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m         )) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(c_2)\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m v_1 \u001b[38;5;129;01min\u001b[39;00m c_1\n\u001b[0;32m     20\u001b[0m ]) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(c_1)\n\u001b[0;32m     22\u001b[0m resultsplatform[(model_1, model_2)] \u001b[38;5;241m=\u001b[39m res\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_1\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_2\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mres\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#do we find differences between platforms, topics and genres in the claims made, based on the embeddings of the claims?\n",
    "grouped_data = dataset_prompt.groupby(\"Platform\")\n",
    "dist = torch.nn.PairwiseDistance()\n",
    "resultsplatform: typing.Dict[typing.Tuple[str, str], float] = {}\n",
    "for model_1, c_1 in tqdm.tqdm(grouped_data['claim_optdef_embed_MXBAI'], total=grouped_data.ngroups):\n",
    "    for model_2, c_2 in tqdm.tqdm(grouped_data['claim_optdef_embed_MXBAI'], total=grouped_data.ngroups):\n",
    "\n",
    "        if (\n",
    "            (model_1, model_2) in resultsplatform.keys() or \n",
    "            (model_2, model_1) in resultsplatform.keys()\n",
    "        ):\n",
    "            continue\n",
    "\n",
    "        res = sum([\n",
    "            sum(dist(\n",
    "                torch.tensor(np.array(v_1)), \n",
    "                torch.tensor(np.array(c_2.tolist()))\n",
    "                )) / len(c_2)\n",
    "            for v_1 in c_1\n",
    "        ]) / len(c_1)\n",
    "\n",
    "        resultsplatform[(model_1, model_2)] = res\n",
    "\n",
    "        print(f'{model_1}:{model_2}:{res.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:0:17.067672729492188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:1:17.221986770629883\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:2:17.286460876464844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:3:17.40349006652832\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [10:15<00:00, 123.15s/it]\n",
      " 20%|██        | 1/5 [10:15<41:03, 615.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:4:17.1921443939209\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1:1:16.982637405395508\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1:2:17.33829116821289\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1:3:17.390308380126953\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [13:07<00:00, 157.58s/it]\n",
      " 40%|████      | 2/5 [23:23<35:51, 717.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1:4:17.308979034423828\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2:2:17.387340545654297\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2:3:17.504169464111328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [10:53<00:00, 130.69s/it]\n",
      " 60%|██████    | 3/5 [34:17<22:55, 688.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2:4:17.38704490661621\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3:3:17.374549865722656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [04:31<00:00, 54.20s/it]\n",
      " 80%|████████  | 4/5 [38:48<08:43, 523.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3:4:17.5199031829834\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [03:08<00:00, 37.64s/it]\n",
      "100%|██████████| 5/5 [41:56<00:00, 503.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4:4:17.18415069580078\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "grouped_data = dataset_prompt.groupby(\"topiccode\")\n",
    "dist = torch.nn.PairwiseDistance()\n",
    "resultstopic: typing.Dict[typing.Tuple[str, str], float] = {}\n",
    "for model_1, c_1 in tqdm.tqdm(grouped_data['claim_optdef_embed_MXBAI'], total=grouped_data.ngroups):\n",
    "    for model_2, c_2 in tqdm.tqdm(grouped_data['claim_optdef_embed_MXBAI'], total=grouped_data.ngroups):\n",
    "\n",
    "        if (\n",
    "            (model_1, model_2) in resultstopic.keys() or \n",
    "            (model_2, model_1) in resultstopic.keys()\n",
    "        ):\n",
    "            continue\n",
    "\n",
    "        res = sum([\n",
    "            sum(dist(\n",
    "                torch.tensor(np.array(v_1)), \n",
    "                torch.tensor(np.array(c_2.tolist()))\n",
    "                )) / len(c_2)\n",
    "            for v_1 in c_1\n",
    "        ]) / len(c_1)\n",
    "\n",
    "        resultstopic[(model_1, model_2)] = res\n",
    "\n",
    "        print(f'{model_1}:{model_2}:{res.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_data = dataset_prompt.groupby(\"Genre\")\n",
    "dist = torch.nn.PairwiseDistance()\n",
    "resultstopic: typing.Dict[typing.Tuple[str, str], float] = {}\n",
    "for model_1, c_1 in tqdm.tqdm(grouped_data['claim_optdef_embed_MXBAI'], total=grouped_data.ngroups):\n",
    "    for model_2, c_2 in tqdm.tqdm(grouped_data['claim_optdef_embed_MXBAI'], total=grouped_data.ngroups):\n",
    "\n",
    "        if (\n",
    "            (model_1, model_2) in resultstopic.keys() or \n",
    "            (model_2, model_1) in resultstopic.keys()\n",
    "        ):\n",
    "            continue\n",
    "\n",
    "        res = sum([\n",
    "            sum(dist(\n",
    "                torch.tensor(np.array(v_1)), \n",
    "                torch.tensor(np.array(c_2.tolist()))\n",
    "                )) / len(c_2)\n",
    "            for v_1 in c_1\n",
    "        ]) / len(c_1)\n",
    "\n",
    "        resultstopic[(model_1, model_2)] = res\n",
    "\n",
    "        print(f'{model_1}:{model_2}:{res.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#does the result differ if we calculate embeddings per claim instead of all claims per post?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#does it matter if we add context to the embedding generation.. we should to remain identical to the post MXBAI embedding generation process..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DefiningDebateQuality",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
