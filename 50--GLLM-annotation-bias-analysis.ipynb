{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4374869",
   "metadata": {},
   "source": [
    "This notebook compares annotations using different GLLMs with codebooks based prompts of Boukes 2024, Jaidka 2022 and Naab 2025 on their respective datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a8edc34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.0.2 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"c:\\Users\\sstolwi\\Github\\llmdiv\\Lib\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"c:\\Users\\sstolwi\\Github\\llmdiv\\Lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"c:\\Users\\sstolwi\\Github\\llmdiv\\Lib\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"c:\\Users\\sstolwi\\Github\\llmdiv\\Lib\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\\Lib\\asyncio\\base_events.py\", line 645, in run_forever\n",
      "    self._run_once()\n",
      "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\\Lib\\asyncio\\base_events.py\", line 1999, in _run_once\n",
      "    handle._run()\n",
      "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\\Lib\\asyncio\\events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"c:\\Users\\sstolwi\\Github\\llmdiv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"c:\\Users\\sstolwi\\Github\\llmdiv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"c:\\Users\\sstolwi\\Github\\llmdiv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"c:\\Users\\sstolwi\\Github\\llmdiv\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"c:\\Users\\sstolwi\\Github\\llmdiv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"c:\\Users\\sstolwi\\Github\\llmdiv\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"c:\\Users\\sstolwi\\Github\\llmdiv\\Lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"c:\\Users\\sstolwi\\Github\\llmdiv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"c:\\Users\\sstolwi\\Github\\llmdiv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"c:\\Users\\sstolwi\\Github\\llmdiv\\Lib\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"c:\\Users\\sstolwi\\Github\\llmdiv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"c:\\Users\\sstolwi\\Github\\llmdiv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"c:\\Users\\sstolwi\\Github\\llmdiv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\sstolwi\\AppData\\Local\\Temp\\ipykernel_16724\\508195922.py\", line 16, in <module>\n",
      "    import src\n",
      "  File \"c:\\Users\\sstolwi\\Github\\TWON-Metrics\\src\\__init__.py\", line 1, in <module>\n",
      "    from .hf_classify import HFClassify\n",
      "  File \"c:\\Users\\sstolwi\\Github\\TWON-Metrics\\src\\hf_classify.py\", line 5, in <module>\n",
      "    import torch\n",
      "  File \"c:\\Users\\sstolwi\\Github\\llmdiv\\Lib\\site-packages\\torch\\__init__.py\", line 1477, in <module>\n",
      "    from .functional import *  # noqa: F403\n",
      "  File \"c:\\Users\\sstolwi\\Github\\llmdiv\\Lib\\site-packages\\torch\\functional.py\", line 9, in <module>\n",
      "    import torch.nn.functional as F\n",
      "  File \"c:\\Users\\sstolwi\\Github\\llmdiv\\Lib\\site-packages\\torch\\nn\\__init__.py\", line 1, in <module>\n",
      "    from .modules import *  # noqa: F403\n",
      "  File \"c:\\Users\\sstolwi\\Github\\llmdiv\\Lib\\site-packages\\torch\\nn\\modules\\__init__.py\", line 35, in <module>\n",
      "    from .transformer import TransformerEncoder, TransformerDecoder, \\\n",
      "  File \"c:\\Users\\sstolwi\\Github\\llmdiv\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 20, in <module>\n",
      "    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
      "c:\\Users\\sstolwi\\Github\\llmdiv\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at ..\\torch\\csrc\\utils\\tensor_numpy.cpp:84.)\n",
      "  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
      "c:\\Users\\sstolwi\\Github\\llmdiv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(\"sjoerdAzure.env\")  # Load environment variables from .env file\n",
    "import time\n",
    "\n",
    "import typing\n",
    "\n",
    "from sklearn.metrics import cohen_kappa_score, classification_report\n",
    "import krippendorff\n",
    "import yaml\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import config\n",
    "import src\n",
    "import tqdm\n",
    "import json\n",
    "import numpy as np\n",
    "import logging\n",
    "\n",
    "#import cltrier_lib as lib\n",
    "import pyreadstat\n",
    "import yaml\n",
    "pd.set_option('display.max_colwidth', 100) \n",
    "#set up helper variables and functions:\n",
    "CFG = config.Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "278268c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data:\n",
    "\n",
    "# Naab2025 data\n",
    "naab = pd.read_parquet(f'{CFG.report_dir}/Naab2025.parquet')\n",
    "# Jaida2024 data\n",
    "jaidka = pd.read_parquet('data/jaidka2022/TwitterDeliberativePolitics2.parquet')\n",
    "# Boukes\n",
    "boukes = pd.read_parquet('data/publicsphere/publicsphere.cardiff_prompt_classify_anon.parquet')\n",
    "boukesT = pd.read_csv('data/publicsphere/full_data.csv')\n",
    "#MH_clemm 2024\n",
    "MHclemm = pd.read_parquet('data/MH_BClemm_data/germany_val_all_llama.parquet') #uses same prompt as Boukes, but different data (and here we use German, but could ask for English and (?) Spanish (?))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8c56af",
   "metadata": {},
   "source": [
    "list the variables we want to use:\n",
    "**rationality** - prompt: 'rationality_simple2', 'rationality_jaidka',        \n",
    "  manual coding: \"Justification\" (Jaidka), RATIONALITY_DUMMY\n",
    "**incivility** - prompt: 'incivility_simple2', 'incivility_jaidka',  civility_jaidka         \n",
    "  manual coding: INCIVILITY_DUMMY, Incivility_tot ('Uncivil_abuse', 'Empathy_Respect'), Uncivil_abuse, \"Empathy_Respect\" (jaidka)\n",
    "**interactivity** - prompt: 'interactivity_acknowledgement_simple2', interactivity_acknowledgement_jaidka       \n",
    "  manual coding: INTERACTIVITY_DUMMY, Reciprocity (Jaidka)\n",
    "**diversity/ideology** - prompt: 'political_ideology_US', 'political_ideology' (german)  -> no ideology in Jaidka\n",
    "  manual coding: LIBERAL_DUMMY, CONSERVATIVE_DUMMY\n",
    "**political_dum** - prompt: 'political_post', political_post_jaidka \n",
    "  manual coding: HAS_OPINION_DUMMY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d190ef58",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model variants:\n",
    "# llama31_8b\n",
    "# llama31_70b\n",
    "# gpt4o\n",
    "# gpt4Turbo\n",
    "\n",
    "#optional:\n",
    "# gpt4 (OPenAI, microsoft)\n",
    "# llama33_70b (Meta)\n",
    "# Gemma3:22b (US, google) (based on Gemini 2)\n",
    "# \"id\":\"deepseek-r1:70b\",\"name\":\"DeepSeek-R1 (china)\n",
    "# qwen2.5:70b (china)\n",
    "# mistral-large:123b\",\"name\":\"Mistral\" (europe)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027effde",
   "metadata": {},
   "source": [
    "list the annotations we have available per dataset:\n",
    "**Jaidka**: 'rationality_simple2_Llama8b_dum', 'rationality_simple2_gpt4o_dum','rationality_jaidka_Llama8b_dum','rationality_jaidka_gpt4o_dum', 'civility_jaidka_gpt4o_dum', incivility_simple2_gpt4o_dum, incivility_jaidka_gpt4o_dum, reciprocity_jaidka_gpt4o_dum, interactivity_acknowledgement_simple_gpt4o_dum, political_post_jaidka_gpt4o_dum, political_post_gpt4o_dum\n",
    "**Boukes**: \n",
    "*rationality*: rationality_simple2_dum (+ rationality_simple_dum, rationality_combine_dum, rationality_combine_exactexample_dum, rationality_prompt_dum (aggregation of indicator prompt scores)), rationality_simple2_gpt4o_system_dum, rationality_simple2_gpt4T_system_dum, rationality_simple2_small_dum, \n",
    "*incivility*: incivility_simple2_dum (+ incivility_simple_dum, incivility_combine_dum), incivility_prompt_dum (aggregation of indicator prompt scores), incivility_simple2_gpt4o_system_dum, incivility_simple2_gpt4T_system_dum, incivility_simple2_small_dum \n",
    "*interactivity*: interactivity_acknowledgement_simple_dum (+ interactivity_acknowledgement_simple2_dum), interactivity_acknowledgement_simple_gpt4o_system_dum, interactivity_acknowledgement_simple_gpt4T_system_dum, interactivity_acknowledgement_simple_small_dum (+interactivity_acknowledgement_simple_small2_dum)\n",
    "*diversity*: political_liberal_US_dum, political_conservative_US_dum, political_liberal_US_gpt4o_system_dum, political_liberal_US_gpt4T_system_dum, political_conservative_US_gpt4o_system_dum, political_conservative_US_gpt4T_system_dum, political_liberal_US_small_dum, political_conservative_US_small_dum\n",
    "*political_dum*: political_opinion_US_dum, political_opinion_US_gpt4o_system_dum, political_opinion_US_gpt4T_system_dum, political_opinion_US_small_dum (either liberal/conservative; Boukes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca488d78",
   "metadata": {},
   "source": [
    "#to be made prompts:\n",
    "#Naab:\n",
    " Argumente/ v75 - rationality in combination with quellenbelege?\n",
    "(Operationalisierung erstellt in Anlehnung an Ziegele & Quiring, 2015)\n",
    "Argumente sind Aussagen, die dazu dienen sollen, Behauptungen zu begründen oder zu widerlegen. Es wird kodiert, ob ein Nutzerkommentar Argumente verwendet, um eine oder mehrere geäußerte(n) Meinung(en) zu begründen.\n",
    "Von Interesse ist hier, ob die Nutzer ihre eigenen Aussagen mit Argumenten unterstützen oder ob diese unbegründet bleiben. Der Leser muss sich bei zusammenhängenden Aussagen im Kommentar fragen, ob die Frage nach dem „warum“ beantwortet wird. Anders gesagt: Gut erkennbar ist ein Argument, wenn man es problemlos mit einem Kausalsatz (weil...) an eine Behauptung anfügen kann oder wenn man eine begründende „Wenn-Dann-Beziehung“ zwischen den Sätzen herstellen kann. Eine Aneinanderreihung von Aussagen zählt nicht als Argumentation.\n",
    "! Achtung: Es geht hier nicht um die Qualität der Argumente! Es wird nur kodiert, ob Begründungen für Aussagen/Behauptungen/Positionen angeführt werden (s. Beispiele)!\n",
    "0\n",
    "Keine Argumente\n",
    "Im Kommentar werden keine Argumente verwendet. Behauptungen werden nicht begründet.\n",
    "Der Abtreibungsparagraph muss abgeschafft werden.\n",
    "Alles ist wichtiger als Fußball.\n",
    "Schade, dass die FDPs in Hessen nochmal geschafft hat aber für Schwarz-Gelb reicht‘s trotzdem nicht mehr. Dafür ist neben rot-rot-grün jetzt auch ‘ne klassische Ampel möglich. (Fragen nach dem „Warum“ bleiben unbeantwortet, Sätze lassen sich nicht mit „weil“ verknüpfen)\n",
    "1\n",
    "Mind. ein Argument\n",
    "Im Kommentar wird mindestens ein Argument, u.U. auch mehrere Argumente, formuliert.\n",
    "Deutschland hätte Snowden schon lange Asyl anbieten müssen: Schließlich ist er ein politisch Verfolgter, dessen Gesundheit auf dem Spiel steht, wenn er von seinen wildgewordenen Landsleuten aufgegriffen werden sollte.\n",
    "In erster Linie pflichte ich der GDL bei, (denn) sie demonstrieren wie es geht. In zweiter Linie möchte ich den Arbeitgebern das Armutszeugnis ausstellen, (denn) ich hab zum ersten Mal das Gefühl dass einer das richtige macht.\n",
    "62\n",
    "99\n",
    "Nicht eindeutig zuzuordnen.\n",
    "\n",
    "\n",
    "Quellenbelege im Kommentar/ v76\n",
    "Es wird kodiert, ob im Kommentar nachprüfbare Belege für Fakten, Tatsachenbehauptungen, Argumente oder Meinungen genannt werden. Dies passiert, indem auf Quellen verwiesen wird. Ein Argument, eine Prognose oder eine Werthaltung alleine sind kein Beleg. Der Beleg macht eine Aussage prinzipiell für jeden intersubjektiv nachprüfbar. Ein Verweis auf Quellen ist ausreichend, die Quellen müssen nicht (können aber) verlinkt sein. Werden zwar Fakten genannt, diese sind jedoch nicht belegt und nur mit größerem Aufwand intersubjektiv prüfbar, zählt dies nicht als Beleg.\n",
    "0\n",
    "Kein Beleg genannt\n",
    "Ich glaube, dass auch der Asylmissbrauch ein Problem ist. Aber sowas ist auch schwer zu erkennen.\n",
    "Assad wird weiter Chemiewaffen einsetzen, er weiß, er kann sich auf seine russischen und chinesischen Freunde verlassen. (Spekulation und Zukunftsprognosen sind keine Fakten)\n",
    "Alle außer Syrien und Iran haben die Amis im Sack und wenn das Problemchen gelöst wurde, dann wird wohl Russland spüren wer Herr im Hause ist und das ist das eigentliche Ziel der USA.\n",
    "In Frankreich gilt ein Mindestlohn von 11,50€, auch das trägt dazu bei, dass Deutschland mit seinem Niedriglohn den europäischen Nachbarn schwer zusetzt!\n",
    "Der Fingerabdrucksensor hat nichts mit NSA zu tun. Wer das denkt ist auf dem Holzweg. Der Fingerabdrucksensor ist ja nur für die Entsperrung des Telefons. 64Bit ist nice to have.\n",
    "1\n",
    "Mindestens ein Quellen-beleg genannt\n",
    "Der Kommentator enthält Verweise auf Quellen, mit\n",
    "Die Tagesschau meldete gestern neben den 20 Toten auch über 70 Verletzte.\n",
    "63\n",
    "denen Fakten oder sonstige Aussagen belegt werden. Dadurch sind sie prinzipiell für jeden intersubjektiv nachprüfbar. Ein Verweis auf Quellen ist ausreichend, sie müssen nicht (können aber) verlinkt sein.\n",
    "Die CDU kommt auf 41,5 Prozent, SPD, Grüne und Linke zusammen auf 42, 7. Und das kann sich jedes Kind auf Wikipedia ansehen: https://de.wikipedia.org/wiki/Bundestagswahl_2013\n",
    "99\n",
    "Nicht eindeutig zuzuordnen.\n",
    "\n",
    "\n",
    "Inziviles im Kommentar/ v711 - check definition\n",
    "(Operationalisierung erstellt in Anlehnung an Ziegele & Quiring, 2015; vgl. auch Coe, Kenski & Rains, 2014; Papacharissi, 2004)\n",
    "Inzivilität wird kodiert, wenn im Kommentar die folgenden Aussagetypen vorkommen, die einen unnötig respektlosen Ton gegenüber den Diskussionsteilnehmern, Themen, nicht-anwesenden Dritten oder Journalisten/Medien transportieren:\n",
    "•\n",
    "Beschimpfungen: Bösartige oder herabsetzende Wörter, die an Personen oder Personengruppen gerichtet sind.\n",
    "o\n",
    "Das verachtenswerte politische Unkraut FDP ist jetzt dort, wo es hingehört. (Thema Landtagswahl in Hessen)\n",
    "•\n",
    "Abfällige Bewertungen: Bösartige oder herabsetzende Wörter und Sätze, die gegen eine Idee, einen Plan, ein Verhalten oder eine Politik gerichtet sind.\n",
    "70\n",
    "o\n",
    "Klassisches Eigentor geschossen mit diesem Schrott Handy! (Thema iPhone-Vorstellung)\n",
    "•\n",
    "Lügen: Explizite oder implizite Aussagen, dass eine Idee, ein Plan, ein Verhalten oder eine Politik unehrlich sind.\n",
    "o\n",
    "Und welche \"Glaubwürdigkeit\" hat ein Präsident, der die Dauer-Lügen seiner Geheimdienste vertritt, ÜBERHAUPT noch? (Thema Syrien-Konflikt)\n",
    "•\n",
    "Vulgarität: Obszönitäten oder eine vulgäre Sprache, die im persönlichen Gespräch unangebracht wäre.\n",
    "o\n",
    "Es gibt genug Spinner, die ihre eigene unzureichende Schwanzlänge kompensieren müssen. (Thema iPhone-Vorstellung)\n",
    "•\n",
    "Abschätzige Bemerkungen: Herabsetzende Bemerkungen über die Art und Weise, wie eine Person auftritt oder kommuniziert.\n",
    "o\n",
    "Damit fühlst du dich nun überlegen und toll, hm? Infantiles Gehabe und Profilierung. (Thema Veggie-Day)\n",
    "•\n",
    "Stereotype: Eine vereinfachende, verallgemeinernde, schematische Reduzierung einer Erfahrung, Meinung oder Vorstellung auf ein meist verfestigtes, oft ungerechtfertigtes und emotional aufgeladenes Vorurteil. Stereotype bilden eine Voraussetzung für die Diskriminierung von Minderheiten und die Ausbildung von Feindbildern, Rassismus und Sexismus. Oftmals – aber nicht immer – ist die Kommunikation von Stereotypen politisch unkorrekt. So werden zum Beispiel einzelne Personen mit einer Gruppe gleichgesetzt, oder auf die Haupteigenschaften einer Gruppe reduziert.\n",
    "o\n",
    "Der Deutsche hasst den Mainstream, kauft bei KIK und Aldi, schmeißt 2 Jahre alte Handys auf den Müll und fährt mit Monatskarte zur Atomkraftdemo. So ambivalent spießig wie der/die Deutsche ist wohl keine Nation.\n",
    "o\n",
    "Frauen sind zum Putzen da!\n",
    "o\n",
    "Du bist doch ein Hauptschüler, so beschränkt wie du dich ausdrückst.\n",
    "•\n",
    "Absprechen von Rechten (individuell): Anderen Diskussionsteilnehmern werden Menschen- und Persönlichkeitsrechte abgesprochen\n",
    "z. B. Angriffe auf die Meinungsfreiheit oder persönliche Freiheit (Recht, Leben frei zu gestalten, auf finanziellen Wohlstand und gesicherte Existenz), Androhung von Gewalt, einschließlich impliziter Drohungen.\n",
    "o\n",
    "Dir gehört der Mund verboten!; Leute mit Einstellungen wie deiner sollten nicht wählen dürfen.\n",
    "o\n",
    "Die sollten alle im Meer ersaufen;\n",
    "•\n",
    "Bedrohung der Demokratie/ Gefährdung demokratischer Werte (gesellschaftlich): Der Kommentar trifft Aussagen, die vermuten lassen, dass die demokratischen Werte/ die Demokratie nicht wertgeschätzt oder diese gar bedroht werden.\n",
    "! Achtung: Die Artikulation von Kritik und Meinungsverschiedenheiten wird nicht als Inzivilität kodiert, sofern diese Artikulation angemessen höflich und sachlich erfolgt. In anderen Worten: Inzivilität ist nicht die Artikulation von Kritik und Meinungsverschiedenheiten an sich, sondern eine unangemessene Art und Weise, diese zu formulieren.\n",
    "0\n",
    "Keine Inzivilität\n",
    "Der Kommentar ist zivil und höflich formuliert. Er transportiert keinen unnötig respektlosen Ton ggü. Anderen bzw. enthält keine der oben genannten Aussagetypen.\n",
    "71\n",
    "1\n",
    "Vereinzelt inzivil\n",
    "Der Kommentar ist überwiegend zivil und höflich formuliert. Er enthält jedoch vereinzelt die o.g. Aussagetypen.\n",
    "2\n",
    "Überwiegend / ausschließlich inzivil\n",
    "Der Kommentar ist kaum oder nicht zivil und höflich formuliert. Er enthält überwiegend oder ausschließlich die o.g. Aussagetypen.\n",
    "99\n",
    "Nicht eindeutig zuzuordnen.\n",
    "\n",
    "\n",
    "Fragen/ v7135 -> interactivity if either?\n",
    "(Operationalisierung erstellt in Anlehnung an Ziegele & Quiring, 2015)\n",
    "Hier wird kodiert, ob Autor innerhalb eines Kommentars Informations-, Wissens- oder Verständnisdefizite aktiv kommunizieren und ausgleichen wollen, oder ob rhetorische Fragen gestellt werden, auf die eigentlich keine Antwort erwünscht ist. Es wird kodiert, ob überhaupt Fragen gestellt werden, nicht aber, an wen diese ggf. gerichtet sind.\n",
    "a)\n",
    "Echte Fragen\n",
    "Als echte Fragen werden alle Aussagen betrachtet, die mit einem Fragezeichen enden oder von ihrer Satzstruktur her als Frage aufgefasst werden können und die ein echtes Informationsbedürfnis anzeigen. Nur wenn eindeutig erkennbar ist, dass der Autor am Erhalt von Informationen/Meinungen interessiert ist, wird die Frage als „echt“ kodiert. Typen von „echten“ Fragen sind u. a. Wissens-, Einstellungs- und Verständnisfragen.\n",
    "Bsp.: Kennt sich hier einer mit dem Völkerrecht aus? Wieso kann Obama überhaupt überlegen einen ‘Alleingang‘ zu starten?\n",
    "b)\n",
    "Rhetorische Fragen\n",
    "Enthält der Kommentar eine oder mehrere rhetorische Fragen? Rhetorische Fragen werden vor allem als Argumentationsstrategie einsetzt, um die eigenen Positionen durchzusetzen. Sie zielen nicht auf Informationsgewinn ab, sondern der Autor will vorrangig Reaktionen wie Zustimmung bzw. Ablehnung erhalten.\n",
    "! Achtung: Rhetorische Fragen zusätzlich beim Diskussionsfaktor Kontroverse kodieren.\n",
    "Kodieranweisung: Rhetorische Frage schlägt echte Frage → sobald eine rhetorische Frage formuliert wird, wird eine „2“ kodiert."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88cc46f4",
   "metadata": {},
   "source": [
    "maybe do the following analyses:\n",
    "-baseline differences between slightly differing prompts on Boukes - best vs long vs best with slight word change vs long exact example vs prompt_dum (indicator aggregation) -> only have this for Llama3.1:70b\n",
    "-model differences on Boukes - best prompt Llama3.1:70b vs 8b vs GPT4T vs GPT4o\n",
    "-codebook differences on Boukes - Jaidka vs Boukes \n",
    "-dataset differences - best prompt Llama3.1:70b vs 8b vs GPT4o Jaidka vs Boukes -> need to run Llama3.1:70b on Jaidka, but no longer available via Trier -> could run both datasets with Llama3.3:70b instead\n",
    "-error analysis - how do differences between models overlap with differences between models and manual data\n",
    "-downstream effect on Boukes2024\n",
    "\n",
    "maybe leave out indicator aggregation prompt, only focus on best vs slight word change vs arbitrary change (Barrie ea 2025 prompt stability),  try run baseline for Boukes and Jaidka with GPT4o (azure) and Llama3.3:70b (Trier)\n",
    "compare model differences on Boukes also for other models if possible (but at least add L3.3:70b)\n",
    "\n",
    "drop Naab data? \n",
    "ask MH for english data for out of sample comparison on political_dum and ideology\n",
    "note we only have low-temperature anotations for Llama available"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48abfb9b",
   "metadata": {},
   "source": [
    "#TO DO:\n",
    "-classify Boukes, Jaidka and MHdata with options_low, Llama3.3:70b for Boukes and Jaidka prompts\n",
    "-classify Boukes with prompt stability/slightly different prompts with Llama3.3:70b and gpt4o -> use Barrie ea 2025 approach of paraphrasing prompt with increasing temperature, perhaps use Llama instead of pegasus for ease of use? - report prompt variations in appendix for manual validation\n",
    "-error analysis\n",
    "-downstream effects on Boukes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "18105311",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up helper variables and functions:\n",
    "CFG = config.Config()\n",
    "\n",
    "def load_json(path: str):\n",
    "    with open(path, encoding='utf-8') as fp:\n",
    "        return json.load(fp)\n",
    "    \n",
    "#set option variables:\n",
    "\n",
    "#set options to low temperature (0,1):\n",
    "options_low_str = \"\"\"\n",
    "seed: 42\n",
    "temperature: 0.1\n",
    "\"\"\"\n",
    "\n",
    "options_low = yaml.safe_load(options_low_str)\n",
    "\n",
    "#apparently the 3.1 70b model is no longer available via Trier...\n",
    "MODELsmall: str = 'llama3.1:8b'\n",
    "MODEL33large: str = 'llama3.3:70b' # options: 'gemma:7b-instruct-q6_K', 'gemma2:27b-instruct-q6_K', 'llama3.1:8b-instruct-q6_K', 'llama3.1:70b-instruct-q6_K', 'mistral:7b-instruct-v0.3-q6_K', 'mistral-large:123b-instruct-2407-q6_K', 'mixtral:8x7b-instruct-v0.1-q6_K', 'mixtral:8x22b-instruct-v0.1-q6_K', 'phi3:14b-medium-128k-instruct-q6_K' or 'qwen2:72b-instruct-q6_K'\n",
    "MODELgpt4o = \"nf-gpt-4o-2024-08-06\" # in principe is er nu van elk model een nf (no filter) en een normale versie beschikbaar, de no filter versies zijn alleen voor onderzoekers beschikbaar voor analyze van content die niet door de filter heen zou komen.\n",
    "MODELgpt4T = \"nf-gpt-4-turbo\" # Can be gpt-35-turbo, gpt-4-turbo, gpt-4 or Meta-Llama-3-8B-Instruct.\n",
    "MODEL33largeAzure = 'Llama-3.3-70B-Instruct' #azureml://registries/azureml-meta/models/Llama-3.3-70B-Instruct/versions/4 / options:  \"data\": [\n",
    "MODEL31largeAzureNF = 'nf-Llama-3.1-70b-instruct'\n",
    "\n",
    "\n",
    "options_zero_str = \"\"\"\n",
    "seed: 42\n",
    "temperature: 0\n",
    "\"\"\"\n",
    "options_zero = yaml.safe_load(options_zero_str)\n",
    "\n",
    "temperature_0 : int = 0\n",
    "SEED: int = 42\n",
    "SEED2: int = 43\n",
    "MAX10: int = 10\n",
    "TOPP1: int = 1\n",
    "\n",
    "options_creative_str = \"\"\"\n",
    "seed: 42\n",
    "temperature: 0.7\n",
    "topp: 0.8\n",
    "\"\"\"\n",
    "options_creative = yaml.safe_load(options_creative_str)\n",
    "\n",
    "options_large_str = \"\"\"\n",
    "seed: 42\n",
    "temperature: 0\n",
    "num_predict: 2000\n",
    "\"\"\"\n",
    "options_large = yaml.safe_load(options_large_str)\n",
    "\n",
    "#load environment variables:\n",
    "api_key = os.environ.get('sjoerd_key')\n",
    "\n",
    "#setttings:\n",
    "api_endpoint = \"https://ai-research-proxy.azurewebsites.net/chat/completions\"\n",
    "api_endpoint_embed = \"https://ai-research-proxy.azurewebsites.net/embeddings\"\n",
    "####### API REQUEST FORMATTING ######\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"Authorization\": \"Bearer \" + api_key\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7b072300",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "classifying rationality_simple2:   0%|          | 2/3862 [01:17<41:34:44, 38.78s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[46], line 8\u001b[0m\n\u001b[0;32m      3\u001b[0m pubspherepromptsrun1 \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrationality_simple2\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m#pubspherepromptsrunall = ['rationality_simple2', 'rationality_jaidka', 'incivility_simple2', 'incivility_jaidka',  'civility_jaidka', 'interactivity_acknowledgement_simple2', 'interactivity_acknowledgement_jaidka', 'political_ideology_US', 'political_post', 'political_post_jaidka' ]  \u001b[39;00m\n\u001b[0;32m      6\u001b[0m predictions2: typing\u001b[38;5;241m.\u001b[39mDict[\u001b[38;5;28mstr\u001b[39m, np\u001b[38;5;241m.\u001b[39mndarray] \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      7\u001b[0m     label: (\n\u001b[1;32m----> 8\u001b[0m         \u001b[43msrc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPromptClassify\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[43mboukesT\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcommentText\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMODEL33large\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions_low\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m     )\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m label, path \u001b[38;5;129;01min\u001b[39;00m CFG\u001b[38;5;241m.\u001b[39mprompt_classify_files\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m label \u001b[38;5;129;01min\u001b[39;00m pubspherepromptsrun1\n\u001b[0;32m     13\u001b[0m }\n",
      "File \u001b[1;32mc:\\Users\\sstolwi\\Github\\TWON-Metrics\\src\\prompt_classify.py:26\u001b[0m, in \u001b[0;36mPromptClassify.__call__\u001b[1;34m(self, samples, model, options)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index, value \u001b[38;5;129;01min\u001b[39;00m tqdm\u001b[38;5;241m.\u001b[39mtqdm(\n\u001b[0;32m     20\u001b[0m         samples\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[0;32m     21\u001b[0m         total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(samples),\n\u001b[0;32m     22\u001b[0m         desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclassifying \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     23\u001b[0m ):\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     25\u001b[0m         predictions[index] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses\u001b[38;5;241m.\u001b[39mget(\n\u001b[1;32m---> 26\u001b[0m             \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhttps://inf.cl.uni-trier.de/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[43m                \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[43m                    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[43m                    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msystem\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtemplate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[43m                    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mprompt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtemplate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[43m                    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moptions\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[43m                    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mjson()[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mstrip(),\n\u001b[0;32m     34\u001b[0m             \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     35\u001b[0m         )\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m _e:\n\u001b[0;32m     38\u001b[0m         logging\u001b[38;5;241m.\u001b[39mwarning(_e)\n",
      "File \u001b[1;32mc:\\Users\\sstolwi\\Github\\llmdiv\\Lib\\site-packages\\requests\\api.py:115\u001b[0m, in \u001b[0;36mpost\u001b[1;34m(url, data, json, **kwargs)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(url, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, json\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    104\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a POST request.\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \n\u001b[0;32m    106\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpost\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\sstolwi\\Github\\llmdiv\\Lib\\site-packages\\requests\\api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\sstolwi\\Github\\llmdiv\\Lib\\site-packages\\requests\\sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[0;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    587\u001b[0m }\n\u001b[0;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32mc:\\Users\\sstolwi\\Github\\llmdiv\\Lib\\site-packages\\requests\\sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[0;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[0;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[1;32mc:\\Users\\sstolwi\\Github\\llmdiv\\Lib\\site-packages\\requests\\adapters.py:667\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    664\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[0;32m    666\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 667\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    670\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    671\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    672\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m    682\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[1;32mc:\\Users\\sstolwi\\Github\\llmdiv\\Lib\\site-packages\\urllib3\\connectionpool.py:789\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    786\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    788\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[1;32m--> 789\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    802\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    804\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[0;32m    805\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\sstolwi\\Github\\llmdiv\\Lib\\site-packages\\urllib3\\connectionpool.py:536\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    534\u001b[0m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[0;32m    535\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 536\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    537\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    538\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[1;32mc:\\Users\\sstolwi\\Github\\llmdiv\\Lib\\site-packages\\urllib3\\connection.py:464\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    461\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mresponse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HTTPResponse\n\u001b[0;32m    463\u001b[0m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[1;32m--> 464\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    466\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    467\u001b[0m     assert_header_parsing(httplib_response\u001b[38;5;241m.\u001b[39mmsg)\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\\Lib\\http\\client.py:1430\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1428\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1429\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1430\u001b[0m         \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1431\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[0;32m   1432\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\\Lib\\http\\client.py:331\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    329\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[0;32m    330\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 331\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    332\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[0;32m    333\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\\Lib\\http\\client.py:292\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    291\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 292\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    293\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[0;32m    294\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\\Lib\\socket.py:720\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    718\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    719\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 720\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    721\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[0;32m    722\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\\Lib\\ssl.py:1251\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1247\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1248\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1249\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[0;32m   1250\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[1;32m-> 1251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1252\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1253\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\\Lib\\ssl.py:1103\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1101\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1102\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1103\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1104\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1105\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#classify Boukes with Llama3.3:70b\n",
    "\n",
    "pubspherepromptsrun1 = ['rationality_simple2']\n",
    "#pubspherepromptsrunall = ['rationality_simple2', 'rationality_jaidka', 'incivility_simple2', 'incivility_jaidka',  'civility_jaidka', 'interactivity_acknowledgement_simple2', 'interactivity_acknowledgement_jaidka', 'political_ideology_US', 'political_post', 'political_post_jaidka' ]  \n",
    "\n",
    "predictions2: typing.Dict[str, np.ndarray] = {\n",
    "    label: (\n",
    "        src.PromptClassify\n",
    "        .from_json(path)\n",
    "        (boukesT[\"commentText\"], model=MODEL33large, options=options_low)\n",
    "    )\n",
    "    for label, path in CFG.prompt_classify_files.items() if label in pubspherepromptsrun1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc6be25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            prompt_label  seed  \\\n",
      "0                     incivility_simple2    43   \n",
      "1  interactivity_acknowledgement_simple2    43   \n",
      "2                  political_ideology_US    43   \n",
      "3                         political_post    43   \n",
      "4                    rationality_simple2    43   \n",
      "5                     incivility_simple2    42   \n",
      "6  interactivity_acknowledgement_simple2    42   \n",
      "7                  political_ideology_US    42   \n",
      "8                         political_post    42   \n",
      "9                    rationality_simple2    42   \n",
      "\n",
      "                                                                                    paraphrased_prompt  \n",
      "0  Is this comment uncivil?\\nGuidelines: Mark Yes (1) if the comment contains name-calling, insults...  \n",
      "1  Is the comment a reference to, an acknowledgment of, or a reply to another user's remark?\\nInstr...  \n",
      "2  Categorize the given message according to its ideological stance as liberal (0), neutral (1), or...  \n",
      "3  Determine whether the given message should be categorized as political (1) or non-political (0)....  \n",
      "4  Assess whether the comment offers logical evaluation.\\nGuidelines: Assign Yes (1) if the comment...  \n",
      "5  Determine if the comment shows incivility.\\nGuidelines: Assign Yes (1) if the comment contains n...  \n",
      "6  Determine if this comment is referencing, recognizing, or replying to another user's comment.\\nG...  \n",
      "7  Determine whether the given text aligns with liberal views (0), is ideologically neutral (1), or...  \n",
      "8  Determine whether the given message is political (1) or non-political (0). A message is consider...  \n",
      "9  Evaluate whether the comment offers logical examination.\\nInstructions: Assign Yes (1) if the co...  \n"
     ]
    }
   ],
   "source": [
    "#use GPT4o to compile paraphrased prompts::\n",
    "def load_json(path: str):\n",
    "    with open(path, encoding='utf-8') as fp:\n",
    "        return json.load(fp)\n",
    "    \n",
    "\n",
    "#pubspherepromptsrunall = ['rationality_simple2', 'rationality_jaidka', 'incivility_simple2', 'incivility_jaidka',  'civility_jaidka', 'interactivity_simple2', 'interactivity_acknowledgement_jaidka', 'political_ideology_US', 'political_post', 'political_post_jaidka' ]  \n",
    "\n",
    "pubsphereparaphraserun1 = ['rationality_simple2', 'incivility_simple2', 'interactivity_acknowledgement_simple2', 'political_ideology_US', 'political_post'] \n",
    "\n",
    "\n",
    "\n",
    "#chunked_result: typing.List[pd.DataFrame] = []\n",
    "\n",
    "for label, path in CFG.prompt_classify_files.items():\n",
    "    if label in pubsphereparaphraserun1: \n",
    "        template = load_json(path).get('template')\n",
    "        classes = load_json(path).get('classes')\n",
    "        retry_count = 0\n",
    "        max_retries = 5\n",
    "        while retry_count < max_retries:\n",
    "            try: \n",
    "                response = requests.post(\n",
    "                        url=api_endpoint,\n",
    "                        headers=headers,\n",
    "                        json={\n",
    "                            'model': MODELgpt4o,\n",
    "                            'messages': [\n",
    "                                {\n",
    "                                    \"role\": \"system\",\n",
    "                                    \"content\": 'You restate a prompt in different words while preserving its meaning and formatting. The prompt is: \"{}\"'\n",
    "                                },\n",
    "                                {\n",
    "                                    \"role\": \"user\",\n",
    "                                    \"content\": 'You restate a prompt in different words while preserving its meaning and formatting. The prompt is: \"{}\"'.format(template)\n",
    "                                }\n",
    "                            ],\n",
    "                            'temperature': 0.7,\n",
    "                            'top_p': 0.8,  \n",
    "                            'seed': SEED\n",
    "                        }\n",
    "                    )  \n",
    "                if response.status_code == 200:\n",
    "                    data_response = response.json()\n",
    "                    chunked_result.append(\n",
    "                        pd.DataFrame(\n",
    "                            data=[[label, SEED, data_response[\"choices\"][0][\"message\"][\"content\"]]],                                \n",
    "                            columns=['prompt_label', 'seed', 'paraphrased_prompt']\n",
    "                        )\n",
    "                    )\n",
    "                    break  # Exit retry loop for this label\n",
    "                elif response.status_code == 429:\n",
    "                    retry_count += 1\n",
    "                    wait_time = 20\n",
    "                    print(f\"Rate limit exceeded. Retrying in {wait_time} seconds...\")\n",
    "                    time.sleep(wait_time)\n",
    "                elif response.status_code == 500:\n",
    "                    retry_count += 1\n",
    "                    wait_time = 20\n",
    "                    print(f\"Failed to connect to API. Status code: {response.status_code}. Retrying in {wait_time} seconds...\")\n",
    "                    time.sleep(wait_time)\n",
    "                else:\n",
    "                    print(f\"Failed to connect to API. Status code: {response.status_code}\")\n",
    "                    print(response.text)\n",
    "                    # Optionally append a row with error info\n",
    "                    chunked_result.append(\n",
    "                        pd.DataFrame(\n",
    "                            data=[[label, f\"ERROR: {response.status_code}\"]],\n",
    "                            columns=['prompt_label', 'paraphrased_prompt']\n",
    "                        )\n",
    "                    )\n",
    "                    break\n",
    "            except requests.exceptions.RequestException as e:   \n",
    "                print(f\"Failed to connect to API: {e}\")\n",
    "                retry_count += 1\n",
    "                wait_time = 60\n",
    "                print(f\"Retrying in {wait_time} seconds...\")\n",
    "                time.sleep(wait_time)\n",
    "        else:\n",
    "            # If all retries failed, append a row with error info\n",
    "            chunked_result.append(\n",
    "                pd.DataFrame(\n",
    "                    data=[[label, \"ERROR: Max retries exceeded\"]],\n",
    "                    columns=['prompt_label', 'paraphrased_prompt']\n",
    "                )\n",
    "            )\n",
    "\n",
    "para1 = pd.concat(chunked_result, ignore_index=True)\n",
    "print(para1)\n",
    "# ...existing code..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f67b8318",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt_label</th>\n",
       "      <th>seed</th>\n",
       "      <th>paraphrased_prompt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>incivility_simple2</td>\n",
       "      <td>43</td>\n",
       "      <td>Is this comment uncivil?\\nGuidelines: Mark Yes (1) if the comment contains name-calling, insults...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>interactivity_acknowledgement_simple2</td>\n",
       "      <td>43</td>\n",
       "      <td>Is the comment a reference to, an acknowledgment of, or a reply to another user's remark?\\nInstr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>political_ideology_US</td>\n",
       "      <td>43</td>\n",
       "      <td>Categorize the given message according to its ideological stance as liberal (0), neutral (1), or...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>political_post</td>\n",
       "      <td>43</td>\n",
       "      <td>Determine whether the given message should be categorized as political (1) or non-political (0)....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>rationality_simple2</td>\n",
       "      <td>43</td>\n",
       "      <td>Assess whether the comment offers logical evaluation.\\nGuidelines: Assign Yes (1) if the comment...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>incivility_simple2</td>\n",
       "      <td>42</td>\n",
       "      <td>Determine if the comment shows incivility.\\nGuidelines: Assign Yes (1) if the comment contains n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>interactivity_acknowledgement_simple2</td>\n",
       "      <td>42</td>\n",
       "      <td>Determine if this comment is referencing, recognizing, or replying to another user's comment.\\nG...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>political_ideology_US</td>\n",
       "      <td>42</td>\n",
       "      <td>Determine whether the given text aligns with liberal views (0), is ideologically neutral (1), or...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>political_post</td>\n",
       "      <td>42</td>\n",
       "      <td>Determine whether the given message is political (1) or non-political (0). A message is consider...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>rationality_simple2</td>\n",
       "      <td>42</td>\n",
       "      <td>Evaluate whether the comment offers logical examination.\\nInstructions: Assign Yes (1) if the co...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            prompt_label  seed  \\\n",
       "0                     incivility_simple2    43   \n",
       "1  interactivity_acknowledgement_simple2    43   \n",
       "2                  political_ideology_US    43   \n",
       "3                         political_post    43   \n",
       "4                    rationality_simple2    43   \n",
       "5                     incivility_simple2    42   \n",
       "6  interactivity_acknowledgement_simple2    42   \n",
       "7                  political_ideology_US    42   \n",
       "8                         political_post    42   \n",
       "9                    rationality_simple2    42   \n",
       "\n",
       "                                                                                    paraphrased_prompt  \n",
       "0  Is this comment uncivil?\\nGuidelines: Mark Yes (1) if the comment contains name-calling, insults...  \n",
       "1  Is the comment a reference to, an acknowledgment of, or a reply to another user's remark?\\nInstr...  \n",
       "2  Categorize the given message according to its ideological stance as liberal (0), neutral (1), or...  \n",
       "3  Determine whether the given message should be categorized as political (1) or non-political (0)....  \n",
       "4  Assess whether the comment offers logical evaluation.\\nGuidelines: Assign Yes (1) if the comment...  \n",
       "5  Determine if the comment shows incivility.\\nGuidelines: Assign Yes (1) if the comment contains n...  \n",
       "6  Determine if this comment is referencing, recognizing, or replying to another user's comment.\\nG...  \n",
       "7  Determine whether the given text aligns with liberal views (0), is ideologically neutral (1), or...  \n",
       "8  Determine whether the given message is political (1) or non-political (0). A message is consider...  \n",
       "9  Evaluate whether the comment offers logical examination.\\nInstructions: Assign Yes (1) if the co...  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "para1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b410b186",
   "metadata": {},
   "source": [
    "This rewording appears as dissimilar as the Jaidka:\n",
    "-Boukes: \"Does this comment provide rational analysis?\\nInstructions: Code Yes (1) if the comment includes:\\nContext or background,\\nEvidence (facts, sources, authorities),\\nReasoning or structured argument.\\nCode No (0) if these are absent\\n\\\\n\\\\nRespond with only the predicted class (0 or 1) of the request.\\\\n\\\\nText: {text}\\\\nClass:\",\n",
    "   \n",
    "-Jaidka: \"This tweet is a reply on Twitter (i.e., a Tweet) to a United States member of the Congress. Please classify this tweet according to whether it has a justification. Read the tweet. Determine which category best describes the tweet. Code YES (1): If this tweet contains personal feelings or experiences. Also code YES (1) If this tweet contains facts, links or evidence from other sources. Code NO (0) If this tweet does not offer a justification. \\n\\\\n\\\\nRespond with only the predicted class (0 or 1) of the request.\\\\n\\\\nText: {text}\\\\nClass:\",\n",
    "\n",
    "-GPT4o_temp0.7_topp0.8_seed1: 'Evaluate whether the comment offers logical examination. \\n\\nGuidelines: Assign Yes (1) if the comment contains:\\n- Context or background information,\\n- Supporting evidence (facts, references, experts),\\n- Logical reasoning or organized argumentation.\\n\\nAssign No (0) if these elements are missing.\\n\\nReply with just the anticipated category (0 or 1) for the request.\\n\\nText: {text}\\nClass:',\n",
    "\n",
    "-GPT4o_temp0.7_topp0.8_seed2:  'Assess whether the comment offers logical evaluation.\\nGuidelines: Assign Yes (1) if the comment contains:\\nContext or background information,\\nSupporting evidence (facts, references, expert opinions),\\nLogical reasoning or organized argumentation.\\nAssign No (0) if these elements are missing.\\n\\\\n\\\\nProvide solely the anticipated classification (0 or 1) of the inquiry.\\\\n\\\\nText: {text}\\\\nClass:',\n",
    "   \n",
    "\n",
    "-GPT4o_temp0.5_topp0.5: 'Evaluate whether the comment offers logical examination.\\nInstructions: Assign Yes (1) if the comment contains:\\nContext or background information,\\nEvidence (facts, references, experts),\\nLogical reasoning or organized argument.\\nAssign No (0) if these elements are missing.\\n\\\\n\\\\nReply with just the anticipated category (0 or 1) of the inquiry.\\\\n\\\\nText: {text}\\\\nClass:',\n",
    "    \n",
    "\n",
    "-> maybe we can generate two paraphrases per boukes prompt with different seeds, which look like they would be rather similar to each other to get a grasp of very small prompt differences, compared to different codebooks or paraphrases of a prompt.\n",
    "also helps to address the potential critizism that our prompts favor Llama, since most prompt engineering was done on lama (but prompt wording of both simple and paraphrased prompts are based on GPT4o)\n",
    "\n",
    "    \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "54fd4577",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the paraphrased prompts to parquet file:\n",
    "para1.to_parquet(f'{CFG.report_dir}/Boukes_paraphrased_prompts.parquet', index=False)\n",
    "#save to json file:\n",
    "para1.to_json(f'{CFG.report_dir}/Boukes_paraphrased_prompts.json', orient='records', force_ascii=False, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmdiv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
