{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4374869",
   "metadata": {},
   "source": [
    "This notebook compares annotations using different GLLMs with codebooks based prompts of Boukes 2024, Jaidka 2022 and Naab 2025 on their respective datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a8edc34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.0.2 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"c:\\Users\\sstolwi\\Github\\llmdiv\\Lib\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"c:\\Users\\sstolwi\\Github\\llmdiv\\Lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"c:\\Users\\sstolwi\\Github\\llmdiv\\Lib\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"c:\\Users\\sstolwi\\Github\\llmdiv\\Lib\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\\Lib\\asyncio\\base_events.py\", line 645, in run_forever\n",
      "    self._run_once()\n",
      "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\\Lib\\asyncio\\base_events.py\", line 1999, in _run_once\n",
      "    handle._run()\n",
      "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\\Lib\\asyncio\\events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"c:\\Users\\sstolwi\\Github\\llmdiv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"c:\\Users\\sstolwi\\Github\\llmdiv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"c:\\Users\\sstolwi\\Github\\llmdiv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"c:\\Users\\sstolwi\\Github\\llmdiv\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"c:\\Users\\sstolwi\\Github\\llmdiv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"c:\\Users\\sstolwi\\Github\\llmdiv\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"c:\\Users\\sstolwi\\Github\\llmdiv\\Lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"c:\\Users\\sstolwi\\Github\\llmdiv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"c:\\Users\\sstolwi\\Github\\llmdiv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"c:\\Users\\sstolwi\\Github\\llmdiv\\Lib\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"c:\\Users\\sstolwi\\Github\\llmdiv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"c:\\Users\\sstolwi\\Github\\llmdiv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"c:\\Users\\sstolwi\\Github\\llmdiv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\sstolwi\\AppData\\Local\\Temp\\ipykernel_18500\\508195922.py\", line 16, in <module>\n",
      "    import src\n",
      "  File \"c:\\Users\\sstolwi\\Github\\TWON-Metrics\\src\\__init__.py\", line 1, in <module>\n",
      "    from .hf_classify import HFClassify\n",
      "  File \"c:\\Users\\sstolwi\\Github\\TWON-Metrics\\src\\hf_classify.py\", line 5, in <module>\n",
      "    import torch\n",
      "  File \"c:\\Users\\sstolwi\\Github\\llmdiv\\Lib\\site-packages\\torch\\__init__.py\", line 1477, in <module>\n",
      "    from .functional import *  # noqa: F403\n",
      "  File \"c:\\Users\\sstolwi\\Github\\llmdiv\\Lib\\site-packages\\torch\\functional.py\", line 9, in <module>\n",
      "    import torch.nn.functional as F\n",
      "  File \"c:\\Users\\sstolwi\\Github\\llmdiv\\Lib\\site-packages\\torch\\nn\\__init__.py\", line 1, in <module>\n",
      "    from .modules import *  # noqa: F403\n",
      "  File \"c:\\Users\\sstolwi\\Github\\llmdiv\\Lib\\site-packages\\torch\\nn\\modules\\__init__.py\", line 35, in <module>\n",
      "    from .transformer import TransformerEncoder, TransformerDecoder, \\\n",
      "  File \"c:\\Users\\sstolwi\\Github\\llmdiv\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 20, in <module>\n",
      "    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
      "c:\\Users\\sstolwi\\Github\\llmdiv\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at ..\\torch\\csrc\\utils\\tensor_numpy.cpp:84.)\n",
      "  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
      "c:\\Users\\sstolwi\\Github\\llmdiv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(\"sjoerdAzure.env\")  # Load environment variables from .env file\n",
    "import time\n",
    "\n",
    "import typing\n",
    "\n",
    "from sklearn.metrics import cohen_kappa_score, classification_report\n",
    "import krippendorff\n",
    "import yaml\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import config\n",
    "import src\n",
    "import tqdm\n",
    "import json\n",
    "import numpy as np\n",
    "import logging\n",
    "\n",
    "#import cltrier_lib as lib\n",
    "import pyreadstat\n",
    "import yaml\n",
    "pd.set_option('display.max_colwidth', 100) \n",
    "#set up helper variables and functions:\n",
    "CFG = config.Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "278268c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data:\n",
    "\n",
    "# Naab2025 data\n",
    "naab = pd.read_parquet(f'{CFG.report_dir}/Naab2025.parquet')\n",
    "# Jaida2024 data\n",
    "jaidka = pd.read_parquet('data/jaidka2022/TwitterDeliberativePolitics2.parquet')\n",
    "# Boukes\n",
    "boukes = pd.read_parquet('data/publicsphere/publicsphere.cardiff_prompt_classify_anon.parquet')\n",
    "boukesT = pd.read_csv('data/publicsphere/full_data.csv') # this includes the comments\n",
    "#the Boukes2024 data is a subset of this, select YT part of Boukes in line with Boukes2024:\n",
    "boukesTYT = boukesT[boukesT['Platform'] == 1]\n",
    "#MH_clemm 2024\n",
    "MHclemm = pd.read_csv('data/MH_BClemm_data/Ideo_Val_GPT_USA.csv') #uses same prompt as Boukes, but different data (and here we use German, but could ask for English and (?) Spanish (?))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "32496032",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>GPT1</th>\n",
       "      <th>GPT2</th>\n",
       "      <th>GPT_Reconciled</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ItÂ’s so great to see our small businesses and entrepreneurs are creating jobs for folks in WI, ...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Thank you Romania Dukes &amp;amp; Mothers Fighting for Justice for putting together a wonderful toy ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>My statement on today's UN Security Council resolution, condemning Israel for its settlement act...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RT @SharedHope: @RepHartzler @RepChrisSmith &amp;amp; @RepAnnWagner speak on the passage of HR 2200 ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Starting first tele-townhall now to answer your questions about the issues important to you. To ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>630</th>\n",
       "      <td>Great news for our nation's #TransCommunity: Pentagon to let transgender individuals serve openl...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>631</th>\n",
       "      <td>With trepidation I enter the #RTCADinner, wherein lurks the Snarkstress, @Kamenta!</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>632</th>\n",
       "      <td>Dropped by @WVUfootball practice today and met Boone county native and Defensive Coordinator Ton...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>633</th>\n",
       "      <td>The American people are tired of top-down mandates from DC, extreme executive overreach, excessi...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>634</th>\n",
       "      <td>RT @LoyolaChicago: Happiest of Birthdays to #LoyolaChicago's favorite Rambler, Sister Jean! Chee...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>635 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                    text  \\\n",
       "0    ItÂ’s so great to see our small businesses and entrepreneurs are creating jobs for folks in WI, ...   \n",
       "1    Thank you Romania Dukes &amp; Mothers Fighting for Justice for putting together a wonderful toy ...   \n",
       "2    My statement on today's UN Security Council resolution, condemning Israel for its settlement act...   \n",
       "3    RT @SharedHope: @RepHartzler @RepChrisSmith &amp; @RepAnnWagner speak on the passage of HR 2200 ...   \n",
       "4    Starting first tele-townhall now to answer your questions about the issues important to you. To ...   \n",
       "..                                                                                                   ...   \n",
       "630  Great news for our nation's #TransCommunity: Pentagon to let transgender individuals serve openl...   \n",
       "631                   With trepidation I enter the #RTCADinner, wherein lurks the Snarkstress, @Kamenta!   \n",
       "632  Dropped by @WVUfootball practice today and met Boone county native and Defensive Coordinator Ton...   \n",
       "633  The American people are tired of top-down mandates from DC, extreme executive overreach, excessi...   \n",
       "634  RT @LoyolaChicago: Happiest of Birthdays to #LoyolaChicago's favorite Rambler, Sister Jean! Chee...   \n",
       "\n",
       "     label  GPT1  GPT2  GPT_Reconciled  \n",
       "0        2     2     2               2  \n",
       "1        1     1     1               1  \n",
       "2        0     1     1               1  \n",
       "3        1     1     1               1  \n",
       "4        1     1     1               1  \n",
       "..     ...   ...   ...             ...  \n",
       "630      0     0     0               0  \n",
       "631      1     1     1               1  \n",
       "632      1     1     1               1  \n",
       "633      2     2     2               2  \n",
       "634      1     1     1               1  \n",
       "\n",
       "[635 rows x 5 columns]"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MHclemm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8c56af",
   "metadata": {},
   "source": [
    "list the variables we want to use:\n",
    "**rationality** - prompt: 'rationality_simple2', 'rationality_jaidka',        \n",
    "  manual coding: \"Justification\" (Jaidka), RATIONALITY_DUMMY\n",
    "**incivility** - prompt: 'incivility_simple2', 'incivility_jaidka',  civility_jaidka         \n",
    "  manual coding: INCIVILITY_DUMMY, Incivility_tot ('Uncivil_abuse', 'Empathy_Respect'), Uncivil_abuse, \"Empathy_Respect\" (jaidka)\n",
    "**interactivity** - prompt: 'interactivity_acknowledgement_simple', interactivity_acknowledgement_jaidka       \n",
    "  manual coding: INTERACTIVITY_DUMMY, Reciprocity (Jaidka)\n",
    "**diversity/ideology** - prompt: 'political_ideology_US', 'political_ideology' (german)  -> no ideology in Jaidka\n",
    "  manual coding: LIBERAL_DUMMY, CONSERVATIVE_DUMMY\n",
    "**political_dum** - prompt: 'political_post', political_post_jaidka \n",
    "  manual coding: HAS_OPINION_DUMMY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d190ef58",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model variants:\n",
    "# llama31_8b\n",
    "# llama31_70b\n",
    "# gpt4o\n",
    "# gpt4Turbo\n",
    "\n",
    "#optional:\n",
    "# gpt4 (OPenAI, microsoft)\n",
    "# llama33_70b (Meta)\n",
    "# Gemma3:22b (US, google) (based on Gemini 2)\n",
    "# \"id\":\"deepseek-r1:70b\",\"name\":\"DeepSeek-R1 (china)\n",
    "# qwen2.5:70b (china)\n",
    "# mistral-large:123b\",\"name\":\"Mistral\" (europe)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027effde",
   "metadata": {},
   "source": [
    "list the annotations we have available per dataset:\n",
    "**Jaidka**: 'rationality_simple2_Llama8b_dum', 'rationality_simple2_gpt4o_dum','rationality_jaidka_Llama8b_dum','rationality_jaidka_gpt4o_dum', 'civility_jaidka_gpt4o_dum', incivility_simple2_gpt4o_dum, incivility_jaidka_gpt4o_dum, reciprocity_jaidka_gpt4o_dum, interactivity_acknowledgement_simple_gpt4o_dum, political_post_jaidka_gpt4o_dum, political_post_gpt4o_dum\n",
    "**Boukes**: \n",
    "*rationality*: rationality_simple2_dum (+ rationality_simple_dum, rationality_combine_dum, rationality_combine_exactexample_dum, rationality_prompt_dum (aggregation of indicator prompt scores)), rationality_simple2_gpt4o_system_dum, rationality_simple2_gpt4T_system_dum, rationality_simple2_small_dum, \n",
    "*incivility*: incivility_simple2_dum (+ incivility_simple_dum, incivility_combine_dum), incivility_prompt_dum (aggregation of indicator prompt scores), incivility_simple2_gpt4o_system_dum, incivility_simple2_gpt4T_system_dum, incivility_simple2_small_dum \n",
    "*interactivity*: interactivity_acknowledgement_simple_dum (+ interactivity_acknowledgement_simple2_dum), interactivity_acknowledgement_simple_gpt4o_system_dum, interactivity_acknowledgement_simple_gpt4T_system_dum, interactivity_acknowledgement_simple_small_dum (+interactivity_acknowledgement_simple_small2_dum)\n",
    "*diversity*: political_liberal_US_dum, political_conservative_US_dum, political_liberal_US_gpt4o_system_dum, political_liberal_US_gpt4T_system_dum, political_conservative_US_gpt4o_system_dum, political_conservative_US_gpt4T_system_dum, political_liberal_US_small_dum, political_conservative_US_small_dum\n",
    "*political_dum*: political_opinion_US_dum, political_opinion_US_gpt4o_system_dum, political_opinion_US_gpt4T_system_dum, political_opinion_US_small_dum (either liberal/conservative; Boukes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca488d78",
   "metadata": {},
   "source": [
    "#to be made prompts:\n",
    "#Naab:\n",
    " Argumente/ v75 - rationality in combination with quellenbelege?\n",
    "(Operationalisierung erstellt in Anlehnung an Ziegele & Quiring, 2015)\n",
    "Argumente sind Aussagen, die dazu dienen sollen, Behauptungen zu begründen oder zu widerlegen. Es wird kodiert, ob ein Nutzerkommentar Argumente verwendet, um eine oder mehrere geäußerte(n) Meinung(en) zu begründen.\n",
    "Von Interesse ist hier, ob die Nutzer ihre eigenen Aussagen mit Argumenten unterstützen oder ob diese unbegründet bleiben. Der Leser muss sich bei zusammenhängenden Aussagen im Kommentar fragen, ob die Frage nach dem „warum“ beantwortet wird. Anders gesagt: Gut erkennbar ist ein Argument, wenn man es problemlos mit einem Kausalsatz (weil...) an eine Behauptung anfügen kann oder wenn man eine begründende „Wenn-Dann-Beziehung“ zwischen den Sätzen herstellen kann. Eine Aneinanderreihung von Aussagen zählt nicht als Argumentation.\n",
    "! Achtung: Es geht hier nicht um die Qualität der Argumente! Es wird nur kodiert, ob Begründungen für Aussagen/Behauptungen/Positionen angeführt werden (s. Beispiele)!\n",
    "0\n",
    "Keine Argumente\n",
    "Im Kommentar werden keine Argumente verwendet. Behauptungen werden nicht begründet.\n",
    "Der Abtreibungsparagraph muss abgeschafft werden.\n",
    "Alles ist wichtiger als Fußball.\n",
    "Schade, dass die FDPs in Hessen nochmal geschafft hat aber für Schwarz-Gelb reicht‘s trotzdem nicht mehr. Dafür ist neben rot-rot-grün jetzt auch ‘ne klassische Ampel möglich. (Fragen nach dem „Warum“ bleiben unbeantwortet, Sätze lassen sich nicht mit „weil“ verknüpfen)\n",
    "1\n",
    "Mind. ein Argument\n",
    "Im Kommentar wird mindestens ein Argument, u.U. auch mehrere Argumente, formuliert.\n",
    "Deutschland hätte Snowden schon lange Asyl anbieten müssen: Schließlich ist er ein politisch Verfolgter, dessen Gesundheit auf dem Spiel steht, wenn er von seinen wildgewordenen Landsleuten aufgegriffen werden sollte.\n",
    "In erster Linie pflichte ich der GDL bei, (denn) sie demonstrieren wie es geht. In zweiter Linie möchte ich den Arbeitgebern das Armutszeugnis ausstellen, (denn) ich hab zum ersten Mal das Gefühl dass einer das richtige macht.\n",
    "62\n",
    "99\n",
    "Nicht eindeutig zuzuordnen.\n",
    "\n",
    "\n",
    "Quellenbelege im Kommentar/ v76\n",
    "Es wird kodiert, ob im Kommentar nachprüfbare Belege für Fakten, Tatsachenbehauptungen, Argumente oder Meinungen genannt werden. Dies passiert, indem auf Quellen verwiesen wird. Ein Argument, eine Prognose oder eine Werthaltung alleine sind kein Beleg. Der Beleg macht eine Aussage prinzipiell für jeden intersubjektiv nachprüfbar. Ein Verweis auf Quellen ist ausreichend, die Quellen müssen nicht (können aber) verlinkt sein. Werden zwar Fakten genannt, diese sind jedoch nicht belegt und nur mit größerem Aufwand intersubjektiv prüfbar, zählt dies nicht als Beleg.\n",
    "0\n",
    "Kein Beleg genannt\n",
    "Ich glaube, dass auch der Asylmissbrauch ein Problem ist. Aber sowas ist auch schwer zu erkennen.\n",
    "Assad wird weiter Chemiewaffen einsetzen, er weiß, er kann sich auf seine russischen und chinesischen Freunde verlassen. (Spekulation und Zukunftsprognosen sind keine Fakten)\n",
    "Alle außer Syrien und Iran haben die Amis im Sack und wenn das Problemchen gelöst wurde, dann wird wohl Russland spüren wer Herr im Hause ist und das ist das eigentliche Ziel der USA.\n",
    "In Frankreich gilt ein Mindestlohn von 11,50€, auch das trägt dazu bei, dass Deutschland mit seinem Niedriglohn den europäischen Nachbarn schwer zusetzt!\n",
    "Der Fingerabdrucksensor hat nichts mit NSA zu tun. Wer das denkt ist auf dem Holzweg. Der Fingerabdrucksensor ist ja nur für die Entsperrung des Telefons. 64Bit ist nice to have.\n",
    "1\n",
    "Mindestens ein Quellen-beleg genannt\n",
    "Der Kommentator enthält Verweise auf Quellen, mit\n",
    "Die Tagesschau meldete gestern neben den 20 Toten auch über 70 Verletzte.\n",
    "63\n",
    "denen Fakten oder sonstige Aussagen belegt werden. Dadurch sind sie prinzipiell für jeden intersubjektiv nachprüfbar. Ein Verweis auf Quellen ist ausreichend, sie müssen nicht (können aber) verlinkt sein.\n",
    "Die CDU kommt auf 41,5 Prozent, SPD, Grüne und Linke zusammen auf 42, 7. Und das kann sich jedes Kind auf Wikipedia ansehen: https://de.wikipedia.org/wiki/Bundestagswahl_2013\n",
    "99\n",
    "Nicht eindeutig zuzuordnen.\n",
    "\n",
    "\n",
    "Inziviles im Kommentar/ v711 - check definition\n",
    "(Operationalisierung erstellt in Anlehnung an Ziegele & Quiring, 2015; vgl. auch Coe, Kenski & Rains, 2014; Papacharissi, 2004)\n",
    "Inzivilität wird kodiert, wenn im Kommentar die folgenden Aussagetypen vorkommen, die einen unnötig respektlosen Ton gegenüber den Diskussionsteilnehmern, Themen, nicht-anwesenden Dritten oder Journalisten/Medien transportieren:\n",
    "•\n",
    "Beschimpfungen: Bösartige oder herabsetzende Wörter, die an Personen oder Personengruppen gerichtet sind.\n",
    "o\n",
    "Das verachtenswerte politische Unkraut FDP ist jetzt dort, wo es hingehört. (Thema Landtagswahl in Hessen)\n",
    "•\n",
    "Abfällige Bewertungen: Bösartige oder herabsetzende Wörter und Sätze, die gegen eine Idee, einen Plan, ein Verhalten oder eine Politik gerichtet sind.\n",
    "70\n",
    "o\n",
    "Klassisches Eigentor geschossen mit diesem Schrott Handy! (Thema iPhone-Vorstellung)\n",
    "•\n",
    "Lügen: Explizite oder implizite Aussagen, dass eine Idee, ein Plan, ein Verhalten oder eine Politik unehrlich sind.\n",
    "o\n",
    "Und welche \"Glaubwürdigkeit\" hat ein Präsident, der die Dauer-Lügen seiner Geheimdienste vertritt, ÜBERHAUPT noch? (Thema Syrien-Konflikt)\n",
    "•\n",
    "Vulgarität: Obszönitäten oder eine vulgäre Sprache, die im persönlichen Gespräch unangebracht wäre.\n",
    "o\n",
    "Es gibt genug Spinner, die ihre eigene unzureichende Schwanzlänge kompensieren müssen. (Thema iPhone-Vorstellung)\n",
    "•\n",
    "Abschätzige Bemerkungen: Herabsetzende Bemerkungen über die Art und Weise, wie eine Person auftritt oder kommuniziert.\n",
    "o\n",
    "Damit fühlst du dich nun überlegen und toll, hm? Infantiles Gehabe und Profilierung. (Thema Veggie-Day)\n",
    "•\n",
    "Stereotype: Eine vereinfachende, verallgemeinernde, schematische Reduzierung einer Erfahrung, Meinung oder Vorstellung auf ein meist verfestigtes, oft ungerechtfertigtes und emotional aufgeladenes Vorurteil. Stereotype bilden eine Voraussetzung für die Diskriminierung von Minderheiten und die Ausbildung von Feindbildern, Rassismus und Sexismus. Oftmals – aber nicht immer – ist die Kommunikation von Stereotypen politisch unkorrekt. So werden zum Beispiel einzelne Personen mit einer Gruppe gleichgesetzt, oder auf die Haupteigenschaften einer Gruppe reduziert.\n",
    "o\n",
    "Der Deutsche hasst den Mainstream, kauft bei KIK und Aldi, schmeißt 2 Jahre alte Handys auf den Müll und fährt mit Monatskarte zur Atomkraftdemo. So ambivalent spießig wie der/die Deutsche ist wohl keine Nation.\n",
    "o\n",
    "Frauen sind zum Putzen da!\n",
    "o\n",
    "Du bist doch ein Hauptschüler, so beschränkt wie du dich ausdrückst.\n",
    "•\n",
    "Absprechen von Rechten (individuell): Anderen Diskussionsteilnehmern werden Menschen- und Persönlichkeitsrechte abgesprochen\n",
    "z. B. Angriffe auf die Meinungsfreiheit oder persönliche Freiheit (Recht, Leben frei zu gestalten, auf finanziellen Wohlstand und gesicherte Existenz), Androhung von Gewalt, einschließlich impliziter Drohungen.\n",
    "o\n",
    "Dir gehört der Mund verboten!; Leute mit Einstellungen wie deiner sollten nicht wählen dürfen.\n",
    "o\n",
    "Die sollten alle im Meer ersaufen;\n",
    "•\n",
    "Bedrohung der Demokratie/ Gefährdung demokratischer Werte (gesellschaftlich): Der Kommentar trifft Aussagen, die vermuten lassen, dass die demokratischen Werte/ die Demokratie nicht wertgeschätzt oder diese gar bedroht werden.\n",
    "! Achtung: Die Artikulation von Kritik und Meinungsverschiedenheiten wird nicht als Inzivilität kodiert, sofern diese Artikulation angemessen höflich und sachlich erfolgt. In anderen Worten: Inzivilität ist nicht die Artikulation von Kritik und Meinungsverschiedenheiten an sich, sondern eine unangemessene Art und Weise, diese zu formulieren.\n",
    "0\n",
    "Keine Inzivilität\n",
    "Der Kommentar ist zivil und höflich formuliert. Er transportiert keinen unnötig respektlosen Ton ggü. Anderen bzw. enthält keine der oben genannten Aussagetypen.\n",
    "71\n",
    "1\n",
    "Vereinzelt inzivil\n",
    "Der Kommentar ist überwiegend zivil und höflich formuliert. Er enthält jedoch vereinzelt die o.g. Aussagetypen.\n",
    "2\n",
    "Überwiegend / ausschließlich inzivil\n",
    "Der Kommentar ist kaum oder nicht zivil und höflich formuliert. Er enthält überwiegend oder ausschließlich die o.g. Aussagetypen.\n",
    "99\n",
    "Nicht eindeutig zuzuordnen.\n",
    "\n",
    "\n",
    "Fragen/ v7135 -> interactivity if either?\n",
    "(Operationalisierung erstellt in Anlehnung an Ziegele & Quiring, 2015)\n",
    "Hier wird kodiert, ob Autor innerhalb eines Kommentars Informations-, Wissens- oder Verständnisdefizite aktiv kommunizieren und ausgleichen wollen, oder ob rhetorische Fragen gestellt werden, auf die eigentlich keine Antwort erwünscht ist. Es wird kodiert, ob überhaupt Fragen gestellt werden, nicht aber, an wen diese ggf. gerichtet sind.\n",
    "a)\n",
    "Echte Fragen\n",
    "Als echte Fragen werden alle Aussagen betrachtet, die mit einem Fragezeichen enden oder von ihrer Satzstruktur her als Frage aufgefasst werden können und die ein echtes Informationsbedürfnis anzeigen. Nur wenn eindeutig erkennbar ist, dass der Autor am Erhalt von Informationen/Meinungen interessiert ist, wird die Frage als „echt“ kodiert. Typen von „echten“ Fragen sind u. a. Wissens-, Einstellungs- und Verständnisfragen.\n",
    "Bsp.: Kennt sich hier einer mit dem Völkerrecht aus? Wieso kann Obama überhaupt überlegen einen ‘Alleingang‘ zu starten?\n",
    "b)\n",
    "Rhetorische Fragen\n",
    "Enthält der Kommentar eine oder mehrere rhetorische Fragen? Rhetorische Fragen werden vor allem als Argumentationsstrategie einsetzt, um die eigenen Positionen durchzusetzen. Sie zielen nicht auf Informationsgewinn ab, sondern der Autor will vorrangig Reaktionen wie Zustimmung bzw. Ablehnung erhalten.\n",
    "! Achtung: Rhetorische Fragen zusätzlich beim Diskussionsfaktor Kontroverse kodieren.\n",
    "Kodieranweisung: Rhetorische Frage schlägt echte Frage → sobald eine rhetorische Frage formuliert wird, wird eine „2“ kodiert."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88cc46f4",
   "metadata": {},
   "source": [
    "maybe do the following analyses:\n",
    "-baseline differences between slightly differing prompts on Boukes - best vs long vs best with slight word change vs long exact example vs prompt_dum (indicator aggregation) -> only have this for Llama3.1:70b\n",
    "-model differences on Boukes - best prompt Llama3.1:70b vs 8b vs GPT4T vs GPT4o\n",
    "-codebook differences on Boukes - Jaidka vs Boukes \n",
    "-dataset differences - best prompt Llama3.1:70b vs 8b vs GPT4o Jaidka vs Boukes -> need to run Llama3.1:70b on Jaidka, but no longer available via Trier -> could run both datasets with Llama3.3:70b instead\n",
    "-error analysis - how do differences between models overlap with differences between models and manual data\n",
    "-downstream effect on Boukes2024\n",
    "\n",
    "maybe leave out indicator aggregation prompt, only focus on best vs slight word change vs arbitrary change (Barrie ea 2025 prompt stability),  try run baseline for Boukes and Jaidka with GPT4o (azure) and Llama3.3:70b (Trier)\n",
    "compare model differences on Boukes also for other models if possible (but at least add L3.3:70b)\n",
    "\n",
    "drop Naab data? \n",
    "ask MH for english data for out of sample comparison on political_dum and ideology\n",
    "note we only have low-temperature anotations for Llama available"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48abfb9b",
   "metadata": {},
   "source": [
    "#TO DO:\n",
    "-classify Boukes, Jaidka and MHdata with options_low, Llama3.3:70b for Boukes and Jaidka prompts\n",
    "-classify Boukes with prompt stability/slightly different prompts with Llama3.3:70b and gpt4o -> use Barrie ea 2025 approach of paraphrasing prompt with increasing temperature, perhaps use Llama instead of pegasus for ease of use? - report prompt variations in appendix for manual validation\n",
    "-error analysis\n",
    "-downstream effects on Boukes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "18105311",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up helper variables and functions:\n",
    "CFG = config.Config()\n",
    "\n",
    "def load_json(path: str):\n",
    "    with open(path, encoding='utf-8') as fp:\n",
    "        return json.load(fp)\n",
    "    \n",
    "#set option variables:\n",
    "\n",
    "#set options to low temperature (0,1):\n",
    "options_low_str = \"\"\"\n",
    "seed: 42\n",
    "temperature: 0.1\n",
    "\"\"\"\n",
    "options_low = yaml.safe_load(options_low_str)\n",
    "\n",
    "#add alternative seed to options_low:\n",
    "options_low2_str = \"\"\"\n",
    "seed: 2\n",
    "temperature: 0.1\n",
    "\"\"\"\n",
    "options_low_seed2 = yaml.safe_load(options_low2_str)\n",
    "\n",
    "#apparently the 3.1 70b model is no longer available via Trier...\n",
    "MODELsmall: str = 'llama3.1:8b'\n",
    "MODEL33large: str = 'llama3.3:70b' # options: 'gemma:7b-instruct-q6_K', 'gemma2:27b-instruct-q6_K', 'llama3.1:8b-instruct-q6_K', 'llama3.1:70b-instruct-q6_K', 'mistral:7b-instruct-v0.3-q6_K', 'mistral-large:123b-instruct-2407-q6_K', 'mixtral:8x7b-instruct-v0.1-q6_K', 'mixtral:8x22b-instruct-v0.1-q6_K', 'phi3:14b-medium-128k-instruct-q6_K' or 'qwen2:72b-instruct-q6_K'\n",
    "MODELgpt4o = \"nf-gpt-4o-2024-08-06\" # in principe is er nu van elk model een nf (no filter) en een normale versie beschikbaar, de no filter versies zijn alleen voor onderzoekers beschikbaar voor analyze van content die niet door de filter heen zou komen.\n",
    "MODELgpt4T = \"nf-gpt-4-turbo\" # Can be gpt-35-turbo, gpt-4-turbo, gpt-4 or Meta-Llama-3-8B-Instruct.\n",
    "MODEL33largeAzure = 'Llama-3.3-70B-Instruct' #azureml://registries/azureml-meta/models/Llama-3.3-70B-Instruct/versions/4 / options:  \"data\": [\n",
    "MODEL31largeAzureNF = 'nf-Llama-3.1-70b-instruct'\n",
    "MODELDSR1_70b = 'deepseek-r1:70b' # DeepSeek-R1 (china)\n",
    "MODELDSR1_7b = 'deepseek-r1:7b' # DeepSeek-R1 (china)\n",
    "MODELQ25_72b = 'qwen2.5:72b' # Qwen2.5 70b (china)\n",
    "\n",
    "\n",
    "options_zero_str = \"\"\"\n",
    "seed: 42\n",
    "temperature: 0\n",
    "\"\"\"\n",
    "options_zero = yaml.safe_load(options_zero_str)\n",
    "\n",
    "options_zero2_str = \"\"\"\n",
    "seed: 2\n",
    "temperature: 0\n",
    "\"\"\"\n",
    "options_zero_seed2 = yaml.safe_load(options_zero2_str)\n",
    "\n",
    "options_DS_zero_str = \"\"\"\n",
    "seed: 42\n",
    "temperature: 0\n",
    "max_tokens: 1\n",
    "\"\"\"\n",
    "options_DS_zero = yaml.safe_load(options_DS_zero_str)\n",
    "\n",
    "temperature_0 : int = 0\n",
    "SEED: int = 42\n",
    "SEED2: int = 43\n",
    "MAX10: int = 10\n",
    "TOPP1: int = 1\n",
    "\n",
    "options_creative_str = \"\"\"\n",
    "seed: 42\n",
    "temperature: 0.7\n",
    "topp: 0.8\n",
    "\"\"\"\n",
    "options_creative = yaml.safe_load(options_creative_str)\n",
    "\n",
    "options_large_str = \"\"\"\n",
    "seed: 42\n",
    "temperature: 0\n",
    "num_predict: 2000\n",
    "\"\"\"\n",
    "options_large = yaml.safe_load(options_large_str)\n",
    "\n",
    "#load environment variables:\n",
    "api_key = os.environ.get('sjoerd_key')\n",
    "\n",
    "#setttings:\n",
    "api_endpoint = \"https://ai-research-proxy.azurewebsites.net/chat/completions\"\n",
    "api_endpoint_embed = \"https://ai-research-proxy.azurewebsites.net/embeddings\"\n",
    "####### API REQUEST FORMATTING ######\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"Authorization\": \"Bearer \" + api_key\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "1e9363f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the datasets to merge results with:\n",
    "dataset_w_pred_2 = pd.read_json(f'{CFG.report_dir}/publicsphere.cardiff_prompt_classify_s.json')\n",
    "dataset_w_pred_anon = pd.read_parquet('data/publicsphere/publicsphere.cardiff_prompt_classify_anon.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b072300",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "classifying incivility_para1: 100%|██████████| 3132/3132 [15:44<00:00,  3.32it/s]\n",
      "classifying incivility_para2: 100%|██████████| 3132/3132 [15:20<00:00,  3.40it/s]\n",
      "classifying incivility_simpa1: 100%|██████████| 3132/3132 [15:19<00:00,  3.41it/s]\n",
      "classifying interactivity_acknowledgement_simple_para1: 100%|██████████| 3132/3132 [15:20<00:00,  3.40it/s]\n",
      "classifying interactivity_acknowledgement_simple_para2: 100%|██████████| 3132/3132 [15:20<00:00,  3.40it/s]\n",
      "classifying interactivity_acknowledgement_simple_simpa1: 100%|██████████| 3132/3132 [15:14<00:00,  3.42it/s]\n"
     ]
    }
   ],
   "source": [
    "#classify Boukes with Llama3.3:70b\n",
    "\n",
    "pubspherepromptsrun1 = ['rationality_simple2']\n",
    "pubspherepromptsrun2 = ['rationality_jaidka', 'incivility_simple2', 'incivility_jaidka',  'civility_jaidka' ]  \n",
    "pubspherepromptsrun3 = ['interactivity_acknowledgement_simple', 'interactivity_acknowledgement_jaidka', 'political_ideology_US', 'political_post', 'political_post_jaidka' ]  \n",
    "pubspherepromptsrun4a = ['rationality_simple2_para1']\n",
    "pubspherepromptsrun4 = ['incivility_simple2_para1', 'political_ideology_US_para1', 'political_post_para1',\n",
    "                        'rationality_simple2_para2', 'incivility_simple2_para2', 'political_ideology_US_para2', 'political_post_para2', \n",
    "                        'interactivity_acknowledgement_simple_para1', 'interactivity_acknowledgement_simple_para2',\n",
    "                        'interactivity_acknowledgement_simple_simpa1', 'rationality_simple2_simpa1', 'incivility_simple2_simpa1', 'political_ideology_US_simpa1', 'political_post_simpa1'] \n",
    "pubspherepromptsrun4b = ['incivility_para1', 'incivility_para2', \n",
    "                        'interactivity_acknowledgement_para1', 'interactivity_acknowledgement_para2',\n",
    "                        'interactivity_acknowledgement_simpa1', 'incivility_simpa1'] \n",
    "\n",
    "#then change seed to 2 (options_low_seed2) to run the intraprompt annotation reliability benchmark for the Boukes2024 prompts:\n",
    "pubspherepromptsrun_intra = ['rationality_simple2', 'incivility_simple2',  'interactivity_acknowledgement_simple',\n",
    "    'political_ideology_US', 'political_post']\n",
    "\n",
    "#compare to annotations of Llama3.1:8b - only need to run the prompts not already run in previous analyses:\n",
    "pubspherepromptsrun_L8b = ['rationality_jaidka', 'incivility_jaidka',  'civility_jaidka', 'political_ideology_US',\n",
    " 'interactivity_acknowledgement_jaidka', 'political_post', 'political_post_jaidka', 'rationality_simple2_para1', 'incivility_para1', \n",
    " 'interactivity_acknowledgement_para1', 'political_ideology_US_para1', 'political_post_para1', 'rationality_simple2_para2', 'incivility_para2', \n",
    " 'interactivity_acknowledgement_para2', 'political_ideology_US_para2', 'political_post_para2', 'interactivity_acknowledgement_simpa1', 'rationality_simple2_simpa1', \n",
    " 'incivility_simpa1', 'political_ideology_US_simpa1', 'political_post_simpa1']\n",
    "\n",
    "#compare to annotations of deepseek-r1:7b:\n",
    "pubspherepromptsrun_DS = ['rationality_simple2', 'incivility_simple2',  'interactivity_acknowledgement_simple',\n",
    "    'political_ideology_US', 'political_post']\n",
    "\n",
    "#compare to annotations of Qwen2.5:72b:\n",
    "pubspherepromptsrun_Q = ['rationality_simple2', 'incivility_simple2', 'rationality_jaidka', 'incivility_jaidka',  'civility_jaidka', 'interactivity_acknowledgement_simple',\n",
    " 'interactivity_acknowledgement_jaidka', 'political_post', 'political_post_jaidka', 'political_ideology_US', 'rationality_simple2_para1', 'incivility_para1', \n",
    " 'interactivity_acknowledgement_para1', 'political_ideology_US_para1', 'political_post_para1', 'rationality_simple2_para2', 'incivility_para2', \n",
    " 'interactivity_acknowledgement_para2', 'political_ideology_US_para2', 'political_post_para2', 'interactivity_acknowledgement_simpa1', 'rationality_simple2_simpa1', \n",
    " 'incivility_simpa1', 'political_ideology_US_simpa1', 'political_post_simpa1']\n",
    "\n",
    "\n",
    "#then change seed to 2 (options_low_seed2) to run the intraprompt annotation reliability benchmark for the Boukes2024 prompts with Llama3.1:8b:\n",
    "pubspherepromptsrun_intra_L8b = ['rationality_simple2', 'incivility_simple2',  'interactivity_acknowledgement_simple',\n",
    "    'political_ideology_US', 'political_post']\n",
    "\n",
    "#run prompts of concepts annotated in Jaidka2024 for Jaidka2024 data with Llama3.3:70b:\n",
    "pubspherepromptsrun_jaidka = ['rationality_simple2', 'rationality_jaidka', 'incivility_simple2', 'incivility_jaidka',  'civility_jaidka',\n",
    " 'interactivity_acknowledgement_simple', 'interactivity_acknowledgement_jaidka',  'political_post', 'political_post_jaidka']\n",
    "\n",
    "#repeat for Jaidka2024 data with Llama3.1:8b:\n",
    "pubspherepromptsrun_jaidka_L8b = ['rationality_simple2', 'rationality_jaidka', 'incivility_simple2', 'incivility_jaidka',  'civility_jaidka',\n",
    " 'interactivity_acknowledgement_simple', 'interactivity_acknowledgement_jaidka',  'political_post', 'political_post_jaidka']\n",
    "\n",
    "#run prompts of concepts annotated in MH2024 for MH2024 data with Llama3.3:70b:   \n",
    "##Need to check if the prompts for political_post originates from Boukes or MH2024 or whether it is the same as the Boukes2024 prompt\n",
    "#included the para1 prompts so we have an out of sample test for prompt rewording effects, also for DS7b, which might be too computationally expansive for all prompts and the larger Boukes or Jaidka dataset.\n",
    "pubspherepromptsrun_MH = ['political_ideology_US', 'political_ideology_US_para1']\n",
    "#repeat for MH2024 data with Llama3.1:8b:\n",
    "pubspherepromptsrun_MH_8b = ['political_ideology_US', 'political_ideology_US_para1']\n",
    "#repeat for MH2024 data with Qwen2.5:72b:\n",
    "pubspherepromptsrun_Q72b = ['political_ideology_US', 'political_ideology_US_para1']\n",
    "\n",
    "#check zero temperature for Llama3.3:70b [?] on Boukes2024 data:\n",
    "pubspherepromptsrun_zero_L33_70b = ['rationality_simple2', 'incivility_simple2', 'rationality_jaidka', 'incivility_jaidka',  'civility_jaidka', 'interactivity_acknowledgement_simple',\n",
    " 'interactivity_acknowledgement_jaidka', 'political_post', 'political_post_jaidka', 'political_ideology_US']\n",
    "\n",
    "#pubspherepromptsrunall = ['rationality_simple2', 'rationality_jaidka', 'incivility_simple2', 'incivility_jaidka',  'civility_jaidka', 'interactivity_acknowledgement_simple', \n",
    "# 'interactivity_acknowledgement_jaidka', 'political_ideology_US', 'political_post', 'political_post_jaidka', 'rationality_simple2_para1', 'incivility_simple2_para1', \n",
    "# 'interactivity_acknowledgement_simple_para1', 'political_ideology_US_para1', 'political_post_para1', 'rationality_simple2_para2', 'incivility_simple2_para2', \n",
    "# 'interactivity_acknowledgement_simple_para2', 'political_ideology_US_para2', 'political_post_para2', 'interactivity_acknowledgement_simple_simpa1', 'rationality_simple2_simpa1', \n",
    "# 'incivility_simple2_simpa1', 'political_ideology_US_simpa1', 'political_post_simpa1'] \n",
    " \n",
    "\n",
    "predictions4b: typing.Dict[str, np.ndarray] = {\n",
    "    label: (\n",
    "        src.PromptClassify\n",
    "        .from_json(path)\n",
    "        (boukesTYT[\"commentText\"], model=MODEL33large, options=options_low)\n",
    "    )\n",
    "    for label, path in CFG.prompt_classify_files.items() if label in pubspherepromptsrun4b\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "87dfa462",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "classifying incivility_simple2: 100%|██████████| 3132/3132 [08:01<00:00,  6.50it/s]\n",
      "classifying interactivity_acknowledgement_simple: 100%|██████████| 3132/3132 [18:13<00:00,  2.87it/s]\n",
      "classifying political_ideology_US: 100%|██████████| 3132/3132 [08:29<00:00,  6.15it/s]\n",
      "classifying political_post: 100%|██████████| 3132/3132 [07:47<00:00,  6.70it/s]\n",
      "classifying rationality_simple2: 100%|██████████| 3132/3132 [07:44<00:00,  6.74it/s]\n"
     ]
    }
   ],
   "source": [
    "#classify Boukes with Llama3.3:8b\n",
    "\n",
    "#then change seed to 2 (options_low_seed2) to run the intraprompt annotation reliability benchmark for the Boukes2024 prompts:\n",
    "pubspherepromptsrun_intra_L8b = ['rationality_simple2', 'incivility_simple2',  'interactivity_acknowledgement_simple',\n",
    "    'political_ideology_US', 'political_post']\n",
    "\n",
    "\n",
    "predictions_intra_L8b: typing.Dict[str, np.ndarray] = {\n",
    "    label: (\n",
    "        src.PromptClassify\n",
    "        .from_json(path)\n",
    "        (boukesTYT[\"commentText\"], model=MODELsmall, options=options_low_seed2)\n",
    "    )\n",
    "    for label, path in CFG.prompt_classify_files.items() if label in pubspherepromptsrun_intra_L8b\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "77c8e2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save output:\n",
    "pd.DataFrame(predictions4b).to_parquet('data/publicsphere/predictions4b.parquet', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "78ab72dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save output:\n",
    "pd.DataFrame(predictions_intra_L8b).to_parquet('data/publicsphere/predictions_intra_L8b.parquet', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "fc30b75c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "classifying civility_jaidka: 100%|██████████| 3132/3132 [07:53<00:00,  6.61it/s]\n",
      "classifying incivility_jaidka: 100%|██████████| 3132/3132 [07:54<00:00,  6.60it/s]\n",
      "classifying incivility_para1: 100%|██████████| 3132/3132 [15:49<00:00,  3.30it/s]\n",
      "classifying incivility_para2: 100%|██████████| 3132/3132 [07:48<00:00,  6.68it/s]\n",
      "classifying incivility_simpa1: 100%|██████████| 3132/3132 [07:37<00:00,  6.85it/s]\n",
      "classifying reciprocity_jaidka: 100%|██████████| 3132/3132 [07:54<00:00,  6.61it/s]\n",
      "classifying interactivity_acknowledgement_simple_para1: 100%|██████████| 3132/3132 [07:33<00:00,  6.91it/s]\n",
      "classifying interactivity_acknowledgement_simple_para2: 100%|██████████| 3132/3132 [22:28<00:00,  2.32it/s]\n",
      "classifying interactivity_acknowledgement_simple_simpa1: 100%|██████████| 3132/3132 [07:57<00:00,  6.56it/s]\n",
      "classifying political_ideology_US_para1: 100%|██████████| 3132/3132 [10:13<00:00,  5.10it/s]\n",
      "classifying political_ideology_US_para2: 100%|██████████| 3132/3132 [10:18<00:00,  5.07it/s]\n",
      "classifying political_ideology_US_simpa1: 100%|██████████| 3132/3132 [09:04<00:00,  5.75it/s]\n",
      "classifying political_post: 100%|██████████| 3132/3132 [07:51<00:00,  6.65it/s]\n",
      "classifying political_post_jaidka: 100%|██████████| 3132/3132 [07:32<00:00,  6.93it/s]\n",
      "classifying political_post_para1: 100%|██████████| 3132/3132 [07:58<00:00,  6.55it/s]\n",
      "classifying political_post_para2: 100%|██████████| 3132/3132 [07:51<00:00,  6.65it/s]\n",
      "classifying political_post_simpa1: 100%|██████████| 3132/3132 [07:55<00:00,  6.59it/s]\n",
      "classifying rationality_jaidka: 100%|██████████| 3132/3132 [08:50<00:00,  5.90it/s]\n",
      "classifying rationality_simple2_para1: 100%|██████████| 3132/3132 [08:29<00:00,  6.14it/s]\n",
      "classifying rationality_simple2_para2: 100%|██████████| 3132/3132 [11:45<00:00,  4.44it/s]\n",
      "classifying rationality_simple2_simpa1: 100%|██████████| 3132/3132 [07:32<00:00,  6.92it/s]\n"
     ]
    }
   ],
   "source": [
    "#classify Boukes with Llama3.1:8b\n",
    "pubspherepromptsrun_L8b = ['rationality_jaidka', 'incivility_jaidka',  'civility_jaidka',\n",
    " 'interactivity_acknowledgement_jaidka', 'political_post', 'political_post_jaidka', 'rationality_simple2_para1', 'incivility_para1', \n",
    " 'interactivity_acknowledgement_para1', 'political_ideology_US_para1', 'political_post_para1', 'rationality_simple2_para2', 'incivility_para2', \n",
    " 'interactivity_acknowledgement_para2', 'political_ideology_US_para2', 'political_post_para2', 'interactivity_acknowledgement_simpa1', 'rationality_simple2_simpa1', \n",
    " 'incivility_simpa1', 'political_ideology_US_simpa1', 'political_post_simpa1']\n",
    "\n",
    "predictions_L8b: typing.Dict[str, np.ndarray] = {\n",
    "    label: (\n",
    "        src.PromptClassify\n",
    "        .from_json(path)\n",
    "        (boukesTYT[\"commentText\"], model=MODELsmall, options=options_low)\n",
    "    )\n",
    "    for label, path in CFG.prompt_classify_files.items() if label in pubspherepromptsrun_L8b\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "113d65a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save output:\n",
    "pd.DataFrame(predictions_L8b).to_parquet('data/publicsphere/predictions_L8b.parquet', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "3a311e1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "classifying civility_jaidka: 100%|██████████| 5585/5585 [27:00<00:00,  3.45it/s]\n",
      "classifying incivility_jaidka: 100%|██████████| 5585/5585 [27:11<00:00,  3.42it/s]\n",
      "classifying incivility_simple2: 100%|██████████| 5585/5585 [26:39<00:00,  3.49it/s]\n",
      "classifying reciprocity_jaidka: 100%|██████████| 5585/5585 [26:44<00:00,  3.48it/s]\n",
      "classifying interactivity_acknowledgement_simple: 100%|██████████| 5585/5585 [26:39<00:00,  3.49it/s]\n",
      "classifying political_post: 100%|██████████| 5585/5585 [26:43<00:00,  3.48it/s]\n",
      "classifying political_post_jaidka: 100%|██████████| 5585/5585 [26:39<00:00,  3.49it/s]\n",
      "classifying rationality_jaidka: 100%|██████████| 5585/5585 [26:45<00:00,  3.48it/s]\n",
      "classifying rationality_simple2: 100%|██████████| 5585/5585 [26:39<00:00,  3.49it/s]\n"
     ]
    }
   ],
   "source": [
    "#run prompts of concepts annotated in Jaidka2024 for Jaidka2024 data with Llama3.3:70b:\n",
    "pubspherepromptsrun_jaidka = ['rationality_simple2', 'rationality_jaidka', 'incivility_simple2', 'incivility_jaidka',  'civility_jaidka',\n",
    " 'interactivity_acknowledgement_simple', 'interactivity_acknowledgement_jaidka',  'political_post', 'political_post_jaidka']\n",
    "\n",
    "predictions_jaidka: typing.Dict[str, np.ndarray] = {\n",
    "    label: (\n",
    "        src.PromptClassify\n",
    "        .from_json(path)\n",
    "        (jaidka[\"message\"], model=MODEL33large, options=options_zero)\n",
    "    )\n",
    "    for label, path in CFG.prompt_classify_files.items() if label in pubspherepromptsrun_jaidka\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "fe92aca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "classifying civility_jaidka: 100%|██████████| 5585/5585 [13:25<00:00,  6.93it/s]\n",
      "classifying incivility_jaidka: 100%|██████████| 5585/5585 [13:52<00:00,  6.71it/s]\n",
      "classifying incivility_simple2: 100%|██████████| 5585/5585 [13:25<00:00,  6.93it/s]\n",
      "classifying reciprocity_jaidka: 100%|██████████| 5585/5585 [13:38<00:00,  6.82it/s]\n",
      "classifying interactivity_acknowledgement_simple: 100%|██████████| 5585/5585 [22:20<00:00,  4.17it/s]\n",
      "classifying political_post: 100%|██████████| 5585/5585 [13:28<00:00,  6.91it/s]\n",
      "classifying political_post_jaidka: 100%|██████████| 5585/5585 [13:05<00:00,  7.11it/s]\n"
     ]
    }
   ],
   "source": [
    "#repeat for Jaidka2024 data with Llama3.1:8b, for the prompts not already classified earlier for this model:\n",
    "pubspherepromptsrun_jaidka_L8b = ['incivility_simple2', 'incivility_jaidka',  'civility_jaidka',\n",
    " 'interactivity_acknowledgement_simple', 'interactivity_acknowledgement_jaidka',  'political_post', 'political_post_jaidka']\n",
    "\n",
    "predictions_jaidka_L8b: typing.Dict[str, np.ndarray] = {\n",
    "    label: (\n",
    "        src.PromptClassify\n",
    "        .from_json(path)\n",
    "        (jaidka[\"message\"], model=MODELsmall, options=options_zero)\n",
    "    )\n",
    "    for label, path in CFG.prompt_classify_files.items() if label in pubspherepromptsrun_jaidka_L8b\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "e3dda628",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save output:\n",
    "pd.DataFrame(predictions_jaidka_L8b).to_parquet('data/publicsphere/predictions_jaidka_L8b.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3426916b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#note that lower performance of models on Jaidka2024 is noteworthy since these annotations are openly available and potentially part of the training data for these models, so they should perform better than on Boukes2024 data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "80bd462b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "classifying incivility_simple2: 100%|██████████| 3132/3132 [16:56<00:00,  3.08it/s]\n",
      "classifying interactivity_acknowledgement_simple: 100%|██████████| 3132/3132 [17:03<00:00,  3.06it/s]\n",
      "classifying political_ideology_US: 100%|██████████| 3132/3132 [17:01<00:00,  3.07it/s]\n",
      "classifying political_post: 100%|██████████| 3132/3132 [17:09<00:00,  3.04it/s]\n",
      "classifying rationality_simple2: 100%|██████████| 3132/3132 [16:54<00:00,  3.09it/s]\n"
     ]
    }
   ],
   "source": [
    "#compare to annotations of Qwen2.5:72b:\n",
    "#pubspherepromptsrun_Q = ['rationality_simple2', 'incivility_simple2', 'rationality_jaidka', 'incivility_jaidka',  'civility_jaidka', 'interactivity_acknowledgement_simple',\n",
    "# 'interactivity_acknowledgement_jaidka', 'political_post', 'political_post_jaidka', 'political_ideology_US', 'rationality_simple2_para1', 'incivility_para1', \n",
    "# 'interactivity_acknowledgement_para1', 'political_ideology_US_para1', 'political_post_para1', 'rationality_simple2_para2', 'incivility_para2', \n",
    "# 'interactivity_acknowledgement_para2', 'political_ideology_US_para2', 'political_post_para2', 'interactivity_acknowledgement_simpa1', 'rationality_simple2_simpa1', \n",
    "# 'incivility_simpa1', 'political_ideology_US_simpa1', 'political_post_simpa1']\n",
    "\n",
    "pubspherepromptsrun_Q1 = ['rationality_simple2', 'incivility_simple2', 'interactivity_acknowledgement_simple', 'political_post', 'political_ideology_US']\n",
    "\n",
    "predictions_Q72b_boukes: typing.Dict[str, np.ndarray] = {\n",
    "    label: (\n",
    "        src.PromptClassify\n",
    "        .from_json(path)\n",
    "        (boukesTYT[\"commentText\"], model=MODELQ25_72b, options=options_zero)\n",
    "    )\n",
    "    for label, path in CFG.prompt_classify_files.items() if label in pubspherepromptsrun_Q1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "4afff1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save output:\n",
    "pd.DataFrame(predictions_Q72b_boukes).to_parquet('data/publicsphere/predictions_Q72b_boukes.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "8684c051",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "classifying incivility_para2:   0%|          | 0/3132 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "classifying incivility_para2: 100%|██████████| 3132/3132 [17:10<00:00,  3.04it/s]\n",
      "classifying incivility_simpa1: 100%|██████████| 3132/3132 [16:54<00:00,  3.09it/s]\n",
      "classifying interactivity_acknowledgement_simple_para2: 100%|██████████| 3132/3132 [16:56<00:00,  3.08it/s]\n",
      "classifying interactivity_acknowledgement_simple_simpa1: 100%|██████████| 3132/3132 [16:53<00:00,  3.09it/s]  \n",
      "classifying political_ideology_US_para2: 100%|██████████| 3132/3132 [16:51<00:00,  3.10it/s]\n",
      "classifying political_ideology_US_simpa1: 100%|██████████| 3132/3132 [16:47<00:00,  3.11it/s]\n",
      "classifying political_post_para2: 100%|██████████| 3132/3132 [16:56<00:00,  3.08it/s]\n",
      "classifying political_post_simpa1: 100%|██████████| 3132/3132 [16:57<00:00,  3.08it/s]\n",
      "classifying rationality_simple2_para2: 100%|██████████| 3132/3132 [16:51<00:00,  3.10it/s]\n",
      "classifying rationality_simple2_simpa1: 100%|██████████| 3132/3132 [16:48<00:00,  3.11it/s]\n"
     ]
    }
   ],
   "source": [
    "#compare to annotations of Qwen2.5:72b:\n",
    "pubspherepromptsrun_Q2 = [ 'rationality_jaidka', 'incivility_jaidka',  'civility_jaidka', \n",
    " 'interactivity_acknowledgement_jaidka', 'political_post_jaidka', 'rationality_simple2_para1', 'incivility_para1', \n",
    " 'interactivity_acknowledgement_para1', 'political_ideology_US_para1', 'political_post_para1']\n",
    "pubspherepromptsrun_Q3 = ['rationality_simple2_para2', 'incivility_para2', \n",
    " 'interactivity_acknowledgement_para2', 'political_ideology_US_para2', 'political_post_para2', 'interactivity_acknowledgement_simpa1', 'rationality_simple2_simpa1', \n",
    " 'incivility_simpa1', 'political_ideology_US_simpa1', 'political_post_simpa1']\n",
    "\n",
    "\n",
    "predictions_Q72b_boukes3: typing.Dict[str, np.ndarray] = {\n",
    "    label: (\n",
    "        src.PromptClassify\n",
    "        .from_json(path)\n",
    "        (boukesTYT[\"commentText\"], model=MODELQ25_72b, options=options_zero)\n",
    "    )\n",
    "    for label, path in CFG.prompt_classify_files.items() if label in pubspherepromptsrun_Q3\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73660d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save output:\n",
    "pd.DataFrame(predictions_Q72b_boukes3).to_parquet('data/publicsphere/predictions_Q72b_boukes3.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f5d6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: run Qwen2.5:72b on Jaidka2024 data (vanavond +- 4.5u), (rest vannacht +- 9u) rerun Qwen2.5:72b on Boukes2024 data with different seed (options_zero).\n",
    "# rerun all default Boukes2024-prompts (also seed2) with options_zero for Llama3.3:70b and Llama3.1:8b, to compare to Boukes2024 results with options_low.\n",
    "# potentially, but long compute: rerun all Boukes2024 analyses with options_zero for Llama3.3:70b and Llama3.1:8b, to compare to Boukes2024 results with options_low.\n",
    "\n",
    "# jaidkapromptsrun_Q72b2 = ['rationality_simple2'] -> fix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "659156be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "classifying incivility_simple2: 100%|██████████| 3132/3132 [16:39<00:00,  3.13it/s]\n",
      "classifying interactivity_acknowledgement_simple: 100%|██████████| 3132/3132 [16:45<00:00,  3.12it/s]\n",
      "classifying political_ideology_US: 100%|██████████| 3132/3132 [16:42<00:00,  3.12it/s]\n",
      "classifying political_post: 100%|██████████| 3132/3132 [16:50<00:00,  3.10it/s]\n",
      "classifying rationality_simple2: 100%|██████████| 3132/3132 [16:44<00:00,  3.12it/s]\n"
     ]
    }
   ],
   "source": [
    "#compare to annotations of Qwen2.5:72b:\n",
    "pubspherepromptsrun_Q1 = ['rationality_simple2', 'incivility_simple2', 'interactivity_acknowledgement_simple', 'political_post', 'political_ideology_US']\n",
    "\n",
    "predictions_Q72b_boukes_seed2: typing.Dict[str, np.ndarray] = {\n",
    "    label: (\n",
    "        src.PromptClassify\n",
    "        .from_json(path)\n",
    "        (boukesTYT[\"commentText\"], model=MODELQ25_72b, options=options_zero_seed2)\n",
    "    )\n",
    "    for label, path in CFG.prompt_classify_files.items() if label in pubspherepromptsrun_Q1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "9ab05fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save output:\n",
    "pd.DataFrame(predictions_Q72b_boukes_seed2).to_parquet('data/publicsphere/predictions_Q72b_boukes_seed2.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "8bc6827f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "classifying incivility_simple2: 100%|██████████| 3132/3132 [15:39<00:00,  3.33it/s]\n",
      "classifying interactivity_acknowledgement_simple: 100%|██████████| 3132/3132 [15:20<00:00,  3.40it/s]\n",
      "classifying political_ideology_US: 100%|██████████| 3132/3132 [15:14<00:00,  3.43it/s]\n",
      "classifying political_post: 100%|██████████| 3132/3132 [15:15<00:00,  3.42it/s]\n",
      "classifying rationality_simple2: 100%|██████████| 3132/3132 [15:14<00:00,  3.42it/s]\n"
     ]
    }
   ],
   "source": [
    "#compare to annotations of Llama3.3:70b with zero temperature:\n",
    "pubspherepromptsrun_Q1 = ['rationality_simple2', 'incivility_simple2', 'interactivity_acknowledgement_simple', 'political_post', 'political_ideology_US']\n",
    "\n",
    "predictions_boukes_L33_70b_zero: typing.Dict[str, np.ndarray] = {\n",
    "    label: (\n",
    "        src.PromptClassify\n",
    "        .from_json(path)\n",
    "        (boukesTYT[\"commentText\"], model=MODEL33large, options=options_zero)\n",
    "    )\n",
    "    for label, path in CFG.prompt_classify_files.items() if label in pubspherepromptsrun_Q1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "id": "5ed1a552",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "incivility_simple2\n",
      "Yes    2034\n",
      "No     1098\n",
      "Name: count, dtype: int64\n",
      "------------------------------------------\n",
      "interactivity_acknowledgement_simple\n",
      "No     2506\n",
      "Yes     626\n",
      "Name: count, dtype: int64\n",
      "------------------------------------------\n",
      "political_ideology_US\n",
      "neutral         1545\n",
      "conservative     920\n",
      "liberal          667\n",
      "Name: count, dtype: int64\n",
      "------------------------------------------\n",
      "political_post\n",
      "political        1933\n",
      "non-political    1199\n",
      "Name: count, dtype: int64\n",
      "------------------------------------------\n",
      "rationality_simple2\n",
      "No     2924\n",
      "Yes     208\n",
      "Name: count, dtype: int64\n",
      "------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#join to the dataset:   \n",
    "for _, preds in predictions_Q72b_boukes_low_seed2.items():\n",
    "    print(preds.value_counts())\n",
    "    print(\"-\" * 42)\n",
    "    dataset_w_pred_2 = dataset_w_pred_2.join(preds, rsuffix='_Q25_72b_low_seed2')\n",
    "    dataset_w_pred_anon = dataset_w_pred_anon.join(preds, rsuffix='_Q25_72b_low_seed2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "7d7e8ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save output:\n",
    "pd.DataFrame(predictions_boukes_L33_70b_zero).to_parquet('data/publicsphere/predictions_boukes_L33_70b_zero.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "91402732",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "classifying incivility_simple2: 100%|██████████| 3132/3132 [15:28<00:00,  3.37it/s]\n",
      "classifying interactivity_acknowledgement_simple: 100%|██████████| 3132/3132 [15:21<00:00,  3.40it/s]\n",
      "classifying political_ideology_US: 100%|██████████| 3132/3132 [15:21<00:00,  3.40it/s]\n",
      "classifying political_post: 100%|██████████| 3132/3132 [15:20<00:00,  3.40it/s]\n",
      "classifying rationality_simple2: 100%|██████████| 3132/3132 [15:16<00:00,  3.42it/s]\n"
     ]
    }
   ],
   "source": [
    "#compare to annotations of Llama3.3:70b with zero temperature and different seed:\n",
    "pubspherepromptsrun_Q1 = ['rationality_simple2', 'incivility_simple2', 'interactivity_acknowledgement_simple', 'political_post', 'political_ideology_US']\n",
    "\n",
    "predictions_boukes_L33_70b_zero_seed2: typing.Dict[str, np.ndarray] = {\n",
    "    label: (\n",
    "        src.PromptClassify\n",
    "        .from_json(path)\n",
    "        (boukesTYT[\"commentText\"], model=MODEL33large, options=options_zero_seed2)\n",
    "    )\n",
    "    for label, path in CFG.prompt_classify_files.items() if label in pubspherepromptsrun_Q1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "6c00689c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save output:\n",
    "pd.DataFrame(predictions_boukes_L33_70b_zero_seed2).to_parquet('data/publicsphere/predictions_boukes_L33_70b_zero_seed2.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "3bb1cbb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "classifying incivility_simple2: 100%|██████████| 3132/3132 [07:56<00:00,  6.57it/s]\n",
      "classifying interactivity_acknowledgement_simple: 100%|██████████| 3132/3132 [18:04<00:00,  2.89it/s]\n",
      "classifying political_ideology_US: 100%|██████████| 3132/3132 [08:24<00:00,  6.20it/s]\n",
      "classifying political_post: 100%|██████████| 3132/3132 [07:53<00:00,  6.61it/s]\n",
      "classifying rationality_simple2: 100%|██████████| 3132/3132 [07:45<00:00,  6.72it/s]\n"
     ]
    }
   ],
   "source": [
    "#compare to annotations of Llama3.1:8b with zero temperature:\n",
    "pubspherepromptsrun_Q1 = ['rationality_simple2', 'incivility_simple2', 'interactivity_acknowledgement_simple', 'political_post', 'political_ideology_US']\n",
    "\n",
    "predictions_boukes_L31_8b_zero: typing.Dict[str, np.ndarray] = {\n",
    "    label: (\n",
    "        src.PromptClassify\n",
    "        .from_json(path)\n",
    "        (boukesTYT[\"commentText\"], model=MODELsmall, options=options_zero)\n",
    "    )\n",
    "    for label, path in CFG.prompt_classify_files.items() if label in pubspherepromptsrun_Q1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "a16037e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save output:\n",
    "pd.DataFrame(predictions_boukes_L31_8b_zero).to_parquet('data/publicsphere/predictions_boukes_L31_8b_zero.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "7b347eaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "classifying incivility_simple2: 100%|██████████| 3132/3132 [07:46<00:00,  6.72it/s]\n",
      "classifying interactivity_acknowledgement_simple: 100%|██████████| 3132/3132 [17:22<00:00,  3.00it/s]\n",
      "classifying political_ideology_US: 100%|██████████| 3132/3132 [08:27<00:00,  6.18it/s]\n",
      "classifying political_post: 100%|██████████| 3132/3132 [07:53<00:00,  6.62it/s]\n",
      "classifying rationality_simple2: 100%|██████████| 3132/3132 [07:46<00:00,  6.71it/s]\n"
     ]
    }
   ],
   "source": [
    "#compare to annotations of Llama3.1:8b with zero temperature and different seed:\n",
    "pubspherepromptsrun_Q1 = ['rationality_simple2', 'incivility_simple2', 'interactivity_acknowledgement_simple', 'political_post', 'political_ideology_US']\n",
    "\n",
    "predictions_boukes_L31_8b_zero_seed2: typing.Dict[str, np.ndarray] = {\n",
    "    label: (\n",
    "        src.PromptClassify\n",
    "        .from_json(path)\n",
    "        (boukesTYT[\"commentText\"], model=MODELsmall, options=options_zero_seed2)\n",
    "    )\n",
    "    for label, path in CFG.prompt_classify_files.items() if label in pubspherepromptsrun_Q1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "0bbd7775",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save output:\n",
    "pd.DataFrame(predictions_boukes_L31_8b_zero_seed2).to_parquet('data/publicsphere/predictions_boukes_L31_8b_zero_seed2.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "9394210c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "classifying incivility_simple2: 100%|██████████| 3132/3132 [17:03<00:00,  3.06it/s]\n",
      "classifying interactivity_acknowledgement_simple: 100%|██████████| 3132/3132 [16:56<00:00,  3.08it/s]\n",
      "classifying political_ideology_US: 100%|██████████| 3132/3132 [16:54<00:00,  3.09it/s]\n",
      "classifying political_post: 100%|██████████| 3132/3132 [16:58<00:00,  3.08it/s]\n",
      "classifying rationality_simple2: 100%|██████████| 3132/3132 [16:56<00:00,  3.08it/s]\n"
     ]
    }
   ],
   "source": [
    "#compare to annotations of Qwen2.5:72b with low temperature:\n",
    "pubspherepromptsrun_Q1 = ['rationality_simple2', 'incivility_simple2', 'interactivity_acknowledgement_simple', 'political_post', 'political_ideology_US']\n",
    "\n",
    "predictions_Q72b_boukes_low: typing.Dict[str, np.ndarray] = {\n",
    "    label: (\n",
    "        src.PromptClassify\n",
    "        .from_json(path)\n",
    "        (boukesTYT[\"commentText\"], model=MODELQ25_72b, options=options_low)\n",
    "    )\n",
    "    for label, path in CFG.prompt_classify_files.items() if label in pubspherepromptsrun_Q1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "19557b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save output:\n",
    "pd.DataFrame(predictions_Q72b_boukes_low).to_parquet('data/publicsphere/predictions_Q72b_boukes_low.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "6144d73b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "classifying incivility_simple2: 100%|██████████| 3132/3132 [16:55<00:00,  3.08it/s]\n",
      "classifying interactivity_acknowledgement_simple: 100%|██████████| 3132/3132 [16:57<00:00,  3.08it/s]\n",
      "classifying political_ideology_US: 100%|██████████| 3132/3132 [16:55<00:00,  3.09it/s]\n",
      "classifying political_post: 100%|██████████| 3132/3132 [17:03<00:00,  3.06it/s]\n",
      "classifying rationality_simple2: 100%|██████████| 3132/3132 [16:59<00:00,  3.07it/s]\n"
     ]
    }
   ],
   "source": [
    "#compare to annotations of Qwen2.5:72b with low temperature and different seed:\n",
    "pubspherepromptsrun_Q1 = ['rationality_simple2', 'incivility_simple2', 'interactivity_acknowledgement_simple', 'political_post', 'political_ideology_US']\n",
    "\n",
    "predictions_Q72b_boukes_low_seed2: typing.Dict[str, np.ndarray] = {\n",
    "    label: (\n",
    "        src.PromptClassify\n",
    "        .from_json(path)\n",
    "        (boukesTYT[\"commentText\"], model=MODELQ25_72b, options=options_low_seed2)\n",
    "    )\n",
    "    for label, path in CFG.prompt_classify_files.items() if label in pubspherepromptsrun_Q1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "3e20258a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save output:\n",
    "pd.DataFrame(predictions_Q72b_boukes_low_seed2).to_parquet('data/publicsphere/predictions_Q72b_boukes_low_seed2.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "acf67ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save output:\n",
    "pd.DataFrame(predictions_Q72b_boukes2).to_parquet('data/publicsphere/predictions_Q72b_boukes2.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "315c1312",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'ff32c61e-49b7-11f0-ac8e-7bac359924a9',\n",
       " 'timestamp': '2025-06-15T07:11:45.784760',\n",
       " 'model': 'deepseek-r1:7b',\n",
       " 'prompt': [{'role': 'system',\n",
       "   'content': 'Does this comment provide rational analysis?\\nInstructions: Code Yes (1) if the comment includes:\\nContext or background,\\nEvidence (facts, sources, authorities),\\nReasoning or structured argument.\\nCode No (0) if these are absent\\n\\\\n\\\\nRespond with only the predicted class (0 or 1) of the request.\\\\n\\\\nText: {text}\\\\nClass:'},\n",
       "  {'role': 'user',\n",
       "   'content': 'Does this comment provide rational analysis?\\nInstructions: Code Yes (1) if the comment includes:\\nContext or background,\\nEvidence (facts, sources, authorities),\\nReasoning or structured argument.\\nCode No (0) if these are absent\\n\\\\n\\\\nRespond with only the predicted class (0 or 1) of the request.\\\\n\\\\nText: sad\\\\nClass:'}],\n",
       " 'response': '<think>\\nOkay, so I need to figure out whether the comment \"sad\" provides a rational analysis based on the given instructions. Let me break this down step by step.\\n\\nFirst, looking at the instructions, they ask to code Yes (1) if the comment includes context or background, evidence (facts, sources, authorities), or reasoning (structured argument). If any of these are present, it\\'s a 1; otherwise, it\\'s a 0.\\n\\nThe text given is just \"sad.\" Now, I need to assess what this means. The word \"sad\" is an emotion descriptor but doesn\\'t provide any specific content about context, evidence, or reasoning. It could be someone expressing sadness, but without more information, there\\'s no way to determine if it\\'s based on facts, sources, or a logical argument.\\n\\nSo, since the comment only conveys an emotion without any supporting details, it doesn\\'t meet any of the criteria for providing rational analysis. Therefore, I should code this as No (0).\\n</think>\\n\\nThe comment \"sad\" lacks context, evidence, or reasoning, so it doesn\\'t provide rational analysis.\\n\\n0'}"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "requests.post(\n",
    "                            'https://inf.cl.uni-trier.de/',\n",
    "                            json={\n",
    "                                'model': MODELDSR1_7b,\n",
    "                                'system': \"Does this comment provide rational analysis?\\nInstructions: Code Yes (1) if the comment includes:\\nContext or background,\\nEvidence (facts, sources, authorities),\\nReasoning or structured argument.\\nCode No (0) if these are absent\\n\\\\n\\\\nRespond with only the predicted class (0 or 1) of the request.\\\\n\\\\nText: {text}\\\\nClass:\",\n",
    "                                'prompt': f\"Does this comment provide rational analysis?\\nInstructions: Code Yes (1) if the comment includes:\\nContext or background,\\nEvidence (facts, sources, authorities),\\nReasoning or structured argument.\\nCode No (0) if these are absent\\n\\\\n\\\\nRespond with only the predicted class (0 or 1) of the request.\\\\n\\\\nText: {boukesTYT[\"commentText\"][0]}\\\\nClass:\",\n",
    "                                'options': options_DS_zero\n",
    "                                }).json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8caae31",
   "metadata": {},
   "outputs": [],
   "source": [
    "#deepseek-r1:7b only returns missing values, since it insists on yielding long explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "1edad5d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:08,  4.43s/it]\n",
      "2it [00:06,  3.35s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>label</th>\n",
       "      <th>Mark_ID</th>\n",
       "      <th>incivility_simple2</th>\n",
       "      <th>rationality_simple2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>119</td>\n",
       "      <td>Class: 0</td>\n",
       "      <td>Class: 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>282</td>\n",
       "      <td>Class: 1</td>\n",
       "      <td>Class: 0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "label  Mark_ID incivility_simple2 rationality_simple2\n",
       "0          119           Class: 0            Class: 0\n",
       "1          282           Class: 1            Class: 0"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pubspherepromptsrun_DStest = ['rationality_simple2', 'incivility_simple2']\n",
    "\n",
    "def load_json(path: str):\n",
    "    with open(path, encoding='utf-8') as fp:\n",
    "        return json.load(fp)\n",
    "\n",
    "results = []\n",
    "\n",
    "for label, path in CFG.prompt_classify_files.items():\n",
    "    if label in pubspherepromptsrun_DStest: \n",
    "        template = load_json(path).get('template')\n",
    "        for index, row in tqdm.tqdm(boukesTYT[:2].iterrows()):\n",
    "            try:\n",
    "                response = requests.post(\n",
    "                    'https://inf.cl.uni-trier.de/',\n",
    "                    json={\n",
    "                        'model': MODELDSR1_7b,\n",
    "                        'system': template,\n",
    "                        'prompt': 'do not print thinking steps, only class labels'+template.format(text=row['commentText']),\n",
    "                        'options': options_DS_zero\n",
    "                    }\n",
    "                ).json()\n",
    "                # Extract the annotation\n",
    "                annotation = response['response'].split('<think>')[-1].split('</think>')[-1].strip().split('\\n')[-1].strip()\n",
    "                results.append({\n",
    "                    'Mark_ID': row['Mark_ID'],\n",
    "                    'label': label,\n",
    "                    'annotation': annotation\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(f\"Error for Mark_ID {row['Mark_ID']} and label {label}: {e}\")\n",
    "\n",
    "# Convert to DataFrame and pivot\n",
    "df_results = pd.DataFrame(results)\n",
    "test = df_results.pivot(index='Mark_ID', columns='label', values='annotation').reset_index()\n",
    "\n",
    "# Now df_pivot has one row per Mark_ID and one column per label\n",
    "test.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460d590f",
   "metadata": {},
   "source": [
    "This is not workable, since results still contain non-parseable output tokens, default back to qwen2.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "0373d2cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "classifying rationality_simple2: 100%|██████████| 5585/5585 [29:04<00:00,  3.20it/s]\n"
     ]
    }
   ],
   "source": [
    "#compare to annotations of Qwen2.5:72b for Jaidka2024 data:\n",
    "jaidkapromptsrun_Q72b2 = ['rationality_simple2']\n",
    "#, 'rationality_jaidka', 'incivility_simple2', 'incivility_jaidka',  'civility_jaidka',\n",
    "# 'interactivity_acknowledgement_simple', 'interactivity_acknowledgement_jaidka',  'political_post', 'political_post_jaidka']\n",
    "\n",
    "predictions_Q72b_jaidka2: typing.Dict[str, np.ndarray] = {\n",
    "    label: (\n",
    "        src.PromptClassify\n",
    "        .from_json(path)\n",
    "        (jaidka[\"message\"], model=MODELQ25_72b, options=options_zero)\n",
    "    )\n",
    "    for label, path in CFG.prompt_classify_files.items() if label in jaidkapromptsrun_Q72b2\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "d52a4817",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save output:\n",
    "pd.DataFrame(predictions_Q72b_jaidka2).to_parquet('data/publicsphere/predictions_Q72b_jaidka2.parquet', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "8ac0e397",
   "metadata": {},
   "outputs": [],
   "source": [
    "#open data:\n",
    "Q72b_jaidka = pd.read_parquet('data/publicsphere/predictions_Q72b_jaidka.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "1df6fdf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>civility_jaidka</th>\n",
       "      <th>incivility_jaidka</th>\n",
       "      <th>incivility_simple2</th>\n",
       "      <th>interactivity_acknowledgement_jaidka</th>\n",
       "      <th>interactivity_acknowledgement_simple</th>\n",
       "      <th>political_post</th>\n",
       "      <th>political_post_jaidka</th>\n",
       "      <th>rationality_jaidka</th>\n",
       "      <th>rationality_simple2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5580</th>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>political</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5581</th>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>political</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5582</th>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>political</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5583</th>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>political</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5584</th>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>political</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     civility_jaidka incivility_jaidka incivility_simple2  \\\n",
       "5580             Yes                No                 No   \n",
       "5581              No                No                 No   \n",
       "5582              No               Yes                Yes   \n",
       "5583              No                No                 No   \n",
       "5584              No                No                 No   \n",
       "\n",
       "     interactivity_acknowledgement_jaidka  \\\n",
       "5580                                   No   \n",
       "5581                                   No   \n",
       "5582                                   No   \n",
       "5583                                   No   \n",
       "5584                                   No   \n",
       "\n",
       "     interactivity_acknowledgement_simple political_post  \\\n",
       "5580                                  Yes      political   \n",
       "5581                                  Yes      political   \n",
       "5582                                   No      political   \n",
       "5583                                  Yes      political   \n",
       "5584                                  Yes      political   \n",
       "\n",
       "     political_post_jaidka rationality_jaidka rationality_simple2  \n",
       "5580                   Yes                 No                None  \n",
       "5581                   Yes                Yes                None  \n",
       "5582                   Yes                 No                None  \n",
       "5583                   Yes                 No                None  \n",
       "5584                   Yes                Yes                None  "
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q72b_jaidka.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "40e3ff1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "classifying political_ideology_US: 100%|██████████| 635/635 [03:04<00:00,  3.44it/s]\n",
      "classifying political_ideology_US_para1: 100%|██████████| 635/635 [03:04<00:00,  3.43it/s]\n"
     ]
    }
   ],
   "source": [
    "pubspherepromptsrun_MH = ['political_ideology_US', 'political_ideology_US_para1']\n",
    "\n",
    "predictions_MH_L33_70b: typing.Dict[str, np.ndarray] = {\n",
    "    label: (\n",
    "        src.PromptClassify\n",
    "        .from_json(path)\n",
    "        (MHclemm['text'], model=MODEL33large, options=options_zero)\n",
    "    )\n",
    "    for label, path in CFG.prompt_classify_files.items() if label in pubspherepromptsrun_MH\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cee138f",
   "metadata": {},
   "source": [
    "we now have annotations for boukes, para1, para2, simpa1 and Jaidka prompts for L33_70b (low), L31_8b (low), Q25_72b (zero)\n",
    "and for boukes only L33_70b_seed2, L33_70b_seed2_run2, L31_8b_seed2, DS7b (all None), Q25_72b_seed2 (zero), L33_70b_zero, L33_70b_zero_seed2, L31_8b_zero, L31_8b_zero_seed2, Q25_72b_low, Q25_72b_low_seed2, L31_70b_low, gpt4o_zero, gpt4Turbo_zero\n",
    "\n",
    "for Jaidka we have annotations for boukes and Jaidka prompts for L31_8b_zero, L33_70b_zero, Q25_72b_zero, gpt4o_zero\n",
    "\n",
    "For MHClemm we have ideology and ideology para1 for L31_8b_zero, L33_70b_zero, Q25_72b_zero\n",
    "\n",
    "#TODO: \n",
    "-gpt4o for  para1, para2, simpa1 and Jaidka prompts on boukes\n",
    "-gpt4o seed2 for boukes\n",
    "-gpt4o MHClemm ideology and ideology para1\n",
    "\n",
    "-change variable label to include temperature low/zero\n",
    "-make dummy variables for all annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "e3e859d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rationality_simple2\n",
      "No     5461\n",
      "Yes     124\n",
      "Name: count, dtype: int64\n",
      "------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#join to the dataset:   \n",
    "for _, preds in predictions_Q72b_jaidka2.items():\n",
    "    print(preds.value_counts())\n",
    "    print(\"-\" * 42)\n",
    "    jaidka = jaidka.join(preds, rsuffix='_Q25_72b_zero')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "14582af1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "political_ideology_US\n",
      "neutral         308\n",
      "liberal         199\n",
      "conservative    128\n",
      "Name: count, dtype: int64\n",
      "------------------------------------------\n",
      "political_ideology_US_para1\n",
      "neutral         333\n",
      "liberal         184\n",
      "conservative    117\n",
      "Name: count, dtype: int64\n",
      "------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#join to the dataset:   \n",
    "for _, preds in predictions_MH_Q72b.items():\n",
    "    print(preds.value_counts())\n",
    "    print(\"-\" * 42)\n",
    "    MHclemm = MHclemm.join(preds, rsuffix='_Q25_72b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "145b055c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#need to make sure all new columns have the proper suffix:\n",
    "MHclemm = MHclemm.rename(columns={\n",
    "  'political_ideology_US': 'political_ideology_US_L31_8b',\n",
    " 'political_ideology_US_para1': 'political_ideology_US_para1_L31_8b',\n",
    " })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "e4fad918",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save new annotations:\n",
    "MHclemm.to_parquet('data/MH_BClemm_data/Ideo_Val_GPT_USA_L33_70b.parquet', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "id": "92f915ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "incivility_simple2\n",
      "Yes    2034\n",
      "No     1098\n",
      "Name: count, dtype: int64\n",
      "------------------------------------------\n",
      "interactivity_acknowledgement_simple\n",
      "No     2506\n",
      "Yes     626\n",
      "Name: count, dtype: int64\n",
      "------------------------------------------\n",
      "political_ideology_US\n",
      "neutral         1545\n",
      "conservative     920\n",
      "liberal          667\n",
      "Name: count, dtype: int64\n",
      "------------------------------------------\n",
      "political_post\n",
      "political        1933\n",
      "non-political    1199\n",
      "Name: count, dtype: int64\n",
      "------------------------------------------\n",
      "rationality_simple2\n",
      "No     2924\n",
      "Yes     208\n",
      "Name: count, dtype: int64\n",
      "------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#join to the dataset:   \n",
    "for _, preds in predictions_Q72b_boukes_seed2.items():\n",
    "    print(preds.value_counts())\n",
    "    print(\"-\" * 42)\n",
    "    dataset_w_pred_2 = dataset_w_pred_2.join(preds, rsuffix='_Q72b_seed2')\n",
    "    dataset_w_pred_anon = dataset_w_pred_anon.join(preds, rsuffix='_Q72b_seed2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "29b531cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#need to make sure all new columns have the proper suffix:\n",
    "jaidka = jaidka.rename(columns={\n",
    " 'rationality_simple2':'rationality_simple2_Q25_72b_zero'\n",
    " })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "ecc2a8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the results:\n",
    "jaidka.to_parquet('data/jaidka2022/TwitterDeliberativePolitics2.parquet', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "26a975ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#need to make sure all new columns have the proper suffix:\n",
    "#rename the columns ['civility_jaidka', 'incivility_jaidka', 'rationality_jaidka']:\n",
    "dataset_w_pred_2 = dataset_w_pred_2.rename(columns={\n",
    "  'incivility_para2': 'incivility_para2_Q25_72b',\n",
    " 'incivility_simpa1':   'incivility_simpa1_Q25_72b',\n",
    " 'interactivity_acknowledgement_simple_para2': 'interactivity_acknowledgement_simple_para2_Q25_72b',\n",
    " 'interactivity_acknowledgement_simple_simpa1': 'interactivity_acknowledgement_simple_simpa1_Q25_72b',\n",
    " 'political_ideology_US_para2': 'political_ideology_US_para2_Q25_72b',\n",
    " 'political_ideology_US_simpa1': 'political_ideology_US_simpa1_Q25_72b',\n",
    " 'political_post_para2': 'political_post_para2_Q25_72b',\n",
    " 'political_post_simpa1': 'political_post_simpa1_Q25_72b',\n",
    " 'rationality_simple2_para2': 'rationality_simple2_para2_Q25_72b',\n",
    " 'rationality_simple2_simpa1': 'rationality_simple2_simpa1_Q25_72b'\n",
    "})\n",
    "dataset_w_pred_anon = dataset_w_pred_anon.rename(columns={\n",
    "  'incivility_para2': 'incivility_para2_Q25_72b',\n",
    " 'incivility_simpa1':   'incivility_simpa1_Q25_72b',\n",
    " 'interactivity_acknowledgement_simple_para2': 'interactivity_acknowledgement_simple_para2_Q25_72b',\n",
    " 'interactivity_acknowledgement_simple_simpa1': 'interactivity_acknowledgement_simple_simpa1_Q25_72b',\n",
    " 'political_ideology_US_para2': 'political_ideology_US_para2_Q25_72b',\n",
    " 'political_ideology_US_simpa1': 'political_ideology_US_simpa1_Q25_72b',\n",
    " 'political_post_para2': 'political_post_para2_Q25_72b',\n",
    " 'political_post_simpa1': 'political_post_simpa1_Q25_72b',\n",
    " 'rationality_simple2_para2': 'rationality_simple2_para2_Q25_72b',\n",
    " 'rationality_simple2_simpa1': 'rationality_simple2_simpa1_Q25_72b'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "id": "35b59abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the results:\n",
    "dataset_w_pred_2.to_json(f'{CFG.report_dir}/publicsphere.cardiff_prompt_classify_s.json', orient='records', force_ascii=False, indent=4)\n",
    "dataset_w_pred_anon.to_parquet('data/publicsphere/publicsphere.cardiff_prompt_classify_anon.parquet', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "bf20d998",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_w_pred_anon.loc[:, 'rationality_simple2_L33_70b_dum'] = dataset_w_pred_anon.loc[:, 'rationality_simple2_L33_70b'].map({\"Yes\": 1, \"No\":0}).fillna(0).astype(int)\n",
    "dataset_w_pred_anon.loc[:, 'rationality_jaidka_L33_70b_dum'] = dataset_w_pred_anon.loc[:, 'rationality_jaidka_L33_70b'].map({\"Yes\": 1, \"No\":0}).fillna(0).astype(int)\n",
    "dataset_w_pred_anon.loc[:, 'incivility_simple2_L33_70b_dum'] = dataset_w_pred_anon.loc[:, 'incivility_simple2_L33_70b'].map({\"Yes\": 1, \"No\":0}).fillna(0).astype(int)\n",
    "dataset_w_pred_anon.loc[:, 'incivility_jaidka_L33_70b_dum'] = dataset_w_pred_anon.loc[:, 'incivility_jaidka_L33_70b'].map({\"Yes\": 1, \"No\":0}).fillna(0).astype(int)\n",
    "dataset_w_pred_anon.loc[:, 'political_post_jaidka_L33_70b_dum'] = dataset_w_pred_anon.loc[:, 'political_post_jaidka_L33_70b'].map({\"Yes\": 1, \"No\":0}).fillna(0).astype(int)\n",
    "dataset_w_pred_anon.loc[:, 'political_post_L33_70b_dum'] = dataset_w_pred_anon.loc[:, 'political_post_L33_70b'].map({\"political\": 1, \"non-political\":0}).fillna(0).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "69bbd86e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rationality_simple2_L33_70b_dum</th>\n",
       "      <th>rationality_jaidka_L33_70b_dum</th>\n",
       "      <th>rationality_simple2_small_dum</th>\n",
       "      <th>rationality_simple2_gpt4o_dum</th>\n",
       "      <th>rationality_simple_dum</th>\n",
       "      <th>RATIONALITY_DUMMY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>rationality_simple2_L33_70b_dum</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rationality_jaidka_L33_70b_dum</th>\n",
       "      <td>0.29</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rationality_simple2_small_dum</th>\n",
       "      <td>0.46</td>\n",
       "      <td>0.12</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rationality_simple2_gpt4o_dum</th>\n",
       "      <td>0.39</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.54</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rationality_simple_dum</th>\n",
       "      <td>0.71</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.32</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RATIONALITY_DUMMY</th>\n",
       "      <td>0.41</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.33</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 rationality_simple2_L33_70b_dum  \\\n",
       "rationality_simple2_L33_70b_dum                             1.00   \n",
       "rationality_jaidka_L33_70b_dum                              0.29   \n",
       "rationality_simple2_small_dum                               0.46   \n",
       "rationality_simple2_gpt4o_dum                               0.39   \n",
       "rationality_simple_dum                                      0.71   \n",
       "RATIONALITY_DUMMY                                           0.41   \n",
       "\n",
       "                                 rationality_jaidka_L33_70b_dum  \\\n",
       "rationality_simple2_L33_70b_dum                            0.29   \n",
       "rationality_jaidka_L33_70b_dum                             1.00   \n",
       "rationality_simple2_small_dum                              0.12   \n",
       "rationality_simple2_gpt4o_dum                              0.11   \n",
       "rationality_simple_dum                                     0.31   \n",
       "RATIONALITY_DUMMY                                          0.30   \n",
       "\n",
       "                                 rationality_simple2_small_dum  \\\n",
       "rationality_simple2_L33_70b_dum                           0.46   \n",
       "rationality_jaidka_L33_70b_dum                            0.12   \n",
       "rationality_simple2_small_dum                             1.00   \n",
       "rationality_simple2_gpt4o_dum                             0.54   \n",
       "rationality_simple_dum                                    0.42   \n",
       "RATIONALITY_DUMMY                                         0.25   \n",
       "\n",
       "                                 rationality_simple2_gpt4o_dum  \\\n",
       "rationality_simple2_L33_70b_dum                           0.39   \n",
       "rationality_jaidka_L33_70b_dum                            0.11   \n",
       "rationality_simple2_small_dum                             0.54   \n",
       "rationality_simple2_gpt4o_dum                             1.00   \n",
       "rationality_simple_dum                                    0.32   \n",
       "RATIONALITY_DUMMY                                         0.25   \n",
       "\n",
       "                                 rationality_simple_dum  RATIONALITY_DUMMY  \n",
       "rationality_simple2_L33_70b_dum                    0.71               0.41  \n",
       "rationality_jaidka_L33_70b_dum                     0.31               0.30  \n",
       "rationality_simple2_small_dum                      0.42               0.25  \n",
       "rationality_simple2_gpt4o_dum                      0.32               0.25  \n",
       "rationality_simple_dum                             1.00               0.33  \n",
       "RATIONALITY_DUMMY                                  0.33               1.00  "
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#feasability check:\n",
    "#do annotations of Llama3.3:70b correlate with gpt4o?\n",
    "dataset_w_pred_anon.loc[:, ['rationality_simple2_L33_70b_dum', 'rationality_jaidka_L33_70b_dum', 'rationality_simple2_small_dum', 'rationality_simple2_gpt4o_dum', 'rationality_simple_dum', 'RATIONALITY_DUMMY']] \\\n",
    "    .corr(method='pearson').round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e3b5e4",
   "metadata": {},
   "source": [
    "improved performance of L33_70b compared to L31_70b (default), and the two models also show the highest overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "3dfa9a1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th>rationality_simple2_L33_70b_dum</th>\n",
       "      <th colspan=\"2\" halign=\"left\">0</th>\n",
       "      <th colspan=\"2\" halign=\"left\">1</th>\n",
       "      <th>Total</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rationality_simple2_small_dum</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RATIONALITY_DUMMY</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2930</td>\n",
       "      <td>14</td>\n",
       "      <td>181</td>\n",
       "      <td>40</td>\n",
       "      <td>3165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>394</td>\n",
       "      <td>1</td>\n",
       "      <td>202</td>\n",
       "      <td>100</td>\n",
       "      <td>697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Total</th>\n",
       "      <td>3324</td>\n",
       "      <td>15</td>\n",
       "      <td>383</td>\n",
       "      <td>140</td>\n",
       "      <td>3862</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "rationality_simple2_L33_70b_dum     0        1      Total\n",
       "rationality_simple2_small_dum       0   1    0    1      \n",
       "RATIONALITY_DUMMY                                        \n",
       "0                                2930  14  181   40  3165\n",
       "1                                 394   1  202  100   697\n",
       "Total                            3324  15  383  140  3862"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#and in crosstabulations:\n",
    "pd.crosstab(dataset_w_pred_anon['RATIONALITY_DUMMY'], [dataset_w_pred_anon['rationality_simple2_L33_70b_dum'], dataset_w_pred_anon['rationality_simple2_small_dum']], margins=True, margins_name='Total')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495c50d5",
   "metadata": {},
   "source": [
    "#L33_70b and small share 100+2930=3030 correct classifications (78%) and share 40+394=434 errors (11%) and differ on 14+1+181+202=398 errors (10%)-> they differ on 48% of errors\n",
    "#L33_70b makes 2930+14+202+100=3246 correct classifications\n",
    "and 394+1+181+40=616 errors = 16%\n",
    "#small makes 2930+181+1+100=3212 correct classifications\n",
    "and 394+14+202+40=650 errors = 17%\n",
    "#we would thus expect 0.17*0.16 = only 3% overlap between errors if the models were random -> they thus do a lot better than that\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "28e300df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th>rationality_simple2_L33_70b_dum</th>\n",
       "      <th>0</th>\n",
       "      <th colspan=\"2\" halign=\"left\">1</th>\n",
       "      <th>Total</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rationality_simple2_gpt4o_dum</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RATIONALITY_DUMMY</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2944</td>\n",
       "      <td>205</td>\n",
       "      <td>16</td>\n",
       "      <td>3165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>395</td>\n",
       "      <td>230</td>\n",
       "      <td>72</td>\n",
       "      <td>697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Total</th>\n",
       "      <td>3339</td>\n",
       "      <td>435</td>\n",
       "      <td>88</td>\n",
       "      <td>3862</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "rationality_simple2_L33_70b_dum     0    1     Total\n",
       "rationality_simple2_gpt4o_dum       0    0   1      \n",
       "RATIONALITY_DUMMY                                   \n",
       "0                                2944  205  16  3165\n",
       "1                                 395  230  72   697\n",
       "Total                            3339  435  88  3862"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#and in crosstabulations:\n",
    "pd.crosstab(dataset_w_pred_anon['RATIONALITY_DUMMY'], [dataset_w_pred_anon['rationality_simple2_L33_70b_dum'], dataset_w_pred_anon['rationality_simple2_gpt4o_dum']], margins=True, margins_name='Total')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e6d1fc",
   "metadata": {},
   "source": [
    "#L33_70b and gpt4o share only 16 errors (0%) and differ on 205+230=435 errors (11%) -> they differ on 96% of errors\n",
    "#L33_70b and gpt4o correctly classify 2944+72=3016 (78%)\n",
    "#so L33_70b shares the same share of correct classifications in combination with small and gpt4o, but errors overlap much more with small than with gpt4o, which makes sense, overlap between errors of L33_70b and gpt4o is equal to chance.\n",
    "#this indicates that these two models don't agree on which manual coding are actually coding errors -> together they only mark 16 comments as potentially wrong coded even though they haven't seen our annotations in their training data, so should be independently judging the rationality of the comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "0ed644e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>incivility_simple2_L33_70b_dum</th>\n",
       "      <th>incivility_jaidka_L33_70b_dum</th>\n",
       "      <th>incivility_simple2_small_dum</th>\n",
       "      <th>incivility_simple2_gpt4o_dum</th>\n",
       "      <th>INCIVILITY_DUMMY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>incivility_simple2_L33_70b_dum</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>incivility_jaidka_L33_70b_dum</th>\n",
       "      <td>0.76</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>incivility_simple2_small_dum</th>\n",
       "      <td>0.49</td>\n",
       "      <td>0.53</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>incivility_simple2_gpt4o_dum</th>\n",
       "      <td>0.58</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.68</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>INCIVILITY_DUMMY</th>\n",
       "      <td>0.54</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.55</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                incivility_simple2_L33_70b_dum  \\\n",
       "incivility_simple2_L33_70b_dum                            1.00   \n",
       "incivility_jaidka_L33_70b_dum                             0.76   \n",
       "incivility_simple2_small_dum                              0.49   \n",
       "incivility_simple2_gpt4o_dum                              0.58   \n",
       "INCIVILITY_DUMMY                                          0.54   \n",
       "\n",
       "                                incivility_jaidka_L33_70b_dum  \\\n",
       "incivility_simple2_L33_70b_dum                           0.76   \n",
       "incivility_jaidka_L33_70b_dum                            1.00   \n",
       "incivility_simple2_small_dum                             0.53   \n",
       "incivility_simple2_gpt4o_dum                             0.63   \n",
       "INCIVILITY_DUMMY                                         0.51   \n",
       "\n",
       "                                incivility_simple2_small_dum  \\\n",
       "incivility_simple2_L33_70b_dum                          0.49   \n",
       "incivility_jaidka_L33_70b_dum                           0.53   \n",
       "incivility_simple2_small_dum                            1.00   \n",
       "incivility_simple2_gpt4o_dum                            0.68   \n",
       "INCIVILITY_DUMMY                                        0.48   \n",
       "\n",
       "                                incivility_simple2_gpt4o_dum  INCIVILITY_DUMMY  \n",
       "incivility_simple2_L33_70b_dum                          0.58              0.54  \n",
       "incivility_jaidka_L33_70b_dum                           0.63              0.51  \n",
       "incivility_simple2_small_dum                            0.68              0.48  \n",
       "incivility_simple2_gpt4o_dum                            1.00              0.55  \n",
       "INCIVILITY_DUMMY                                        0.55              1.00  "
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#do annotations of Llama3.3:70b correlate with gpt4o?\n",
    "dataset_w_pred_anon.loc[:, ['incivility_simple2_L33_70b_dum', 'incivility_jaidka_L33_70b_dum', 'incivility_simple2_small_dum', 'incivility_simple2_gpt4o_dum', 'INCIVILITY_DUMMY']] \\\n",
    "    .corr(method='pearson').round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "48145025",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>political_post_L33_70b_dum</th>\n",
       "      <th>political_post_jaidka_L33_70b_dum</th>\n",
       "      <th>TopicRelevance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>political_post_L33_70b_dum</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>political_post_jaidka_L33_70b_dum</th>\n",
       "      <td>0.49</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TopicRelevance</th>\n",
       "      <td>0.58</td>\n",
       "      <td>0.38</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   political_post_L33_70b_dum  \\\n",
       "political_post_L33_70b_dum                               1.00   \n",
       "political_post_jaidka_L33_70b_dum                        0.49   \n",
       "TopicRelevance                                           0.58   \n",
       "\n",
       "                                   political_post_jaidka_L33_70b_dum  \\\n",
       "political_post_L33_70b_dum                                      0.49   \n",
       "political_post_jaidka_L33_70b_dum                               1.00   \n",
       "TopicRelevance                                                  0.38   \n",
       "\n",
       "                                   TopicRelevance  \n",
       "political_post_L33_70b_dum                   0.58  \n",
       "political_post_jaidka_L33_70b_dum            0.38  \n",
       "TopicRelevance                               1.00  "
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#do annotations of Llama3.3:70b correlate with gpt4o?\n",
    "dataset_w_pred_anon.loc[:, ['political_post_L33_70b_dum', 'political_post_jaidka_L33_70b_dum', 'TopicRelevance']] \\\n",
    "    .corr(method='pearson').round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23db806",
   "metadata": {},
   "outputs": [],
   "source": [
    "#what is the infuence of temperature on the results of intraprompt annotation reliability?\n",
    "\n",
    "#Note the logic of our comparisons:\n",
    "#we compare the results of the same prompt with different models, and different seeds, to see if the model and/or seed influences the results.\n",
    "#we use similar options per model, but the options are not the same for all models, they differ in temperature and seed (since the same seed might mean something different for different models).\n",
    "#but since we compare intraprompt annotation reliability for the same prompt with different seeds, the difference in temperature is not a problem, the annotation might differ per output of the model for that seed/temperature, but the difference with another seed should be minimal.\n",
    "#anyway we can test the origins of intraprompt reliability by comparing the result of the same prompt with the same seed and low temperature, different seed and low temperature and same seed and zero temperature\n",
    "#if it turns out that temperature does have a larger influence, but still the influence of different models or prompts is larger, we can still conclude that the model and prompt are more important than the temperature.\n",
    "#stronger still a higher temperature intraprompt benchmark is harder to beat especially for the simpa-prompts\n",
    "#for the between model comparisons temperature per model should not be a problem, since it will only vary the result of the model, not the comparison between models, only downside of low temperature is potential slightly lower reproducibility of the exact results and potetially slightly lower performance due to less creativity, but Barry ea 2025 does not seem to suggest this is the case for such low temperatures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2cc6be25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           prompt_label  seed  \\\n",
      "0  interactivity_acknowledgement_simple    43   \n",
      "\n",
      "                                                                                    paraphrased_prompt  \n",
      "0  Determine if this remark recognizes or replies to a different user's remark.\\nGuidelines: Mark Y...  \n"
     ]
    }
   ],
   "source": [
    "#use GPT4o to compile paraphrased prompts::\n",
    "def load_json(path: str):\n",
    "    with open(path, encoding='utf-8') as fp:\n",
    "        return json.load(fp)\n",
    "    \n",
    "\n",
    "#pubspherepromptsrunall = ['rationality_simple2', 'rationality_jaidka', 'incivility_simple2', 'incivility_jaidka',  'civility_jaidka', 'interactivity_simple2', 'interactivity_acknowledgement_jaidka', 'political_ideology_US', 'political_post', 'political_post_jaidka' ]  \n",
    "\n",
    "pubsphereparaphraserun1 = ['rationality_simple2', 'incivility_simple2', 'interactivity_acknowledgement_simple2', 'political_ideology_US', 'political_post'] \n",
    "pubsphereparaphraserun2 = ['interactivity_acknowledgement_simple']\n",
    "\n",
    "\n",
    "chunked_result: typing.List[pd.DataFrame] = []\n",
    "\n",
    "for label, path in CFG.prompt_classify_files.items():\n",
    "    if label in pubsphereparaphraserun2: \n",
    "        template = load_json(path).get('template')\n",
    "        classes = load_json(path).get('classes')\n",
    "        retry_count = 0\n",
    "        max_retries = 5\n",
    "        while retry_count < max_retries:\n",
    "            try: \n",
    "                response = requests.post(\n",
    "                        url=api_endpoint,\n",
    "                        headers=headers,\n",
    "                        json={\n",
    "                            'model': MODELgpt4o,\n",
    "                            'messages': [\n",
    "                                {\n",
    "                                    \"role\": \"system\",\n",
    "                                    \"content\": 'You restate a prompt in different words while preserving its meaning and formatting. The prompt is: \"{}\"'\n",
    "                                },\n",
    "                                {\n",
    "                                    \"role\": \"user\",\n",
    "                                    \"content\": 'You restate a prompt in different words while preserving its meaning and formatting. The prompt is: \"{}\"'.format(template)\n",
    "                                }\n",
    "                            ],\n",
    "                            'temperature': 0.7,\n",
    "                            'top_p': 0.8,  \n",
    "                            'seed': SEED2\n",
    "                        }\n",
    "                    )  \n",
    "                if response.status_code == 200:\n",
    "                    data_response = response.json()\n",
    "                    chunked_result.append(\n",
    "                        pd.DataFrame(\n",
    "                            data=[[label, SEED2, data_response[\"choices\"][0][\"message\"][\"content\"]]],                                \n",
    "                            columns=['prompt_label', 'seed', 'paraphrased_prompt']\n",
    "                        )\n",
    "                    )\n",
    "                    break  # Exit retry loop for this label\n",
    "                elif response.status_code == 429:\n",
    "                    retry_count += 1\n",
    "                    wait_time = 20\n",
    "                    print(f\"Rate limit exceeded. Retrying in {wait_time} seconds...\")\n",
    "                    time.sleep(wait_time)\n",
    "                elif response.status_code == 500:\n",
    "                    retry_count += 1\n",
    "                    wait_time = 20\n",
    "                    print(f\"Failed to connect to API. Status code: {response.status_code}. Retrying in {wait_time} seconds...\")\n",
    "                    time.sleep(wait_time)\n",
    "                else:\n",
    "                    print(f\"Failed to connect to API. Status code: {response.status_code}\")\n",
    "                    print(response.text)\n",
    "                    # Optionally append a row with error info\n",
    "                    chunked_result.append(\n",
    "                        pd.DataFrame(\n",
    "                            data=[[label, f\"ERROR: {response.status_code}\"]],\n",
    "                            columns=['prompt_label', 'paraphrased_prompt']\n",
    "                        )\n",
    "                    )\n",
    "                    break\n",
    "            except requests.exceptions.RequestException as e:   \n",
    "                print(f\"Failed to connect to API: {e}\")\n",
    "                retry_count += 1\n",
    "                wait_time = 60\n",
    "                print(f\"Retrying in {wait_time} seconds...\")\n",
    "                time.sleep(wait_time)\n",
    "        else:\n",
    "            # If all retries failed, append a row with error info\n",
    "            chunked_result.append(\n",
    "                pd.DataFrame(\n",
    "                    data=[[label, \"ERROR: Max retries exceeded\"]],\n",
    "                    columns=['prompt_label', 'paraphrased_prompt']\n",
    "                )\n",
    "            )\n",
    "\n",
    "para1 = pd.concat(chunked_result, ignore_index=True)\n",
    "print(para1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b410b186",
   "metadata": {},
   "source": [
    "This rewording appears as dissimilar as the Jaidka:\n",
    "-Boukes: \"Does this comment provide rational analysis?\\nInstructions: Code Yes (1) if the comment includes:\\nContext or background,\\nEvidence (facts, sources, authorities),\\nReasoning or structured argument.\\nCode No (0) if these are absent\\n\\\\n\\\\nRespond with only the predicted class (0 or 1) of the request.\\\\n\\\\nText: {text}\\\\nClass:\",\n",
    "   \n",
    "-Jaidka: \"This tweet is a reply on Twitter (i.e., a Tweet) to a United States member of the Congress. Please classify this tweet according to whether it has a justification. Read the tweet. Determine which category best describes the tweet. Code YES (1): If this tweet contains personal feelings or experiences. Also code YES (1) If this tweet contains facts, links or evidence from other sources. Code NO (0) If this tweet does not offer a justification. \\n\\\\n\\\\nRespond with only the predicted class (0 or 1) of the request.\\\\n\\\\nText: {text}\\\\nClass:\",\n",
    "\n",
    "-GPT4o_temp0.7_topp0.8_seed1: 'Evaluate whether the comment offers logical examination. \\n\\nGuidelines: Assign Yes (1) if the comment contains:\\n- Context or background information,\\n- Supporting evidence (facts, references, experts),\\n- Logical reasoning or organized argumentation.\\n\\nAssign No (0) if these elements are missing.\\n\\nReply with just the anticipated category (0 or 1) for the request.\\n\\nText: {text}\\nClass:',\n",
    "\n",
    "-GPT4o_temp0.7_topp0.8_seed2:  'Assess whether the comment offers logical evaluation.\\nGuidelines: Assign Yes (1) if the comment contains:\\nContext or background information,\\nSupporting evidence (facts, references, expert opinions),\\nLogical reasoning or organized argumentation.\\nAssign No (0) if these elements are missing.\\n\\\\n\\\\nProvide solely the anticipated classification (0 or 1) of the inquiry.\\\\n\\\\nText: {text}\\\\nClass:',\n",
    "   \n",
    "\n",
    "-GPT4o_temp0.5_topp0.5: 'Evaluate whether the comment offers logical examination.\\nInstructions: Assign Yes (1) if the comment contains:\\nContext or background information,\\nEvidence (facts, references, experts),\\nLogical reasoning or organized argument.\\nAssign No (0) if these elements are missing.\\n\\\\n\\\\nReply with just the anticipated category (0 or 1) of the inquiry.\\\\n\\\\nText: {text}\\\\nClass:',\n",
    "    \n",
    "\n",
    "-> maybe we can generate two paraphrases per boukes prompt with different seeds, which look like they would be rather similar to each other to get a grasp of very small prompt differences, compared to different codebooks or paraphrases of a prompt.\n",
    "also helps to address the potential critizism that our prompts favor Llama, since most prompt engineering was done on lama (but prompt wording of both simple and paraphrased prompts are based on GPT4o)\n",
    "\n",
    "    \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5de645c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the existing paraphrased prompts:\n",
    "existing_paraphrases = pd.read_parquet(f'{CFG.report_dir}/Boukes_paraphrased_prompts.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9f189b5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt_label</th>\n",
       "      <th>seed</th>\n",
       "      <th>paraphrased_prompt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>incivility_simple2</td>\n",
       "      <td>43</td>\n",
       "      <td>Is this comment uncivil?\\nGuidelines: Mark Yes (1) if the comment contains name-calling, insults...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>interactivity_acknowledgement_simple2</td>\n",
       "      <td>43</td>\n",
       "      <td>Is the comment a reference to, an acknowledgment of, or a reply to another user's remark?\\nInstr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>political_ideology_US</td>\n",
       "      <td>43</td>\n",
       "      <td>Categorize the given message according to its ideological stance as liberal (0), neutral (1), or...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>political_post</td>\n",
       "      <td>43</td>\n",
       "      <td>Determine whether the given message should be categorized as political (1) or non-political (0)....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>rationality_simple2</td>\n",
       "      <td>43</td>\n",
       "      <td>Assess whether the comment offers logical evaluation.\\nGuidelines: Assign Yes (1) if the comment...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>incivility_simple2</td>\n",
       "      <td>42</td>\n",
       "      <td>Determine if the comment shows incivility.\\nGuidelines: Assign Yes (1) if the comment contains n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>interactivity_acknowledgement_simple2</td>\n",
       "      <td>42</td>\n",
       "      <td>Determine if this comment is referencing, recognizing, or replying to another user's comment.\\nG...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>political_ideology_US</td>\n",
       "      <td>42</td>\n",
       "      <td>Determine whether the given text aligns with liberal views (0), is ideologically neutral (1), or...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>political_post</td>\n",
       "      <td>42</td>\n",
       "      <td>Determine whether the given message is political (1) or non-political (0). A message is consider...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>rationality_simple2</td>\n",
       "      <td>42</td>\n",
       "      <td>Evaluate whether the comment offers logical examination.\\nInstructions: Assign Yes (1) if the co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>interactivity_acknowledgement_simple</td>\n",
       "      <td>42</td>\n",
       "      <td>Is this comment an acknowledgment or reply to another user's remark?\\nInstructions: Assign Yes (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>interactivity_acknowledgement_simple</td>\n",
       "      <td>43</td>\n",
       "      <td>Determine if this remark recognizes or replies to a different user's remark.\\nGuidelines: Mark Y...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             prompt_label  seed  \\\n",
       "0                      incivility_simple2    43   \n",
       "1   interactivity_acknowledgement_simple2    43   \n",
       "2                   political_ideology_US    43   \n",
       "3                          political_post    43   \n",
       "4                     rationality_simple2    43   \n",
       "5                      incivility_simple2    42   \n",
       "6   interactivity_acknowledgement_simple2    42   \n",
       "7                   political_ideology_US    42   \n",
       "8                          political_post    42   \n",
       "9                     rationality_simple2    42   \n",
       "10   interactivity_acknowledgement_simple    42   \n",
       "11   interactivity_acknowledgement_simple    43   \n",
       "\n",
       "                                                                                     paraphrased_prompt  \n",
       "0   Is this comment uncivil?\\nGuidelines: Mark Yes (1) if the comment contains name-calling, insults...  \n",
       "1   Is the comment a reference to, an acknowledgment of, or a reply to another user's remark?\\nInstr...  \n",
       "2   Categorize the given message according to its ideological stance as liberal (0), neutral (1), or...  \n",
       "3   Determine whether the given message should be categorized as political (1) or non-political (0)....  \n",
       "4   Assess whether the comment offers logical evaluation.\\nGuidelines: Assign Yes (1) if the comment...  \n",
       "5   Determine if the comment shows incivility.\\nGuidelines: Assign Yes (1) if the comment contains n...  \n",
       "6   Determine if this comment is referencing, recognizing, or replying to another user's comment.\\nG...  \n",
       "7   Determine whether the given text aligns with liberal views (0), is ideologically neutral (1), or...  \n",
       "8   Determine whether the given message is political (1) or non-political (0). A message is consider...  \n",
       "9   Evaluate whether the comment offers logical examination.\\nInstructions: Assign Yes (1) if the co...  \n",
       "10  Is this comment an acknowledgment or reply to another user's remark?\\nInstructions: Assign Yes (...  \n",
       "11  Determine if this remark recognizes or replies to a different user's remark.\\nGuidelines: Mark Y...  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test join:\n",
    "pd.concat([existing_paraphrases, para1], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "20098bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#add the new paraphrases with the existing ones:\n",
    "para1 = pd.concat([existing_paraphrases, para1], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "22d7f449",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             prompt_label  seed  \\\n",
      "0                      incivility_simple2    43   \n",
      "1   interactivity_acknowledgement_simple2    43   \n",
      "2                   political_ideology_US    43   \n",
      "3                          political_post    43   \n",
      "4                     rationality_simple2    43   \n",
      "5                      incivility_simple2    42   \n",
      "6   interactivity_acknowledgement_simple2    42   \n",
      "7                   political_ideology_US    42   \n",
      "8                          political_post    42   \n",
      "9                     rationality_simple2    42   \n",
      "10   interactivity_acknowledgement_simple    42   \n",
      "11   interactivity_acknowledgement_simple    43   \n",
      "\n",
      "                                                                                     paraphrased_prompt  \n",
      "0   Is this comment uncivil?\\nGuidelines: Mark Yes (1) if the comment contains name-calling, insults...  \n",
      "1   Is the comment a reference to, an acknowledgment of, or a reply to another user's remark?\\nInstr...  \n",
      "2   Categorize the given message according to its ideological stance as liberal (0), neutral (1), or...  \n",
      "3   Determine whether the given message should be categorized as political (1) or non-political (0)....  \n",
      "4   Assess whether the comment offers logical evaluation.\\nGuidelines: Assign Yes (1) if the comment...  \n",
      "5   Determine if the comment shows incivility.\\nGuidelines: Assign Yes (1) if the comment contains n...  \n",
      "6   Determine if this comment is referencing, recognizing, or replying to another user's comment.\\nG...  \n",
      "7   Determine whether the given text aligns with liberal views (0), is ideologically neutral (1), or...  \n",
      "8   Determine whether the given message is political (1) or non-political (0). A message is consider...  \n",
      "9   Evaluate whether the comment offers logical examination.\\nInstructions: Assign Yes (1) if the co...  \n",
      "10  Is this comment an acknowledgment or reply to another user's remark?\\nInstructions: Assign Yes (...  \n",
      "11  Determine if this remark recognizes or replies to a different user's remark.\\nGuidelines: Mark Y...  \n"
     ]
    }
   ],
   "source": [
    "#inspect the results:\n",
    "print(para1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "54fd4577",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the paraphrased prompts to parquet file:\n",
    "para1.to_parquet(f'{CFG.report_dir}/Boukes_paraphrased_prompts.parquet', index=False)\n",
    "#save to json file:\n",
    "para1.to_json(f'{CFG.report_dir}/Boukes_paraphrased_prompts.json', orient='records', force_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6078db85",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prompts appear still quite different between seeds, so maybe add some irrelevant difference manually, maybe in formatting or so?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d9adcb2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           prompt_label  seed  \\\n",
      "0                    incivility_simple2    42   \n",
      "1  interactivity_acknowledgement_simple    42   \n",
      "2                 political_ideology_US    42   \n",
      "3                        political_post    42   \n",
      "4                   rationality_simple2    42   \n",
      "\n",
      "                                                                                    paraphrased_prompt  \n",
      "0  Does this comment display incivility?  \\nInstructions: Code Yes (1) if the comment includes name...  \n",
      "1  Does this comment acknowledge or respond to another user's comment?  \\nInstructions:  \\nCode Yes...  \n",
      "2  Classify the following message as ideologically liberal (0), ideologically neutral (1), or ideol...  \n",
      "3  Classify the following message as following messages as political (1) or non-political (0). Poli...  \n",
      "4  Does this comment provide rational analysis?  \\nInstructions: Code Yes (1) if the comment includ...  \n"
     ]
    }
   ],
   "source": [
    "#use GPT4o to compile slightly altered prompts:\n",
    "def load_json(path: str):\n",
    "    with open(path, encoding='utf-8') as fp:\n",
    "        return json.load(fp)\n",
    "    \n",
    "\n",
    "#pubspherepromptsrunall = ['rationality_simple2', 'rationality_jaidka', 'incivility_simple2', 'incivility_jaidka',  'civility_jaidka', 'interactivity_simple2', 'interactivity_acknowledgement_jaidka', 'political_ideology_US', 'political_post', 'political_post_jaidka' ]  \n",
    "\n",
    "pubsphereparaphraserun1 = ['rationality_simple2', 'incivility_simple2', 'interactivity_acknowledgement_simple', 'political_ideology_US', 'political_post'] \n",
    "\n",
    "\n",
    "\n",
    "chunked_result: typing.List[pd.DataFrame] = []\n",
    "\n",
    "for label, path in CFG.prompt_classify_files.items():\n",
    "    if label in pubsphereparaphraserun1: \n",
    "        template = load_json(path).get('template')\n",
    "        classes = load_json(path).get('classes')\n",
    "        retry_count = 0\n",
    "        max_retries = 5\n",
    "        while retry_count < max_retries:\n",
    "            try: \n",
    "                response = requests.post(\n",
    "                        url=api_endpoint,\n",
    "                        headers=headers,\n",
    "                        json={\n",
    "                            'model': MODELgpt4o,\n",
    "                            'messages': [\n",
    "                                {\n",
    "                                    \"role\": \"system\",\n",
    "                                    \"content\": 'You restate a prompt in the same words only alter formatting. The prompt is: \"{}\"'\n",
    "                                },\n",
    "                                {\n",
    "                                    \"role\": \"user\",\n",
    "                                    \"content\": 'You restate a prompt in the same words only alter formatting. The prompt is: \"{}\"'.format(template)\n",
    "                                }\n",
    "                            ],\n",
    "                            'temperature': 0.1,\n",
    "                            'top_p': 0.8,  \n",
    "                            'seed': SEED\n",
    "                        }\n",
    "                    )  \n",
    "                if response.status_code == 200:\n",
    "                    data_response = response.json()\n",
    "                    chunked_result.append(\n",
    "                        pd.DataFrame(\n",
    "                            data=[[label, SEED, data_response[\"choices\"][0][\"message\"][\"content\"]]],                                \n",
    "                            columns=['prompt_label', 'seed', 'paraphrased_prompt']\n",
    "                        )\n",
    "                    )\n",
    "                    break  # Exit retry loop for this label\n",
    "                elif response.status_code == 429:\n",
    "                    retry_count += 1\n",
    "                    wait_time = 20\n",
    "                    print(f\"Rate limit exceeded. Retrying in {wait_time} seconds...\")\n",
    "                    time.sleep(wait_time)\n",
    "                elif response.status_code == 500:\n",
    "                    retry_count += 1\n",
    "                    wait_time = 20\n",
    "                    print(f\"Failed to connect to API. Status code: {response.status_code}. Retrying in {wait_time} seconds...\")\n",
    "                    time.sleep(wait_time)\n",
    "                else:\n",
    "                    print(f\"Failed to connect to API. Status code: {response.status_code}\")\n",
    "                    print(response.text)\n",
    "                    # Optionally append a row with error info\n",
    "                    chunked_result.append(\n",
    "                        pd.DataFrame(\n",
    "                            data=[[label, f\"ERROR: {response.status_code}\"]],\n",
    "                            columns=['prompt_label', 'paraphrased_prompt']\n",
    "                        )\n",
    "                    )\n",
    "                    break\n",
    "            except requests.exceptions.RequestException as e:   \n",
    "                print(f\"Failed to connect to API: {e}\")\n",
    "                retry_count += 1\n",
    "                wait_time = 60\n",
    "                print(f\"Retrying in {wait_time} seconds...\")\n",
    "                time.sleep(wait_time)\n",
    "        else:\n",
    "            # If all retries failed, append a row with error info\n",
    "            chunked_result.append(\n",
    "                pd.DataFrame(\n",
    "                    data=[[label, \"ERROR: Max retries exceeded\"]],\n",
    "                    columns=['prompt_label', 'similarized_prompt', 'seed']\n",
    "                )\n",
    "            )\n",
    "\n",
    "simpa1 = pd.concat(chunked_result, ignore_index=True)\n",
    "print(simpa1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2161fcf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Does this comment display incivility?  \\nInstructions: Code Yes (1) if the comment includes name-calling, insults, inflammatory language, sarcasm, shouting (ALL CAPS), vulgarity, discrimination, threats, or restrictions on rights. Code No (0) if none of these are present.  \\n\\nRespond with only the predicted class (0 or 1) of the request.  \\n\\nText: {text}  \\nClass:'\n",
      " \"Does this comment acknowledge or respond to another user's comment?  \\nInstructions:  \\nCode Yes (1) if the comment shows agreement or disagreement with a specific user's statement, often signaled by a username or phrases like ‘Yes,’ ‘No,’ or ‘I agree.’  \\nCode No (0) if it lacks a clear acknowledgment or is only an insult.  \\n  \\nRespond with only the predicted class (0 or 1) of the request.  \\n  \\nText: {text}  \\nClass:\"\n",
      " 'Classify the following message as ideologically liberal (0), ideologically neutral (1), or ideologically conservative (2). Ideology here is defined in the context of the US political system. Messages with no ideological content are classified as neutral.  \\n\\nRespond with only the predicted class (0 or 1 or 2) of the request.  \\n\\nText: {text}  \\nClass:'\n",
      " 'Classify the following message as following messages as political (1) or non-political (0). Political is defined as any message which is directly about a political topic, references political developments, or makes reference to a political figure, group, or agency. References to federal organisations are political, as are references to branches of government. Broad mentions of national economic developments are political, but discussions of individual stock prices are not.\\n\\nRespond with only the predicted class (0 or 1) of the request.\\n\\nText: {text}  \\nClass:'\n",
      " 'Does this comment provide rational analysis?  \\nInstructions: Code Yes (1) if the comment includes:  \\n- Context or background,  \\n- Evidence (facts, sources, authorities),  \\n- Reasoning or structured argument.  \\nCode No (0) if these are absent  \\n\\nRespond with only the predicted class (0 or 1) of the request.  \\n\\nText: {text}  \\nClass:']\n"
     ]
    }
   ],
   "source": [
    "print(simpa1.paraphrased_prompt.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc2b906",
   "metadata": {},
   "source": [
    "new:      'Does this comment provide rational analysis?  \\nInstructions: Code Yes (1) if the comment includes:  \\n- Context or background,  \\n- Evidence (facts, sources, authorities),  \\n- Reasoning or structured argument.  \\nCode No (0) if these are absent  \\n\\nRespond with only the predicted class (0 or 1) of the request.  \\n\\nText: {text}  \\nClass:'\n",
    "\n",
    "original: \"Does this comment provide rational analysis?\\nInstructions: Code Yes (1) if the comment includes:\\nContext or background,\\nEvidence (facts, sources, authorities),\\nReasoning or structured argument.\\nCode No (0) if these are absent\\n\\\\n\\\\nRespond with only the predicted class (0 or 1) of the request.\\\\n\\\\nText: {text}\\\\nClass:\","
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "22193a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the paraphrased prompts to parquet file:\n",
    "simpa1.to_parquet(f'{CFG.report_dir}/Boukes_similarized_prompts.parquet', index=False)\n",
    "#save to json file:\n",
    "simpa1.to_json(f'{CFG.report_dir}/Boukes_similarized_prompts.json', orient='records', force_ascii=False, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmdiv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
