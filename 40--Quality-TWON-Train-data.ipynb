{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(\"sjoerdAzure.env\")  # Load environment variables from .env file\n",
    "import time\n",
    "\n",
    "import typing\n",
    "\n",
    "from sklearn.metrics import cohen_kappa_score, classification_report\n",
    "import krippendorff\n",
    "import yaml\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import config\n",
    "import src\n",
    "import tqdm\n",
    "import json\n",
    "import numpy as np\n",
    "import logging\n",
    "\n",
    "#import cltrier_lib as lib\n",
    "import pyreadr\n",
    "import yaml\n",
    "pd.set_option('display.max_colwidth', 100) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up helper variables and functions:\n",
    "CFG = config.Config()\n",
    "\n",
    "def load_json(path: str):\n",
    "    with open(path, encoding='utf-8') as fp:\n",
    "        return json.load(fp)\n",
    "    \n",
    "#set option variables:\n",
    "\n",
    "#set options to low temperature (0,1):\n",
    "options_low_str = \"\"\"\n",
    "seed: 42\n",
    "temperature: 0.1\n",
    "\"\"\"\n",
    "\n",
    "options_low = yaml.safe_load(options_low_str)\n",
    "\n",
    "MODELsmall: str = 'llama3.1:8b-instruct-q6_K' # options: 'gemma:7b-instruct-q6_K', 'gemma2:27b-instruct-q6_K', 'llama3.1:8b-instruct-q6_K', 'llama3.1:70b-instruct-q6_K', 'mistral:7b-instruct-v0.3-q6_K', 'mistral-large:123b-instruct-2407-q6_K', 'mixtral:8x7b-instruct-v0.1-q6_K', 'mixtral:8x22b-instruct-v0.1-q6_K', 'phi3:14b-medium-128k-instruct-q6_K' or 'qwen2:72b-instruct-q6_K'\n",
    "MODELlarge: str = 'llama3.1:70b-instruct-q6_K' # options: 'gemma:7b-instruct-q6_K', 'gemma2:27b-instruct-q6_K', 'llama3.1:8b-instruct-q6_K', 'llama3.1:70b-instruct-q6_K', 'mistral:7b-instruct-v0.3-q6_K', 'mistral-large:123b-instruct-2407-q6_K', 'mixtral:8x7b-instruct-v0.1-q6_K', 'mixtral:8x22b-instruct-v0.1-q6_K', 'phi3:14b-medium-128k-instruct-q6_K' or 'qwen2:72b-instruct-q6_K'\n",
    "MODELgpt4o = \"nf-gpt-4o-2024-08-06\" # in principe is er nu van elk model een nf (no filter) en een normale versie beschikbaar, de no filter versies zijn alleen voor onderzoekers beschikbaar voor analyze van content die niet door de filter heen zou komen.\n",
    "MODELgpt4T = \"nf-gpt-4-turbo\" # Can be gpt-35-turbo, gpt-4-turbo, gpt-4 or Meta-Llama-3-8B-Instruct.\n",
    "MODELgpt4 = \"nf-gpt-4\" # Can be gpt-35-turbo, gpt-4-turbo, gpt-4 or Meta-Llama-3-8B-Instruct.\n",
    "\n",
    "options_zero_str = \"\"\"\n",
    "seed: 42\n",
    "temperature: 0\n",
    "\"\"\"\n",
    "options_zero = yaml.safe_load(options_zero_str)\n",
    "\n",
    "temperature_0 : int = 0\n",
    "SEED: int = 42\n",
    "MAX10: int = 10\n",
    "TOPP1: int = 1\n",
    "\n",
    "\n",
    "options_large_str = \"\"\"\n",
    "seed: 42\n",
    "temperature: 0\n",
    "num_predict: 2000\n",
    "\"\"\"\n",
    "options_large = yaml.safe_load(options_large_str)\n",
    "\n",
    "#load environment variables:\n",
    "api_key = os.environ.get('sjoerd_key')\n",
    "\n",
    "#setttings:\n",
    "api_endpoint = \"https://ai-research-proxy.azurewebsites.net/chat/completions\"\n",
    "api_endpoint_embed = \"https://ai-research-proxy.azurewebsites.net/embeddings\"\n",
    "####### API REQUEST FORMATTING ######\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"Authorization\": \"Bearer \" + api_key\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "LibrdataError",
     "evalue": "Invalid file, or file has unsupported features",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLibrdataError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Read data from .rds file\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m tweets \u001b[38;5;241m=\u001b[39m \u001b[43mpyreadr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_r\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata/TWON_German_data/GermanyMdBTweets_2023.rds\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m replies \u001b[38;5;241m=\u001b[39m pyreadr\u001b[38;5;241m.\u001b[39mread_r(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/TWON_German_data/GermanyReplies2023.rds\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m users \u001b[38;5;241m=\u001b[39m pyreadr\u001b[38;5;241m.\u001b[39mread_r(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/TWON_German_data/Germany2023Users.rds\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\sstolwi\\Github\\llmdiv\\Lib\\site-packages\\pyreadr\\pyreadr.py:66\u001b[0m, in \u001b[0;36mread_r\u001b[1;34m(path, use_objects, timezone)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(filename_bytes):\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PyreadrError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m does not exist!\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(filename_bytes))\n\u001b[1;32m---> 66\u001b[0m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename_bytes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     68\u001b[0m result \u001b[38;5;241m=\u001b[39m OrderedDict()\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m table_index, table \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(parser\u001b[38;5;241m.\u001b[39mtable_data):\n",
      "File \u001b[1;32mc:\\Users\\sstolwi\\Github\\llmdiv\\Lib\\site-packages\\pyreadr\\librdata.pyx:149\u001b[0m, in \u001b[0;36mpyreadr.librdata.Parser.parse\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\sstolwi\\Github\\llmdiv\\Lib\\site-packages\\pyreadr\\librdata.pyx:178\u001b[0m, in \u001b[0;36mpyreadr.librdata.Parser.parse\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mLibrdataError\u001b[0m: Invalid file, or file has unsupported features"
     ]
    }
   ],
   "source": [
    "# Read data from .rds file\n",
    "tweets = pyreadr.read_r('data/TWON_German_data/GermanyMdBTweets_2023.rds')\n",
    "replies = pyreadr.read_r('data/TWON_German_data/GermanyReplies2023.rds')\n",
    "users = pyreadr.read_r('data/TWON_German_data/Germany2023Users.rds')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is highly nested data..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmdiv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
