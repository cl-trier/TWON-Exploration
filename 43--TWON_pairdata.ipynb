{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "311bb8df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.0.2 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"c:\\Users\\sstolwi\\Github\\llmdiv\\Lib\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"c:\\Users\\sstolwi\\Github\\llmdiv\\Lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"c:\\Users\\sstolwi\\Github\\llmdiv\\Lib\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"c:\\Users\\sstolwi\\Github\\llmdiv\\Lib\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\\Lib\\asyncio\\base_events.py\", line 645, in run_forever\n",
      "    self._run_once()\n",
      "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\\Lib\\asyncio\\base_events.py\", line 1999, in _run_once\n",
      "    handle._run()\n",
      "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\\Lib\\asyncio\\events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"c:\\Users\\sstolwi\\Github\\llmdiv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"c:\\Users\\sstolwi\\Github\\llmdiv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"c:\\Users\\sstolwi\\Github\\llmdiv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"c:\\Users\\sstolwi\\Github\\llmdiv\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"c:\\Users\\sstolwi\\Github\\llmdiv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"c:\\Users\\sstolwi\\Github\\llmdiv\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"c:\\Users\\sstolwi\\Github\\llmdiv\\Lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"c:\\Users\\sstolwi\\Github\\llmdiv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"c:\\Users\\sstolwi\\Github\\llmdiv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"c:\\Users\\sstolwi\\Github\\llmdiv\\Lib\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"c:\\Users\\sstolwi\\Github\\llmdiv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"c:\\Users\\sstolwi\\Github\\llmdiv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"c:\\Users\\sstolwi\\Github\\llmdiv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\sstolwi\\AppData\\Local\\Temp\\ipykernel_8988\\3709438642.py\", line 21, in <module>\n",
      "    import src\n",
      "  File \"c:\\Users\\sstolwi\\Github\\TWON-Metrics\\src\\__init__.py\", line 1, in <module>\n",
      "    from .hf_classify import HFClassify\n",
      "  File \"c:\\Users\\sstolwi\\Github\\TWON-Metrics\\src\\hf_classify.py\", line 5, in <module>\n",
      "    import torch\n",
      "  File \"c:\\Users\\sstolwi\\Github\\llmdiv\\Lib\\site-packages\\torch\\__init__.py\", line 1477, in <module>\n",
      "    from .functional import *  # noqa: F403\n",
      "  File \"c:\\Users\\sstolwi\\Github\\llmdiv\\Lib\\site-packages\\torch\\functional.py\", line 9, in <module>\n",
      "    import torch.nn.functional as F\n",
      "  File \"c:\\Users\\sstolwi\\Github\\llmdiv\\Lib\\site-packages\\torch\\nn\\__init__.py\", line 1, in <module>\n",
      "    from .modules import *  # noqa: F403\n",
      "  File \"c:\\Users\\sstolwi\\Github\\llmdiv\\Lib\\site-packages\\torch\\nn\\modules\\__init__.py\", line 35, in <module>\n",
      "    from .transformer import TransformerEncoder, TransformerDecoder, \\\n",
      "  File \"c:\\Users\\sstolwi\\Github\\llmdiv\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 20, in <module>\n",
      "    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
      "c:\\Users\\sstolwi\\Github\\llmdiv\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at ..\\torch\\csrc\\utils\\tensor_numpy.cpp:84.)\n",
      "  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
      "c:\\Users\\sstolwi\\Github\\llmdiv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#load packages:\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import config\n",
    "import json\n",
    "import pyreadr\n",
    "import pprint\n",
    "\n",
    "import requests\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(\"sjoerdAzure.env\")  # Load environment variables from .env file\n",
    "import time\n",
    "import re\n",
    "import typing\n",
    "\n",
    "#from sklearn.metrics import cohen_kappa_score, classification_report\n",
    "#import krippendorff\n",
    "import yaml\n",
    "\n",
    "import src\n",
    "import tqdm\n",
    "import logging\n",
    "\n",
    "#import cltrier_lib as lib\n",
    "#import pyreadstat\n",
    "import yaml\n",
    "pd.set_option('display.max_colwidth', 100) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af1abd0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up helper variables and functions:\n",
    "CFG = config.Config()\n",
    "\n",
    "def load_json(path: str):\n",
    "    with open(path, encoding='utf-8') as fp:\n",
    "        return json.load(fp)\n",
    "    \n",
    "#set option variables:\n",
    "\n",
    "#set options to low temperature (0,1):\n",
    "options_low_str = \"\"\"\n",
    "seed: 42\n",
    "temperature: 0.1\n",
    "\"\"\"\n",
    "\n",
    "options_low = yaml.safe_load(options_low_str)\n",
    "\n",
    "#apparently the 3.1 70b model is no longer available via Trier...\n",
    "MODELsmall: str = 'llama3.1:8b'\n",
    "MODEL33large: str = 'llama3.3:70b' # options: 'gemma:7b-instruct-q6_K', 'gemma2:27b-instruct-q6_K', 'llama3.1:8b-instruct-q6_K', 'llama3.1:70b-instruct-q6_K', 'mistral:7b-instruct-v0.3-q6_K', 'mistral-large:123b-instruct-2407-q6_K', 'mixtral:8x7b-instruct-v0.1-q6_K', 'mixtral:8x22b-instruct-v0.1-q6_K', 'phi3:14b-medium-128k-instruct-q6_K' or 'qwen2:72b-instruct-q6_K'\n",
    "MODELgpt4o = \"nf-gpt-4o-2024-08-06\" # in principe is er nu van elk model een nf (no filter) en een normale versie beschikbaar, de no filter versies zijn alleen voor onderzoekers beschikbaar voor analyze van content die niet door de filter heen zou komen.\n",
    "MODELgpt4T = \"nf-gpt-4-turbo\" # Can be gpt-35-turbo, gpt-4-turbo, gpt-4 or Meta-Llama-3-8B-Instruct.\n",
    "MODEL33largeAzure = 'Llama-3.3-70B-Instruct' #azureml://registries/azureml-meta/models/Llama-3.3-70B-Instruct/versions/4 / options:  \"data\": [\n",
    "MODEL31largeAzureNF = 'nf-Llama-3.1-70b-instruct'\n",
    "\n",
    "#    {\n",
    "#      \"id\": \"gpt4o\",\n",
    "#      \"object\": \"model\",\n",
    "#      \"created\": 1677610602,\n",
    "#      \"owned_by\": \"openai\"\n",
    "#    },\n",
    "#    {\n",
    "#      \"id\": \"gpt-4\",\n",
    "#      \"object\": \"model\",\n",
    "#      \"created\": 1677610602,\n",
    "#      \"owned_by\": \"openai\"\n",
    "#    },\n",
    "#    {\n",
    "#      \"id\": \"Llama-3.2-90B-Vision-Instruct\",\n",
    "#      \"object\": \"model\",\n",
    "#      \"created\": 1677610602,\n",
    "#      \"owned_by\": \"openai\"\n",
    "#    },\n",
    "#    {\n",
    "#      \"id\": \"dall-e-3\",\n",
    "#      \"object\": \"model\",\n",
    "#      \"created\": 1677610602,\n",
    "#      \"owned_by\": \"openai\"\n",
    "#    },\n",
    "#    {\n",
    "#      \"id\": \"Mistral-Nemo\",\n",
    "#      \"object\": \"model\",\n",
    "#      \"created\": 1677610602,\n",
    "#      \"owned_by\": \"openai\"\n",
    "#    },\n",
    "#    {\n",
    "#      \"id\": \"Mistral-small\",\n",
    "#      \"object\": \"model\",\n",
    "#      \"created\": 1677610602,\n",
    "#      \"owned_by\": \"openai\"\n",
    "#    },\n",
    "#    {\n",
    "#      \"id\": \"Codestral-2501\",\n",
    "#      \"object\": \"model\",\n",
    "#      \"created\": 1677610602,\n",
    "#      \"owned_by\": \"openai\"\n",
    "#    },\n",
    "#    {\n",
    "#      \"id\": \"gpt-35-turbo\",\n",
    "#      \"object\": \"model\",\n",
    "#      \"created\": 1677610602,\n",
    "#      \"owned_by\": \"openai\"\n",
    "#    },\n",
    "#    {\n",
    "#      \"id\": \"Ministral-3B\",\n",
    "#      \"object\": \"model\",\n",
    "#      \"created\": 1677610602,\n",
    "#      \"owned_by\": \"openai\"\n",
    "#    },\n",
    "#    {\n",
    "#      \"id\": \"text-embedding-ada-002\",\n",
    "#      \"object\": \"model\",\n",
    "#      \"created\": 1677610602,\n",
    "#      \"owned_by\": \"openai\"\n",
    "#    },\n",
    "#    {\n",
    "#      \"id\": \"nf-gpt-4\",\n",
    "#      \"object\": \"model\",\n",
    "#      \"created\": 1677610602,\n",
    "#      \"owned_by\": \"openai\"\n",
    "#    },\n",
    "#    {\n",
    "#      \"id\": \"nf-gpt-4o-mini\",\n",
    "#      \"object\": \"model\",\n",
    "#      \"created\": 1677610602,\n",
    "#      \"owned_by\": \"openai\"\n",
    "#    },\n",
    "#    {\n",
    "#      \"id\": \"Mistral-Large-2411\",\n",
    "#      \"object\": \"model\",\n",
    "#      \"created\": 1677610602,\n",
    "#      \"owned_by\": \"openai\"\n",
    "#    },\n",
    "#    {\n",
    "#      \"id\": \"nf-text-embedding-ada-002\",\n",
    "#      \"object\": \"model\",\n",
    "#      \"created\": 1677610602,\n",
    "#      \"owned_by\": \"openai\"\n",
    "#    },\n",
    "#    {\n",
    "#      \"id\": \"nf-Llama-3.1-70b-instruct\",\n",
    "#      \"object\": \"model\",\n",
    "#      \"created\": 1677610602,\n",
    "#      \"owned_by\": \"openai\"\n",
    "#    },\n",
    "#    {\n",
    "#      \"id\": \"nf-gpt-4-vision\",\n",
    "#      \"object\": \"model\",\n",
    "#      \"created\": 1677610602,\n",
    "#      \"owned_by\": \"openai\"\n",
    "#    },\n",
    "#    {\n",
    "#      \"id\": \"gpt-4-turbo\",\n",
    "#      \"object\": \"model\",\n",
    "#      \"created\": 1677610602,\n",
    "#      \"owned_by\": \"openai\"\n",
    "#    },\n",
    "#    {\n",
    "#      \"id\": \"nf-gpt-35-turbo\",\n",
    "#      \"object\": \"model\",\n",
    "#      \"created\": 1677610602,\n",
    "#      \"owned_by\": \"openai\"\n",
    "#    },\n",
    "#    {\n",
    "#      \"id\": \"Llama-3.3-70B-Instruct\",\n",
    "#      \"object\": \"model\",\n",
    "#      \"created\": 1677610602,\n",
    "#      \"owned_by\": \"openai\"\n",
    "#    },\n",
    "#    {\n",
    "#      \"id\": \"nf-dall-e-3\",\n",
    "#      \"object\": \"model\",\n",
    "#      \"created\": 1677610602,\n",
    "#      \"owned_by\": \"openai\"\n",
    "#    },\n",
    "#    {\n",
    "#      \"id\": \"gpt-4-vision\",\n",
    "#      \"object\": \"model\",\n",
    "#      \"created\": 1677610602,\n",
    "#      \"owned_by\": \"openai\"\n",
    "#    },\n",
    "#    {\n",
    "#      \"id\": \"gpt-4o-mini\",\n",
    "#      \"object\": \"model\",\n",
    "#      \"created\": 1677610602,\n",
    "#      \"owned_by\": \"openai\"\n",
    "#    },\n",
    "#    {\n",
    "#      \"id\": \"nf-gpt-4o\",\n",
    "#      \"object\": \"model\",\n",
    "#      \"created\": 1677610602,\n",
    "#      \"owned_by\": \"openai\"\n",
    "#    },\n",
    "#    {\n",
    "#      \"id\": \"nf-gpt-4o-2024-08-06\",\n",
    "#      \"object\": \"model\",\n",
    "#      \"created\": 1677610602,\n",
    "#      \"owned_by\": \"openai\"\n",
    "#    },\n",
    "#    {\n",
    "#      \"id\": \"nf-gpt-4-turbo\",\n",
    "#      \"object\": \"model\",\n",
    "#      \"created\": 1677610602,\n",
    "#      \"owned_by\": \"openai\"\n",
    "#    },\n",
    "#    {\n",
    "#      \"id\": \"nf-Llama-3.1-8b-instruct\",\n",
    "#      \"object\": \"model\",\n",
    "#      \"created\": 1677610602,\n",
    "#      \"owned_by\": \"openai\"\n",
    "#    },\n",
    "#    {\n",
    "#      \"id\": \"Llama-3.2-11B-Vision-Instruct\",\n",
    "#      \"object\": \"model\",\n",
    "#      \"created\": 1677610602,\n",
    "#      \"owned_by\": \"openai\"\n",
    "#    }\n",
    "#  ],\n",
    "#  \"object\": \"list\"\n",
    "#}\n",
    "\n",
    "options_zero_str = \"\"\"\n",
    "seed: 42\n",
    "temperature: 0\n",
    "\"\"\"\n",
    "options_zero = yaml.safe_load(options_zero_str)\n",
    "\n",
    "temperature_0 : int = 0\n",
    "SEED: int = 42\n",
    "MAX10: int = 10\n",
    "TOPP1: int = 1\n",
    "\n",
    "\n",
    "options_large_str = \"\"\"\n",
    "seed: 42\n",
    "temperature: 0\n",
    "num_predict: 2000\n",
    "\"\"\"\n",
    "options_large = yaml.safe_load(options_large_str)\n",
    "\n",
    "#load environment variables:\n",
    "api_key = os.environ.get('sjoerd_key')\n",
    "\n",
    "#setttings:\n",
    "api_endpoint = \"https://ai-research-proxy.azurewebsites.net/chat/completions\"\n",
    "api_endpoint_embed = \"https://ai-research-proxy.azurewebsites.net/embeddings\"\n",
    "####### API REQUEST FORMATTING ######\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"Authorization\": \"Bearer \" + api_key\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2518a1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_claim: str = \\\n",
    "    \"\"\"\n",
    "        Instruction:\n",
    "\n",
    "        You are a text annotation assistant. Analyze a social media comment enclosed in chevrons <..>. Identify and list the claims within this comment. Claims can be related to events, issues, opinions or concerns.\n",
    "        Claims are defined as the main assertion or conclusion of an argument.\n",
    "        You summarize each claim into a short simple sentence.\n",
    "\n",
    "        Response format:\n",
    "\n",
    "        You provide only the list of claims, separated by commas, without any additional text or explanations. If no claims can be identified, return an empty list [].\n",
    "\n",
    "        Response format template:\n",
    "        \n",
    "        [\"claim 1\", \"claim 2\", ... \"claim x\"]\n",
    "\t\"\"\"\n",
    "\n",
    "#note updated on 30-12-2024:\n",
    "SYSTEM_expansion: str = \\\n",
    "    \"\"\"\n",
    "        # Instruction\\n\\nRephrase a social media post to reflect its meaning within the context of a conversation thread:\\n\\n1. You'll receive a thread in chevrons `<...>` and a target post in double chevrons `<<...>>`.\\n2. If the post is clear without context, repeat it as-is.\\n3. If the post depends on context, expand it to include necessary details.\\n4. Respond with only the expanded post. \\n5. If the post does not refer to context provided in the thread, or if in doubt, respond with the exact target post as you received it. If no target post was presented reply with an empty list [] \\n\\n## Example\\n\\n**Input**:\\n- Thread: <'Comment 1', 'Comment 2', 'Comment 3'>\\n- Target reply: `<<This is so relatable!>>`\\n\\n**Output**:\\n- '[context from previous comments] is so relatable!'\\n\\n**Text:\n",
    "    \"\"\"\n",
    "\n",
    "SYSTEM_expansion_refwords: str = \\\n",
    "    \"\"\"\n",
    "        # Instruction\\n\\nRephrase a social media post to reflect its meaning within the context of a conversation thread, pay special attention to reference words like that/they/this/it, or other words that may refer to previous information like 'disagree':\\n\\n1. You'll receive a thread in chevrons `<...>` and a target post in double chevrons `<<...>>`.\\n2. If the post is clear without context, repeat it as-is.\\n3. If the post depends on context, expand it to include necessary details. For example if it includes reference words, replace them with what they refer to. \\n4. Respond with only the expanded post. \\n5. If the post does not refer to context provided in the thread, or if in doubt, respond with the exact target post as you received it. If no target post was presented reply with an empty list [] \\n\\n## Example\\n\\n**Input**:\\n- Thread: <'Comment 1', 'Comment 2', 'Comment 3'>\\n- Target reply: `<<This is so relatable!>>`\\n\\n**Output**:\\n- '[context from previous comments] is so relatable!'\\n\\n**Text:\n",
    "    \"\"\"\n",
    "\n",
    "SYSTEM_context_claim: str = \\\n",
    "    \"\"\"\n",
    "        Instruction:\n",
    "\n",
    "        You are a text annotation assistant. Analyze a target social media reply enclosed in chevrons <..> while taking into account the comment to which it replied (the context, provided in double chevrons <<...>>) to verbalize the claims made. Identify and list the claims within this target reply. Claims can be related to events, issues, opinions or concerns.\n",
    "        Claims are defined as the main assertion or conclusion of an argument.\n",
    "        You summarize each claim into a short simple sentence.\n",
    "\n",
    "        Response format:\n",
    "\n",
    "        You provide only the list of claims, separated by commas, without any additional text or explanations. If no claims can be identified, return an empty list [].\n",
    "\n",
    "        Response format template:\n",
    "        \n",
    "        [\"claim 1\", \"claim 2\", ... \"claim x\"]\n",
    "\t\"\"\"\n",
    "\n",
    "\n",
    "SYSTEM_claim_new: str = \\\n",
    "    \"\"\"\n",
    "        # Instruction\n",
    "\n",
    "        Identify whether a claim adds new information (2), expands on information (1) or does not add new information (0) with respect to preceding claims. New information is present (2) if the claim introduces a new argument or perspective. \n",
    "        A claim expands on an existing argument or perspective (1) if it strongly related to it, or it provides additional context or examples to an argument or perspective in a preceding claim, or if it provides a sub-argument (parent-child relation). \n",
    "        If the claim is very similar to a preceding claim or does not add any perspective or argument, assign a value of 0. Follow these steps:\n",
    "\n",
    "\n",
    "        Follow these steps:\n",
    "\n",
    "        1. You will receive a target claim in double chevrons <<...>> along with the preceding claims in single chevrons <...>.\n",
    "        2. Identify if new information is presented in the target claim with respect to the preceding posts. \n",
    "        4. If in doubt, assign a value of 0.\n",
    "\n",
    "        Respond with only the predicted class (0 or 1 or 2) of the request. Do not include any additional text or explanations.\n",
    "        Class:\n",
    "\t\"\"\"\n",
    "\n",
    "SYSTEM_claim_simscore: str = \\\n",
    "    \"\"\"\n",
    "        # Instruction\n",
    "\n",
    "        Identify degree of similarity of the information presented in a claim with respect to preceding claims in a thread on an integer scale of 1 (completely dissimilar) to 10 (identical). A score of 5 indicates that the claims share context or topic, but otherwise present different information. \n",
    "        Follow these steps:\n",
    "\n",
    "        1. You will receive a target claim in double chevrons <<...>> along with a JSON containing the preceding claims and their claim_index enclosed in single chevrons <...>.\n",
    "        2. Determine the most similar claim to the target claim in the preceding claims in terms of the information they present.. If only one preceding claim is provided pick this claim as the most similar claim.\n",
    "        3. Find the claim_index of this most similar claim. If you can't decide which claim is most similar, pick a preceding comment at random.\n",
    "        4. Identify the degree of similarity of the target claim with respect to that claim on a range of 1-10. \n",
    "        5. If no target claim is porvided or only '[]' return an empty list [] as value for both the most_similar_claim_index and the similarity_score, if you can't decide on the similarity score return an empty list for that value. If no preceding claims are provided or only an empty string '', return an empty list [] as value for both the most_similar_claim_index and the similarity_score.\n",
    "        6. Always and only respond with the claim_index and similarity score.\n",
    "\n",
    "        Response format in JSON:\n",
    "\n",
    "        [\n",
    "            {\n",
    "                \"most_similar_claim_index\": \"1\",\n",
    "                \"similarity_score\": \"1\"\t\n",
    "            }\n",
    "        ]\n",
    "\t\"\"\"\n",
    "\n",
    "SYSTEM_post_simscore: str = \\\n",
    "    \"\"\"\n",
    "        # Instruction \n",
    "        \n",
    "        Identify degree of similarity of the information presented in a social media comment with respect to preceding comments in a thread on an integer scale of 1 (completely dissimilar) to 10 (identical). A score of 5 indicates that the comments share context or topic, but otherwise present different information. \n",
    "        Follow these steps:\n",
    "        \n",
    "        1. You will receive a target comment in double chevrons <<...>> along with a JSON containing the preceding comments and their comment_index enclosed in single chevrons <...>.\n",
    "        2. Determine the most similar comment to the target comment in the preceding comments in terms of the information they present. If only one preceding comment is provided pick this comment as the most similar comment.\n",
    "        3. Find the comment_index of this most similar comment. If you can't decide which comment is most similar, pick a preceding comment at random.\n",
    "        4. Identify the degree of similarity of the target comment with respect to that comment on a range of 1-10. \n",
    "        5. If no target comment is provided or only '[]' return an empty list [] as value for both the most_similar_comment_index and the similarity_score, if you can't decide on the similarity score return an empty list for that value.\n",
    "        6. Always and only respond with the comment_index and similarity score.\n",
    "        \n",
    "        Response format in JSON:\n",
    "        \n",
    "        [\n",
    "            {\n",
    "                \"most_similar_comment_index\": \"1\",\n",
    "                \"similarity_score\": \"1\"\t\n",
    "            }\n",
    "        ]\n",
    "\t\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399f8db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#inspect TWON conversation data nesting structure:\n",
    "pprint.pprint(load_json('data/TWON_export_Nils/conv_group2_august.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5c7ff7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load TWON conversation data:\n",
    "Nilspair = load_json('data/TWON_export_Nils/conv_group2_august.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3ffd896a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a list ID to each item\n",
    "flattened_data = []\n",
    "for thread_id, sublist in enumerate(Nilspair):\n",
    "    for item in sublist:\n",
    "        item['thread_id'] = thread_id  # Add the list ID\n",
    "        flattened_data.append(item)\n",
    "        \n",
    "NilsThread = pd.json_normalize(flattened_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f44f535",
   "metadata": {},
   "outputs": [],
   "source": [
    "NilsThread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "edde0f0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108644\n",
      "2.0858491955377194\n"
     ]
    }
   ],
   "source": [
    "#how many threads are there?\n",
    "print(len(NilsThread['thread_id'].unique()))\n",
    "#how many comments per thread on average:\n",
    "print(NilsThread.groupby('thread_id').size().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dbc7b53a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>full_text</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>screen_name</th>\n",
       "      <th>original_user_id</th>\n",
       "      <th>retweeted_user_ID</th>\n",
       "      <th>collected_at</th>\n",
       "      <th>reply_to_id</th>\n",
       "      <th>reply_to_user</th>\n",
       "      <th>expandedURL</th>\n",
       "      <th>thread_id</th>\n",
       "      <th>parsed_create_time</th>\n",
       "      <th>ascending_comment_index</th>\n",
       "      <th>preceding_comment_index</th>\n",
       "      <th>indexcol</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>James Comer claims House Republicans were just about to crack the Hunter Biden case wide open an...</td>\n",
       "      <td>1.690720e+18</td>\n",
       "      <td>2023-08-13 13:39:27+00:00</td>\n",
       "      <td>MeidasTouch</td>\n",
       "      <td>1.243560e+18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-08-15 17:27:17.883106</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{'url': 'https://t.co/ULhlQ86ycJ', 'expanded_url': 'https://www.meidastouch.com/news/inspector-...</td>\n",
       "      <td>0</td>\n",
       "      <td>2023-08-13 13:39:27+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@MeidasTouch Because of the Unfathomable Stupidity of the Republican Voter, GOP politicians have...</td>\n",
       "      <td>1.691029e+18</td>\n",
       "      <td>2023-08-14 10:09:35+00:00</td>\n",
       "      <td>EdHull8</td>\n",
       "      <td>8.886505e+08</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-08-15 17:47:19.740945</td>\n",
       "      <td>1.690720e+18</td>\n",
       "      <td>1.243560e+18</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>2023-08-14 10:09:35+00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                             full_text  \\\n",
       "0  James Comer claims House Republicans were just about to crack the Hunter Biden case wide open an...   \n",
       "1  @MeidasTouch Because of the Unfathomable Stupidity of the Republican Voter, GOP politicians have...   \n",
       "\n",
       "       tweet_id                 created_at  screen_name  original_user_id  \\\n",
       "0  1.690720e+18  2023-08-13 13:39:27+00:00  MeidasTouch      1.243560e+18   \n",
       "1  1.691029e+18  2023-08-14 10:09:35+00:00      EdHull8      8.886505e+08   \n",
       "\n",
       "   retweeted_user_ID                collected_at   reply_to_id  reply_to_user  \\\n",
       "0                NaN  2023-08-15 17:27:17.883106           NaN            NaN   \n",
       "1                NaN  2023-08-15 17:47:19.740945  1.690720e+18   1.243560e+18   \n",
       "\n",
       "                                                                                           expandedURL  \\\n",
       "0  [{'url': 'https://t.co/ULhlQ86ycJ', 'expanded_url': 'https://www.meidastouch.com/news/inspector-...   \n",
       "1                                                                                                   []   \n",
       "\n",
       "   thread_id        parsed_create_time  ascending_comment_index  \\\n",
       "0          0 2023-08-13 13:39:27+00:00                        0   \n",
       "1          0 2023-08-14 10:09:35+00:00                        1   \n",
       "\n",
       "   preceding_comment_index  indexcol  \n",
       "0                      NaN         0  \n",
       "1                      0.0         1  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#add datetime variable:\n",
    "NilsThread['parsed_create_time'] = pd.to_datetime(NilsThread['created_at'], errors='coerce', utc=True)\n",
    "#add preceding  comment index variable:\n",
    "NilsThread_sort = NilsThread.sort_values(by=['thread_id', 'parsed_create_time'], ascending=True)\n",
    "NilsThread_sort['ascending_comment_index'] = NilsThread_sort.groupby('thread_id').cumcount()\n",
    "NilsThread_sort['preceding_comment_index'] = NilsThread_sort.groupby('thread_id')['ascending_comment_index'].shift(1)\n",
    "NilsThread_sort['indexcol'] = NilsThread_sort.index\n",
    "NilsThread_sort.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8b99089f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#select a sample of 1000 threads:\n",
    "NilsThread_sample_set = NilsThread_sort.thread_id.sample(1000, random_state=42)\n",
    "NilsThread1k = NilsThread_sort[NilsThread_sort['thread_id'].isin(NilsThread_sample_set)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd77952",
   "metadata": {},
   "outputs": [],
   "source": [
    "NilsThread1k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c5706b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Michael Heseltine's TWON german data from JSON:\n",
    "MHpost = load_json('data/TWON_sample_MHeseltine/Germany_posts_sample_Sjoerd.json')\n",
    "MHreply = load_json('data/TWON_sample_MHeseltine/Germany_replies_sample_Sjoerd.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a33c5bb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author_id</th>\n",
       "      <th>conversation_id</th>\n",
       "      <th>referenced_tweets</th>\n",
       "      <th>edit_history_tweet_ids</th>\n",
       "      <th>lang</th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>created_at</th>\n",
       "      <th>possibly_sensitive</th>\n",
       "      <th>in_reply_to_user_id</th>\n",
       "      <th>...</th>\n",
       "      <th>public_metrics.reply_count</th>\n",
       "      <th>public_metrics.like_count</th>\n",
       "      <th>public_metrics.quote_count</th>\n",
       "      <th>public_metrics.impression_count</th>\n",
       "      <th>entities.urls</th>\n",
       "      <th>public_metrics.bookmark_count</th>\n",
       "      <th>entities.hashtags</th>\n",
       "      <th>attachments.media_keys</th>\n",
       "      <th>entities.annotations</th>\n",
       "      <th>geo.place_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>27245876</td>\n",
       "      <td>1623065216740106240</td>\n",
       "      <td>[{'type': 'replied_to', 'id': '1623082761224347651'}]</td>\n",
       "      <td>[1623292019744423936]</td>\n",
       "      <td>de</td>\n",
       "      <td>1623292019744423936</td>\n",
       "      <td>@ninastahr @gitta_connemann Mensch @ninastahr das wollte ich grade sagen....</td>\n",
       "      <td>2023-02-08T12:05:51.000Z</td>\n",
       "      <td>False</td>\n",
       "      <td>19530651</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>452</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>845282647261761536</td>\n",
       "      <td>1649771111108816898</td>\n",
       "      <td>[{'type': 'quoted', 'id': '1649319733630566401'}]</td>\n",
       "      <td>[1649771111108816898]</td>\n",
       "      <td>de</td>\n",
       "      <td>1649771111108816898</td>\n",
       "      <td>Das sieht schon mal gut aus @energy_charts_d - zeigt auch, dass sich der erfolgreiche Kampf unsr...</td>\n",
       "      <td>2023-04-22T13:44:18.000Z</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>717</td>\n",
       "      <td>[{'start': 234, 'end': 257, 'url': 'https://t.co/22Ys0EjhnX', 'expanded_url': 'https://twitter.c...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            author_id      conversation_id  \\\n",
       "0            27245876  1623065216740106240   \n",
       "1  845282647261761536  1649771111108816898   \n",
       "\n",
       "                                       referenced_tweets  \\\n",
       "0  [{'type': 'replied_to', 'id': '1623082761224347651'}]   \n",
       "1      [{'type': 'quoted', 'id': '1649319733630566401'}]   \n",
       "\n",
       "  edit_history_tweet_ids lang                   id  \\\n",
       "0  [1623292019744423936]   de  1623292019744423936   \n",
       "1  [1649771111108816898]   de  1649771111108816898   \n",
       "\n",
       "                                                                                                  text  \\\n",
       "0                         @ninastahr @gitta_connemann Mensch @ninastahr das wollte ich grade sagen....   \n",
       "1  Das sieht schon mal gut aus @energy_charts_d - zeigt auch, dass sich der erfolgreiche Kampf unsr...   \n",
       "\n",
       "                 created_at  possibly_sensitive in_reply_to_user_id  ...  \\\n",
       "0  2023-02-08T12:05:51.000Z               False            19530651  ...   \n",
       "1  2023-04-22T13:44:18.000Z               False                 NaN  ...   \n",
       "\n",
       "  public_metrics.reply_count  public_metrics.like_count  \\\n",
       "0                          1                          8   \n",
       "1                          1                          4   \n",
       "\n",
       "   public_metrics.quote_count public_metrics.impression_count  \\\n",
       "0                           0                             452   \n",
       "1                           1                             717   \n",
       "\n",
       "                                                                                         entities.urls  \\\n",
       "0                                                                                                  NaN   \n",
       "1  [{'start': 234, 'end': 257, 'url': 'https://t.co/22Ys0EjhnX', 'expanded_url': 'https://twitter.c...   \n",
       "\n",
       "   public_metrics.bookmark_count  entities.hashtags  attachments.media_keys  \\\n",
       "0                            NaN                NaN                     NaN   \n",
       "1                            NaN                NaN                     NaN   \n",
       "\n",
       "   entities.annotations geo.place_id  \n",
       "0                   NaN          NaN  \n",
       "1                   NaN          NaN  \n",
       "\n",
       "[2 rows x 25 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#unnest MHpost and reply:\n",
    "MHpostN = pd.json_normalize(MHpost)\n",
    "MHreplyN = pd.json_normalize(MHreply)\n",
    "MHpostN.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b756b31f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>conversation_id</th>\n",
       "      <th>in_reply_to_user_id</th>\n",
       "      <th>edit_history_tweet_ids</th>\n",
       "      <th>id</th>\n",
       "      <th>author_id</th>\n",
       "      <th>lang</th>\n",
       "      <th>text</th>\n",
       "      <th>referenced_tweets</th>\n",
       "      <th>possibly_sensitive</th>\n",
       "      <th>created_at</th>\n",
       "      <th>...</th>\n",
       "      <th>entities.urls</th>\n",
       "      <th>entities.annotations</th>\n",
       "      <th>attachments.media_keys</th>\n",
       "      <th>entities.hashtags</th>\n",
       "      <th>geo.place_id</th>\n",
       "      <th>attachments.poll_ids</th>\n",
       "      <th>entities.cashtags</th>\n",
       "      <th>withheld.country_codes</th>\n",
       "      <th>withheld.copyright</th>\n",
       "      <th>parsed_create_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1609287864310702080</td>\n",
       "      <td>16808099</td>\n",
       "      <td>[1609626055735746562]</td>\n",
       "      <td>1609626055735746562</td>\n",
       "      <td>1267930834490863621</td>\n",
       "      <td>de</td>\n",
       "      <td>@Ralf_Stegner @fuecks Das wird sicher noch aufgearbeitet werden, wenn die SPD nicht mehr an der ...</td>\n",
       "      <td>[{'type': 'replied_to', 'id': '1609578597572038656'}]</td>\n",
       "      <td>False</td>\n",
       "      <td>2023-01-01T19:02:11.000Z</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-01-01 19:02:11+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1609287864310702080</td>\n",
       "      <td>16808099</td>\n",
       "      <td>[1609626793635467267]</td>\n",
       "      <td>1609626793635467267</td>\n",
       "      <td>1328778631406104577</td>\n",
       "      <td>de</td>\n",
       "      <td>@Ralf_Stegner @fuecks Wollen Sie doch noch einmal überlegen, für welchen Fall genau Nordstream 2...</td>\n",
       "      <td>[{'type': 'replied_to', 'id': '1609578597572038656'}]</td>\n",
       "      <td>False</td>\n",
       "      <td>2023-01-01T19:05:07.000Z</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-01-01 19:05:07+00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       conversation_id in_reply_to_user_id edit_history_tweet_ids  \\\n",
       "0  1609287864310702080            16808099  [1609626055735746562]   \n",
       "1  1609287864310702080            16808099  [1609626793635467267]   \n",
       "\n",
       "                    id            author_id lang  \\\n",
       "0  1609626055735746562  1267930834490863621   de   \n",
       "1  1609626793635467267  1328778631406104577   de   \n",
       "\n",
       "                                                                                                  text  \\\n",
       "0  @Ralf_Stegner @fuecks Das wird sicher noch aufgearbeitet werden, wenn die SPD nicht mehr an der ...   \n",
       "1  @Ralf_Stegner @fuecks Wollen Sie doch noch einmal überlegen, für welchen Fall genau Nordstream 2...   \n",
       "\n",
       "                                       referenced_tweets  possibly_sensitive  \\\n",
       "0  [{'type': 'replied_to', 'id': '1609578597572038656'}]               False   \n",
       "1  [{'type': 'replied_to', 'id': '1609578597572038656'}]               False   \n",
       "\n",
       "                 created_at  ... entities.urls entities.annotations  \\\n",
       "0  2023-01-01T19:02:11.000Z  ...           NaN                  NaN   \n",
       "1  2023-01-01T19:05:07.000Z  ...           NaN                  NaN   \n",
       "\n",
       "  attachments.media_keys  entities.hashtags  geo.place_id  \\\n",
       "0                    NaN                NaN           NaN   \n",
       "1                    NaN                NaN           NaN   \n",
       "\n",
       "   attachments.poll_ids  entities.cashtags  withheld.country_codes  \\\n",
       "0                   NaN                NaN                     NaN   \n",
       "1                   NaN                NaN                     NaN   \n",
       "\n",
       "   withheld.copyright        parsed_create_time  \n",
       "0                 NaN 2023-01-01 19:02:11+00:00  \n",
       "1                 NaN 2023-01-01 19:05:07+00:00  \n",
       "\n",
       "[2 rows x 33 columns]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#add datetime variable to each:\n",
    "# Parse datetime values\n",
    "MHpostN['parsed_create_time'] = pd.to_datetime(MHpostN['created_at'], errors='coerce', utc=True)\n",
    "MHreplyN['parsed_create_time'] = pd.to_datetime(MHreplyN['created_at'], errors='coerce', utc=True)\n",
    "MHreplyN.head(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bed04141",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>full_text</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>screen_name</th>\n",
       "      <th>original_user_id</th>\n",
       "      <th>retweeted_user_ID</th>\n",
       "      <th>collected_at</th>\n",
       "      <th>reply_to_id</th>\n",
       "      <th>reply_to_user</th>\n",
       "      <th>expandedURL</th>\n",
       "      <th>thread_id</th>\n",
       "      <th>parsed_create_time</th>\n",
       "      <th>ascending_comment_index</th>\n",
       "      <th>preceding_comment_index</th>\n",
       "      <th>indexcol</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>If every American would get out of their partisan boxes, this is what every American SHOULD say:...</td>\n",
       "      <td>1.691070e+18</td>\n",
       "      <td>2023-08-14 12:52:42+00:00</td>\n",
       "      <td>WalshFreedom</td>\n",
       "      <td>2.364879e+08</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-08-15 17:13:37.935350</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{'url': 'https://t.co/ixjywZCcYV', 'expanded_url': 'https://twitter.com/i/web/status/1691070339...</td>\n",
       "      <td>57</td>\n",
       "      <td>2023-08-14 12:52:42+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>@WalshFreedom Agree with all of this!</td>\n",
       "      <td>1.691100e+18</td>\n",
       "      <td>2023-08-14 14:50:30+00:00</td>\n",
       "      <td>JMom27</td>\n",
       "      <td>2.514812e+07</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-08-15 16:03:09.973646</td>\n",
       "      <td>1.691070e+18</td>\n",
       "      <td>2.364879e+08</td>\n",
       "      <td>[]</td>\n",
       "      <td>57</td>\n",
       "      <td>2023-08-14 14:50:30+00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>483</th>\n",
       "      <td>We’re gonna get a mugshot in Georgia aren’t we?!?!? 😈</td>\n",
       "      <td>1.691127e+18</td>\n",
       "      <td>2023-08-14 16:37:26+00:00</td>\n",
       "      <td>JoJoFromJerz</td>\n",
       "      <td>8.188931e+17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-08-15 17:20:21.521808</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>234</td>\n",
       "      <td>2023-08-14 16:37:26+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>484</th>\n",
       "      <td>@JoJoFromJerz https://t.co/iICBTy7Fn7</td>\n",
       "      <td>1.691280e+18</td>\n",
       "      <td>2023-08-15 02:43:53+00:00</td>\n",
       "      <td>bsallday1</td>\n",
       "      <td>3.028141e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-08-15 16:10:59.447257</td>\n",
       "      <td>1.691127e+18</td>\n",
       "      <td>8.188931e+17</td>\n",
       "      <td>[]</td>\n",
       "      <td>234</td>\n",
       "      <td>2023-08-15 02:43:53+00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>713</th>\n",
       "      <td>Bring back Aunt Jemima!\\nBring back Uncle Ben!\\nBring back the Land O’ Lakes Indian!\\nBring back...</td>\n",
       "      <td>1.690911e+18</td>\n",
       "      <td>2023-08-14 02:19:15+00:00</td>\n",
       "      <td>ACTBrigitte</td>\n",
       "      <td>7.225285e+17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-08-15 18:06:13.350287</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>346</td>\n",
       "      <td>2023-08-14 02:19:15+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>714</th>\n",
       "      <td>@ACTBrigitte Cancel Bud Light! What?????</td>\n",
       "      <td>1.691207e+18</td>\n",
       "      <td>2023-08-14 21:54:13+00:00</td>\n",
       "      <td>DebbyHouse5</td>\n",
       "      <td>8.036636e+17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-08-15 16:12:20.432690</td>\n",
       "      <td>1.690911e+18</td>\n",
       "      <td>7.225285e+17</td>\n",
       "      <td>[]</td>\n",
       "      <td>346</td>\n",
       "      <td>2023-08-14 21:54:13+00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>884</th>\n",
       "      <td>An extremist Supreme Court has once again reversed decades of settled law, rolled back the march...</td>\n",
       "      <td>1.674449e+18</td>\n",
       "      <td>2023-06-29 16:03:55+00:00</td>\n",
       "      <td>SenWarren</td>\n",
       "      <td>9.702073e+08</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-08-15 17:57:58.914650</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{'url': 'https://t.co/mUeEsQcvEY', 'expanded_url': 'https://twitter.com/i/web/status/1674448621...</td>\n",
       "      <td>429</td>\n",
       "      <td>2023-06-29 16:03:55+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>885</th>\n",
       "      <td>@SenWarren So apparently you don't have faith in certain minorities, to be able to compete in th...</td>\n",
       "      <td>1.674527e+18</td>\n",
       "      <td>2023-06-29 21:16:06+00:00</td>\n",
       "      <td>Mothersdream</td>\n",
       "      <td>3.671425e+07</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-08-15 16:18:03.248367</td>\n",
       "      <td>1.674449e+18</td>\n",
       "      <td>9.702073e+08</td>\n",
       "      <td>[]</td>\n",
       "      <td>429</td>\n",
       "      <td>2023-06-29 21:16:06+00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1084</th>\n",
       "      <td>💜 @Theo_TJ_Jordan https://t.co/9xdBeKH0x3</td>\n",
       "      <td>1.691467e+18</td>\n",
       "      <td>2023-08-15 15:10:47+00:00</td>\n",
       "      <td>DrKarlynB</td>\n",
       "      <td>6.811832e+06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-08-15 16:12:19.829854</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{'url': 'https://t.co/9xdBeKH0x3', 'expanded_url': 'https://twitter.com/theo_tj_jordan/status/1...</td>\n",
       "      <td>527</td>\n",
       "      <td>2023-08-15 15:10:47+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1085</th>\n",
       "      <td>@DrKarlynB True words are easily spoken, my friend. 👊 I appreciate you.</td>\n",
       "      <td>1.691469e+18</td>\n",
       "      <td>2023-08-15 15:18:12+00:00</td>\n",
       "      <td>Theo_TJ_Jordan</td>\n",
       "      <td>1.328423e+18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-08-15 20:17:58.204249</td>\n",
       "      <td>1.691467e+18</td>\n",
       "      <td>6.811832e+06</td>\n",
       "      <td>[]</td>\n",
       "      <td>527</td>\n",
       "      <td>2023-08-15 15:18:12+00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1085</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                full_text  \\\n",
       "117   If every American would get out of their partisan boxes, this is what every American SHOULD say:...   \n",
       "118                                                                 @WalshFreedom Agree with all of this!   \n",
       "483                                                 We’re gonna get a mugshot in Georgia aren’t we?!?!? 😈   \n",
       "484                                                                 @JoJoFromJerz https://t.co/iICBTy7Fn7   \n",
       "713   Bring back Aunt Jemima!\\nBring back Uncle Ben!\\nBring back the Land O’ Lakes Indian!\\nBring back...   \n",
       "714                                                              @ACTBrigitte Cancel Bud Light! What?????   \n",
       "884   An extremist Supreme Court has once again reversed decades of settled law, rolled back the march...   \n",
       "885   @SenWarren So apparently you don't have faith in certain minorities, to be able to compete in th...   \n",
       "1084                                                            💜 @Theo_TJ_Jordan https://t.co/9xdBeKH0x3   \n",
       "1085                              @DrKarlynB True words are easily spoken, my friend. 👊 I appreciate you.   \n",
       "\n",
       "          tweet_id                 created_at     screen_name  \\\n",
       "117   1.691070e+18  2023-08-14 12:52:42+00:00    WalshFreedom   \n",
       "118   1.691100e+18  2023-08-14 14:50:30+00:00          JMom27   \n",
       "483   1.691127e+18  2023-08-14 16:37:26+00:00    JoJoFromJerz   \n",
       "484   1.691280e+18  2023-08-15 02:43:53+00:00       bsallday1   \n",
       "713   1.690911e+18  2023-08-14 02:19:15+00:00     ACTBrigitte   \n",
       "714   1.691207e+18  2023-08-14 21:54:13+00:00     DebbyHouse5   \n",
       "884   1.674449e+18  2023-06-29 16:03:55+00:00       SenWarren   \n",
       "885   1.674527e+18  2023-06-29 21:16:06+00:00    Mothersdream   \n",
       "1084  1.691467e+18  2023-08-15 15:10:47+00:00       DrKarlynB   \n",
       "1085  1.691469e+18  2023-08-15 15:18:12+00:00  Theo_TJ_Jordan   \n",
       "\n",
       "      original_user_id  retweeted_user_ID                collected_at  \\\n",
       "117       2.364879e+08                NaN  2023-08-15 17:13:37.935350   \n",
       "118       2.514812e+07                NaN  2023-08-15 16:03:09.973646   \n",
       "483       8.188931e+17                NaN  2023-08-15 17:20:21.521808   \n",
       "484       3.028141e+09                NaN  2023-08-15 16:10:59.447257   \n",
       "713       7.225285e+17                NaN  2023-08-15 18:06:13.350287   \n",
       "714       8.036636e+17                NaN  2023-08-15 16:12:20.432690   \n",
       "884       9.702073e+08                NaN  2023-08-15 17:57:58.914650   \n",
       "885       3.671425e+07                NaN  2023-08-15 16:18:03.248367   \n",
       "1084      6.811832e+06                NaN  2023-08-15 16:12:19.829854   \n",
       "1085      1.328423e+18                NaN  2023-08-15 20:17:58.204249   \n",
       "\n",
       "       reply_to_id  reply_to_user  \\\n",
       "117            NaN            NaN   \n",
       "118   1.691070e+18   2.364879e+08   \n",
       "483            NaN            NaN   \n",
       "484   1.691127e+18   8.188931e+17   \n",
       "713            NaN            NaN   \n",
       "714   1.690911e+18   7.225285e+17   \n",
       "884            NaN            NaN   \n",
       "885   1.674449e+18   9.702073e+08   \n",
       "1084           NaN            NaN   \n",
       "1085  1.691467e+18   6.811832e+06   \n",
       "\n",
       "                                                                                              expandedURL  \\\n",
       "117   [{'url': 'https://t.co/ixjywZCcYV', 'expanded_url': 'https://twitter.com/i/web/status/1691070339...   \n",
       "118                                                                                                    []   \n",
       "483                                                                                                    []   \n",
       "484                                                                                                    []   \n",
       "713                                                                                                    []   \n",
       "714                                                                                                    []   \n",
       "884   [{'url': 'https://t.co/mUeEsQcvEY', 'expanded_url': 'https://twitter.com/i/web/status/1674448621...   \n",
       "885                                                                                                    []   \n",
       "1084  [{'url': 'https://t.co/9xdBeKH0x3', 'expanded_url': 'https://twitter.com/theo_tj_jordan/status/1...   \n",
       "1085                                                                                                   []   \n",
       "\n",
       "      thread_id        parsed_create_time  ascending_comment_index  \\\n",
       "117          57 2023-08-14 12:52:42+00:00                        0   \n",
       "118          57 2023-08-14 14:50:30+00:00                        1   \n",
       "483         234 2023-08-14 16:37:26+00:00                        0   \n",
       "484         234 2023-08-15 02:43:53+00:00                        1   \n",
       "713         346 2023-08-14 02:19:15+00:00                        0   \n",
       "714         346 2023-08-14 21:54:13+00:00                        1   \n",
       "884         429 2023-06-29 16:03:55+00:00                        0   \n",
       "885         429 2023-06-29 21:16:06+00:00                        1   \n",
       "1084        527 2023-08-15 15:10:47+00:00                        0   \n",
       "1085        527 2023-08-15 15:18:12+00:00                        1   \n",
       "\n",
       "      preceding_comment_index  indexcol  \n",
       "117                       NaN       117  \n",
       "118                       0.0       118  \n",
       "483                       NaN       483  \n",
       "484                       0.0       484  \n",
       "713                       NaN       713  \n",
       "714                       0.0       714  \n",
       "884                       NaN       884  \n",
       "885                       0.0       885  \n",
       "1084                      NaN      1084  \n",
       "1085                      0.0      1085  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NilsThread1k[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f954978",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 496.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Thread\":\n",
      "<['If every American would get out of their partisan boxes, this is what every American SHOULD say:\\n\\n1. If there’s ever sufficient evidence Hunter Biden committed crimes, indict him. \\n\\n2. If there’s ever sufficient evidence Joe Biden committed crimes, indict him.\\n\\n3. Per our justice… https://t.co/ixjywZCcYV']>, \"Target reply\":<<@WalshFreedom Agree with all of this!>>\n",
      "1 1 57\n",
      "\"Thread\":\n",
      "<['We’re gonna get a mugshot in Georgia aren’t we?!?!? 😈']>, \"Target reply\":<<@JoJoFromJerz https://t.co/iICBTy7Fn7>>\n",
      "1 1 234\n",
      "\"Thread\":\n",
      "<['Bring back Aunt Jemima!\\nBring back Uncle Ben!\\nBring back the Land O’ Lakes Indian!\\nBring back the Cleveland Indians!\\nBring back Blockbuster!\\nBring back the Washington Redskins!\\nBring back GOYA!\\n\\nCancel culture has no place in America!']>, \"Target reply\":<<@ACTBrigitte Cancel Bud Light! What?????>>\n",
      "1 1 346\n",
      "\"Thread\":\n",
      "<[\"An extremist Supreme Court has once again reversed decades of settled law, rolled back the march toward racial justice, and narrowed educational opportunity for all. I won't stop fighting for young people with big dreams who deserve an equal chance to pursue their future. https://t.co/mUeEsQcvEY\"]>, \"Target reply\":<<@SenWarren So apparently you don't have faith in certain minorities, to be able to compete in this world. Your racism is showing again.>>\n",
      "1 1 429\n",
      "\"Thread\":\n",
      "<['💜 @Theo_TJ_Jordan https://t.co/9xdBeKH0x3']>, \"Target reply\":<<@DrKarlynB True words are easily spoken, my friend. 👊 I appreciate you.>>\n",
      "1 1 527\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#test post expansion prompt creation with Llama3.3:\n",
    "chunked_result: typing.List[pd.DataFrame] = []\n",
    "GROUPER='thread_id'\n",
    "\n",
    "#sort data by Time_comment\n",
    "groupeddata = NilsThread1k[:10].groupby(GROUPER)\n",
    "for group, df in tqdm.tqdm(groupeddata):\n",
    "    df = df.loc[df.full_text != '',:]\n",
    "    df.sort_values(by=['parsed_create_time'], ascending=True).reset_index(drop=True, inplace=True)\n",
    "    df.loc[:, 'preceding_index'] = df.ascending_comment_index.shift(1)\n",
    "    df.set_index('ascending_comment_index', inplace=True, drop=False)\n",
    "    for index, row in df.iterrows():\n",
    "        if pd.notna(row['preceding_index']):\n",
    "            print(f'\"Thread\":\\n<{df[\"full_text\"][:index].to_list()}>, \"Target reply\":<<{row[\"full_text\"]}>>')\n",
    "            print(row['ascending_comment_index'], row['thread_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "7e2cbb1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 234/234 [2:49:08<00:00, 43.37s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     index  ascending_comment_index  thread_id  \\\n",
      "0      117                        0         57   \n",
      "1      118                        1         57   \n",
      "2      483                        0        234   \n",
      "3      484                        1        234   \n",
      "4      713                        0        346   \n",
      "..     ...                      ...        ...   \n",
      "495  50284                        1      24261   \n",
      "496  50675                        0      24451   \n",
      "497  50676                        1      24451   \n",
      "498  50788                        0      24507   \n",
      "499  50789                        1      24507   \n",
      "\n",
      "                                                                                          post_expansion  \n",
      "0    If every American would get out of their partisan boxes, this is what every American SHOULD say:...  \n",
      "1    I completely agree that if there's sufficient evidence, Hunter Biden and Joe Biden should be ind...  \n",
      "2                                                  We’re gonna get a mugshot in Georgia aren’t we?!?!? 😈  \n",
      "3                                                                '@JoJoFromJerz https://t.co/iICBTy7Fn7'  \n",
      "4    Bring back Aunt Jemima!\\nBring back Uncle Ben!\\nBring back the Land O’ Lakes Indian!\\nBring back...  \n",
      "..                                                                                                   ...  \n",
      "495  \"@NickAdamsinUSA, your idea of an alpha male, which includes prioritizing God and Bible, friends...  \n",
      "496  There is nothing wrong with going to Hooters with the boys, or even taking your family to Hooter...  \n",
      "497  You're obsessed with criticizing people for choosing to dine at Hooters with their families, des...  \n",
      "498                                    They Think We Are Stupid, downer edition… https://t.co/1Xswp3IvTi  \n",
      "499  @akheriaty seems to be skeptical or questioning the content of the link provided in the previous...  \n",
      "\n",
      "[500 rows x 4 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#test post expansion with Llama3.3:\n",
    "chunked_result: typing.List[pd.DataFrame] = []\n",
    "GROUPER='thread_id'\n",
    "\n",
    "#sort data by Time_comment\n",
    "groupeddata = NilsThread1k[:500].groupby(GROUPER)\n",
    "for group, df in tqdm.tqdm(groupeddata):\n",
    "    df = df.loc[df.full_text != '',:]\n",
    "    df.sort_values(by=['parsed_create_time'], ascending=True).reset_index(drop=True, inplace=True)\n",
    "    df.loc[:, 'preceding_index'] = df.ascending_comment_index.shift(1)\n",
    "    df.set_index('ascending_comment_index', inplace=True, drop=False)\n",
    "    for index, row in df.iterrows():\n",
    "        if pd.isna(row['preceding_index']):\n",
    "            chunked_result.append(\n",
    "                pd.DataFrame(\n",
    "                    data=[[row['indexcol'], row['ascending_comment_index'], row['thread_id'], row['full_text']]],\n",
    "                    columns=['index', 'ascending_comment_index', 'thread_id', 'post_expansion']\n",
    "                )\n",
    "            )\n",
    "        if pd.notna(row['preceding_index']):\n",
    "            try: \n",
    "                chunked_result.append(\n",
    "                   pd.DataFrame(\n",
    "                       data=[[row['indexcol'], row['ascending_comment_index'], row['thread_id'],\n",
    "                           requests.post(\n",
    "                               'https://inf.cl.uni-trier.de/',\n",
    "                               json={\n",
    "                                   'model': MODEL33large,\n",
    "                                   'system': SYSTEM_expansion,\n",
    "                                   'prompt': f'\"Thread\":\\n<{df[\"full_text\"][:index].to_list()}>, \"Target reply\":<<{row[\"full_text\"]}>>',\n",
    "                                   'options': options_zero,\n",
    "                                   'seed': SEED,\n",
    "                                   }).json()['response']                       \n",
    "                       ]],\n",
    "                       columns=['index', 'ascending_comment_index', 'thread_id', 'post_expansion']\n",
    "                   )\n",
    "                )\n",
    "            except json.JSONDecodeError:\n",
    "                print(\"invalid json response, skipping to next batch\")\n",
    "\n",
    "expanded_posts = pd.concat(chunked_result, ignore_index=True)\n",
    "print(expanded_posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "f6d548fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "NilsThread500merge = NilsThread1k[:500].merge(expanded_posts, left_on='indexcol', right_on='index', how='left').drop(columns=['index', 'ascending_comment_index_y', 'thread_id_y'], axis=1).rename(columns={'ascending_comment_index_x': 'ascending_comment_index', 'thread_id_x': 'thread_id'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6daca50",
   "metadata": {},
   "outputs": [],
   "source": [
    "NilsThread500merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "38693821",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save dataframe to parquet file:\n",
    "NilsThread500merge.to_parquet('data/TWON_export_Nils/NilsThread500merge.parquet', index=False, engine='pyarrow')\n",
    "expanded_posts.to_parquet('data/TWON_export_Nils/expanded_posts500.parquet', index=False, engine='pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07eb31be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the parquet file:\n",
    "NilsThread500merge = pd.read_parquet('data/TWON_export_Nils/NilsThread500merge.parquet', engine='pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97edef9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check whether we have the whole thread or only chains of replies of replies, in the latter case perhaps only add preceding comment rather than the whole thread as context?\n",
    "#what is the post depth?\n",
    "#currently the average number of tweets per thread is rather low, so I wonder whether we have the whole thread...\n",
    "#MHpost probably do have the whole thread, but this corpus is in German, which can be more challenging for the model...\n",
    "#also the NilsThread1k corpus does not match the structure stipulated by Nils in his email: \"The repo also includes a sample dataset generated using one of my trained models. Note that the format differs from our standard structure - I've combined all tweets in each thread using the format: \"\\n\\n>Username: {Tweet}\". This approach allows me to insert the complete conversational state into the model at once.\n",
    "#For each example, you'll find both the synthetic reply (generated by one of my models) and the actual user reply for comparison.\"\n",
    "#perhaps this format is only used for the synthetic replies, and the original tweets are still in the original format?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "af949b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we can simply check whether the reply_to_id is always equal to the tweet_id of the preceding post\n",
    "#add the tweet_id of the preceding post to the dataframe, by merging the dataframe with itself on the preceding_comment_index:\n",
    "NilsThread1k_plus = NilsThread1k.merge(NilsThread1k.loc[:, ['thread_id', 'ascending_comment_index', 'tweet_id']], left_on=['thread_id','preceding_comment_index'], right_on=['thread_id','ascending_comment_index'], how='left', suffixes=('', '_preceding'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "5df11193",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(1.0)"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(NilsThread1k_plus.loc[~NilsThread1k_plus.tweet_id_preceding.isna(),'tweet_id_preceding'] == NilsThread1k_plus.loc[~NilsThread1k_plus.tweet_id_preceding.isna(),'reply_to_id']).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b30bf99",
   "metadata": {},
   "outputs": [],
   "source": [
    "#confirmation that this is a reply of reply structure, rather than a full thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d082b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/234 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 11/234 [00:09<05:03,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to connect to API. Status code: 400\n",
      "{\"error\":{\"message\":\"litellm.BadRequestError: Azure_aiException - {\\\"error\\\":{\\\"message\\\":\\\"The response was filtered due to the prompt triggering Microsoft's content management policy. Please modify your prompt and retry.\\\",\\\"type\\\":null,\\\"param\\\":\\\"prompt\\\",\\\"code\\\":\\\"content_filter\\\",\\\"status\\\":400,\\\"innererror\\\":{\\\"code\\\":\\\"ResponsibleAIPolicyViolation\\\",\\\"content_filter_result\\\":{\\\"hate\\\":{\\\"filtered\\\":true,\\\"severity\\\":\\\"medium\\\"},\\\"self_harm\\\":{\\\"filtered\\\":false,\\\"severity\\\":\\\"safe\\\"},\\\"sexual\\\":{\\\"filtered\\\":false,\\\"severity\\\":\\\"safe\\\"},\\\"violence\\\":{\\\"filtered\\\":false,\\\"severity\\\":\\\"safe\\\"}}}}}. Received Model Group=Llama-3.3-70B-Instruct\\nAvailable Model Group Fallbacks=None\",\"type\":null,\"param\":null,\"code\":\"400\"}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 29/234 [00:22<02:02,  1.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to connect to API. Status code: 500. Retrying in 20 seconds...\n",
      "{\"error\":{\"message\":\"litellm.APIConnectionError: APIConnectionError: Azure_aiException - Invalid response object Traceback (most recent call last):\\n  File \\\"/usr/lib/python3.13/site-packages/litellm/litellm_core_utils/llm_response_utils/convert_dict_to_response.py\\\", line 473, in convert_to_model_response_object\\n    reasoning_content, content = _extract_reasoning_content(\\n                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~^\\n        choice[\\\"message\\\"]\\n        ^^^^^^^^^^^^^^^^^\\n    )\\n    ^\\n  File \\\"/usr/lib/python3.13/site-packages/litellm/litellm_core_utils/llm_response_utils/convert_dict_to_response.py\\\", line 253, in _extract_reasoning_content\\n    return message[\\\"reasoning_content\\\"], message[\\\"content\\\"]\\n                                         ~~~~~~~^^^^^^^^^^^\\nKeyError: 'content'\\n\\n\\nreceived_args={'response_object': {'choices': [{'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}, 'finish_reason': 'content_filter', 'index': 0, 'message': {'reasoning_content': None, 'role': 'assistant', 'tool_calls': []}}], 'created': 1746018516, 'id': 'd30699db-7c04-4aea-bc67-4c24c9fb3aef', 'model': 'Llama-3.3-70B-Instruct', 'object': 'chat.completion', 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'low'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'usage': {'completion_tokens': 16, 'prompt_tokens': 390, 'prompt_tokens_details': None, 'total_tokens': 406}}, 'model_response_object': ModelResponse(id='chatcmpl-15269f79-fb2f-4917-b4a9-b9ebc124f0f1', created=1746018513, model='azure_ai/Llama-3.3-70B-Instruct', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content=None, role='assistant', tool_calls=None, function_call=None, provider_specific_fields=None))], usage=Usage(completion_tokens=0, prompt_tokens=0, total_tokens=0, completion_tokens_details=None, prompt_tokens_details=None)), 'response_type': 'completion', 'stream': False, 'start_time': None, 'end_time': None, 'hidden_params': {'headers': Headers({'content-length': '830', 'content-type': 'application/json', 'prompt_token_len': '390', 'sampling_token_len': '16', 'x-ms-rai-invoked': 'true', 'x-request-id': 'd30699db-7c04-4aea-bc67-4c24c9fb3aef', 'azureml-destination-model-group': 'meta-sdc-3-3-70b-fp8-2-scaleset', 'azureml-destination-region': 'swedencentral', 'azureml-destination-deployment': 'dep-70b-fp8-tp2-250401012727', 'azureml-destination-endpoint': 'sdc-llama-3-3-70b-llgtrt-fp8-ep', 'x-ms-client-request-id': '85db0126-893d-4ec3-ba4f-e2b316a3c24b', 'request-context': 'appId=cid-v1:bf86afeb-6735-4630-a419-996fd98375c2', 'azureml-model-session': 'dep-70b-fp8-tp2-250401012727', 'azureml-model-group': 'meta-sdc-3-3-70b-fp8-2-scaleset', 'date': 'Wed, 30 Apr 2025 13:08:33 GMT'}), 'additional_headers': {'llm_provider-content-length': '830', 'llm_provider-content-type': 'application/json', 'llm_provider-prompt_token_len': '390', 'llm_provider-sampling_token_len': '16', 'llm_provider-x-ms-rai-invoked': 'true', 'llm_provider-x-request-id': 'd30699db-7c04-4aea-bc67-4c24c9fb3aef', 'llm_provider-azureml-destination-model-group': 'meta-sdc-3-3-70b-fp8-2-scaleset', 'llm_provider-azureml-destination-region': 'swedencentral', 'llm_provider-azureml-destination-deployment': 'dep-70b-fp8-tp2-250401012727', 'llm_provider-azureml-destination-endpoint': 'sdc-llama-3-3-70b-llgtrt-fp8-ep', 'llm_provider-x-ms-client-request-id': '85db0126-893d-4ec3-ba4f-e2b316a3c24b', 'llm_provider-request-context': 'appId=cid-v1:bf86afeb-6735-4630-a419-996fd98375c2', 'llm_provider-azureml-model-session': 'dep-70b-fp8-tp2-250401012727', 'llm_provider-azureml-model-group': 'meta-sdc-3-3-70b-fp8-2-scaleset', 'llm_provider-date': 'Wed, 30 Apr 2025 13:08:33 GMT'}}, '_response_headers': {'content-length': '830', 'content-type': 'application/json', 'prompt_token_len': '390', 'sampling_token_len': '16', 'x-ms-rai-invoked': 'true', 'x-request-id': 'd30699db-7c04-4aea-bc67-4c24c9fb3aef', 'azureml-destination-model-group': 'meta-sdc-3-3-70b-fp8-2-scaleset', 'azureml-destination-region': 'swedencentral', 'azureml-destination-deployment': 'dep-70b-fp8-tp2-250401012727', 'azureml-destination-endpoint': 'sdc-llama-3-3-70b-llgtrt-fp8-ep', 'x-ms-client-request-id': '85db0126-893d-4ec3-ba4f-e2b316a3c24b', 'request-context': 'appId=cid-v1:bf86afeb-6735-4630-a419-996fd98375c2', 'azureml-model-session': 'dep-70b-fp8-tp2-250401012727', 'azureml-model-group': 'meta-sdc-3-3-70b-fp8-2-scaleset', 'date': 'Wed, 30 Apr 2025 13:08:33 GMT'}, 'convert_tool_call_to_json_mode': None}. Received Model Group=Llama-3.3-70B-Instruct\\nAvailable Model Group Fallbacks=None\",\"type\":null,\"param\":null,\"code\":\"500\"}}\n",
      "Failed to connect to API. Status code: 500. Retrying in 20 seconds...\n",
      "{\"error\":{\"message\":\"litellm.APIConnectionError: APIConnectionError: Azure_aiException - Invalid response object Traceback (most recent call last):\\n  File \\\"/usr/lib/python3.13/site-packages/litellm/litellm_core_utils/llm_response_utils/convert_dict_to_response.py\\\", line 473, in convert_to_model_response_object\\n    reasoning_content, content = _extract_reasoning_content(\\n                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~^\\n        choice[\\\"message\\\"]\\n        ^^^^^^^^^^^^^^^^^\\n    )\\n    ^\\n  File \\\"/usr/lib/python3.13/site-packages/litellm/litellm_core_utils/llm_response_utils/convert_dict_to_response.py\\\", line 253, in _extract_reasoning_content\\n    return message[\\\"reasoning_content\\\"], message[\\\"content\\\"]\\n                                         ~~~~~~~^^^^^^^^^^^\\nKeyError: 'content'\\n\\n\\nreceived_args={'response_object': {'choices': [{'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}, 'finish_reason': 'content_filter', 'index': 0, 'message': {'reasoning_content': None, 'role': 'assistant', 'tool_calls': []}}], 'created': 1746018537, 'id': '0cdb2f17-2918-4852-b4ef-25e62fc0d2e9', 'model': 'Llama-3.3-70B-Instruct', 'object': 'chat.completion', 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'low'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'usage': {'completion_tokens': 16, 'prompt_tokens': 390, 'prompt_tokens_details': None, 'total_tokens': 406}}, 'model_response_object': ModelResponse(id='chatcmpl-261ca06e-0c7c-4fcd-9e6a-e0d2b3ad7a3c', created=1746018536, model='azure_ai/Llama-3.3-70B-Instruct', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content=None, role='assistant', tool_calls=None, function_call=None, provider_specific_fields=None))], usage=Usage(completion_tokens=0, prompt_tokens=0, total_tokens=0, completion_tokens_details=None, prompt_tokens_details=None)), 'response_type': 'completion', 'stream': False, 'start_time': None, 'end_time': None, 'hidden_params': {'headers': Headers({'content-length': '830', 'content-type': 'application/json', 'prompt_token_len': '390', 'sampling_token_len': '16', 'x-ms-rai-invoked': 'true', 'x-request-id': '0cdb2f17-2918-4852-b4ef-25e62fc0d2e9', 'azureml-destination-model-group': 'meta-sdc-3-3-70b-fp8-2-scaleset', 'azureml-destination-region': 'swedencentral', 'azureml-destination-deployment': 'dep-70b-fp8-tp2-250401005907', 'azureml-destination-endpoint': 'sdc-llama-3-3-70b-llgtrt-fp8-ep', 'x-ms-client-request-id': 'f2f3762c-47f0-4d92-877a-9fe137695aec', 'request-context': 'appId=cid-v1:bf86afeb-6735-4630-a419-996fd98375c2', 'azureml-model-session': 'dep-70b-fp8-tp2-250401005907', 'azureml-model-group': 'meta-sdc-3-3-70b-fp8-2-scaleset', 'date': 'Wed, 30 Apr 2025 13:08:56 GMT'}), 'additional_headers': {'llm_provider-content-length': '830', 'llm_provider-content-type': 'application/json', 'llm_provider-prompt_token_len': '390', 'llm_provider-sampling_token_len': '16', 'llm_provider-x-ms-rai-invoked': 'true', 'llm_provider-x-request-id': '0cdb2f17-2918-4852-b4ef-25e62fc0d2e9', 'llm_provider-azureml-destination-model-group': 'meta-sdc-3-3-70b-fp8-2-scaleset', 'llm_provider-azureml-destination-region': 'swedencentral', 'llm_provider-azureml-destination-deployment': 'dep-70b-fp8-tp2-250401005907', 'llm_provider-azureml-destination-endpoint': 'sdc-llama-3-3-70b-llgtrt-fp8-ep', 'llm_provider-x-ms-client-request-id': 'f2f3762c-47f0-4d92-877a-9fe137695aec', 'llm_provider-request-context': 'appId=cid-v1:bf86afeb-6735-4630-a419-996fd98375c2', 'llm_provider-azureml-model-session': 'dep-70b-fp8-tp2-250401005907', 'llm_provider-azureml-model-group': 'meta-sdc-3-3-70b-fp8-2-scaleset', 'llm_provider-date': 'Wed, 30 Apr 2025 13:08:56 GMT'}}, '_response_headers': {'content-length': '830', 'content-type': 'application/json', 'prompt_token_len': '390', 'sampling_token_len': '16', 'x-ms-rai-invoked': 'true', 'x-request-id': '0cdb2f17-2918-4852-b4ef-25e62fc0d2e9', 'azureml-destination-model-group': 'meta-sdc-3-3-70b-fp8-2-scaleset', 'azureml-destination-region': 'swedencentral', 'azureml-destination-deployment': 'dep-70b-fp8-tp2-250401005907', 'azureml-destination-endpoint': 'sdc-llama-3-3-70b-llgtrt-fp8-ep', 'x-ms-client-request-id': 'f2f3762c-47f0-4d92-877a-9fe137695aec', 'request-context': 'appId=cid-v1:bf86afeb-6735-4630-a419-996fd98375c2', 'azureml-model-session': 'dep-70b-fp8-tp2-250401005907', 'azureml-model-group': 'meta-sdc-3-3-70b-fp8-2-scaleset', 'date': 'Wed, 30 Apr 2025 13:08:56 GMT'}, 'convert_tool_call_to_json_mode': None}. Received Model Group=Llama-3.3-70B-Instruct\\nAvailable Model Group Fallbacks=None\",\"type\":null,\"param\":null,\"code\":\"500\"}}\n",
      "Failed to connect to API. Status code: 500. Retrying in 20 seconds...\n",
      "{\"error\":{\"message\":\"litellm.APIConnectionError: APIConnectionError: Azure_aiException - Invalid response object Traceback (most recent call last):\\n  File \\\"/usr/lib/python3.13/site-packages/litellm/litellm_core_utils/llm_response_utils/convert_dict_to_response.py\\\", line 473, in convert_to_model_response_object\\n    reasoning_content, content = _extract_reasoning_content(\\n                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~^\\n        choice[\\\"message\\\"]\\n        ^^^^^^^^^^^^^^^^^\\n    )\\n    ^\\n  File \\\"/usr/lib/python3.13/site-packages/litellm/litellm_core_utils/llm_response_utils/convert_dict_to_response.py\\\", line 253, in _extract_reasoning_content\\n    return message[\\\"reasoning_content\\\"], message[\\\"content\\\"]\\n                                         ~~~~~~~^^^^^^^^^^^\\nKeyError: 'content'\\n\\n\\nreceived_args={'response_object': {'choices': [{'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}, 'finish_reason': 'content_filter', 'index': 0, 'message': {'reasoning_content': None, 'role': 'assistant', 'tool_calls': []}}], 'created': 1746018563, 'id': '4ef587f3-2d7b-4725-a40e-f68be5b16047', 'model': 'Llama-3.3-70B-Instruct', 'object': 'chat.completion', 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'low'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'usage': {'completion_tokens': 16, 'prompt_tokens': 390, 'prompt_tokens_details': None, 'total_tokens': 406}}, 'model_response_object': ModelResponse(id='chatcmpl-359e0553-be2a-4f88-b93e-7595debbfb4b', created=1746018559, model='azure_ai/Llama-3.3-70B-Instruct', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content=None, role='assistant', tool_calls=None, function_call=None, provider_specific_fields=None))], usage=Usage(completion_tokens=0, prompt_tokens=0, total_tokens=0, completion_tokens_details=None, prompt_tokens_details=None)), 'response_type': 'completion', 'stream': False, 'start_time': None, 'end_time': None, 'hidden_params': {'headers': Headers({'content-length': '830', 'content-type': 'application/json', 'prompt_token_len': '390', 'sampling_token_len': '16', 'x-ms-rai-invoked': 'true', 'x-request-id': '4ef587f3-2d7b-4725-a40e-f68be5b16047', 'azureml-destination-model-group': 'meta-sdc-3-3-70b-fp8-2-scaleset', 'azureml-destination-region': 'swedencentral', 'azureml-destination-deployment': 'dep-70b-fp8-tp2-250401012727', 'azureml-destination-endpoint': 'sdc-llama-3-3-70b-llgtrt-fp8-ep', 'x-ms-client-request-id': '71b01fb4-a4c1-43cd-9c87-28532aa63622', 'request-context': 'appId=cid-v1:bf86afeb-6735-4630-a419-996fd98375c2', 'azureml-model-session': 'dep-70b-fp8-tp2-250401012727', 'azureml-model-group': 'meta-sdc-3-3-70b-fp8-2-scaleset', 'date': 'Wed, 30 Apr 2025 13:09:20 GMT'}), 'additional_headers': {'llm_provider-content-length': '830', 'llm_provider-content-type': 'application/json', 'llm_provider-prompt_token_len': '390', 'llm_provider-sampling_token_len': '16', 'llm_provider-x-ms-rai-invoked': 'true', 'llm_provider-x-request-id': '4ef587f3-2d7b-4725-a40e-f68be5b16047', 'llm_provider-azureml-destination-model-group': 'meta-sdc-3-3-70b-fp8-2-scaleset', 'llm_provider-azureml-destination-region': 'swedencentral', 'llm_provider-azureml-destination-deployment': 'dep-70b-fp8-tp2-250401012727', 'llm_provider-azureml-destination-endpoint': 'sdc-llama-3-3-70b-llgtrt-fp8-ep', 'llm_provider-x-ms-client-request-id': '71b01fb4-a4c1-43cd-9c87-28532aa63622', 'llm_provider-request-context': 'appId=cid-v1:bf86afeb-6735-4630-a419-996fd98375c2', 'llm_provider-azureml-model-session': 'dep-70b-fp8-tp2-250401012727', 'llm_provider-azureml-model-group': 'meta-sdc-3-3-70b-fp8-2-scaleset', 'llm_provider-date': 'Wed, 30 Apr 2025 13:09:20 GMT'}}, '_response_headers': {'content-length': '830', 'content-type': 'application/json', 'prompt_token_len': '390', 'sampling_token_len': '16', 'x-ms-rai-invoked': 'true', 'x-request-id': '4ef587f3-2d7b-4725-a40e-f68be5b16047', 'azureml-destination-model-group': 'meta-sdc-3-3-70b-fp8-2-scaleset', 'azureml-destination-region': 'swedencentral', 'azureml-destination-deployment': 'dep-70b-fp8-tp2-250401012727', 'azureml-destination-endpoint': 'sdc-llama-3-3-70b-llgtrt-fp8-ep', 'x-ms-client-request-id': '71b01fb4-a4c1-43cd-9c87-28532aa63622', 'request-context': 'appId=cid-v1:bf86afeb-6735-4630-a419-996fd98375c2', 'azureml-model-session': 'dep-70b-fp8-tp2-250401012727', 'azureml-model-group': 'meta-sdc-3-3-70b-fp8-2-scaleset', 'date': 'Wed, 30 Apr 2025 13:09:20 GMT'}, 'convert_tool_call_to_json_mode': None}. Received Model Group=Llama-3.3-70B-Instruct\\nAvailable Model Group Fallbacks=None\",\"type\":null,\"param\":null,\"code\":\"500\"}}\n",
      "Failed to connect to API. Status code: 500. Retrying in 20 seconds...\n",
      "{\"error\":{\"message\":\"litellm.APIConnectionError: APIConnectionError: Azure_aiException - Invalid response object Traceback (most recent call last):\\n  File \\\"/usr/lib/python3.13/site-packages/litellm/litellm_core_utils/llm_response_utils/convert_dict_to_response.py\\\", line 473, in convert_to_model_response_object\\n    reasoning_content, content = _extract_reasoning_content(\\n                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~^\\n        choice[\\\"message\\\"]\\n        ^^^^^^^^^^^^^^^^^\\n    )\\n    ^\\n  File \\\"/usr/lib/python3.13/site-packages/litellm/litellm_core_utils/llm_response_utils/convert_dict_to_response.py\\\", line 253, in _extract_reasoning_content\\n    return message[\\\"reasoning_content\\\"], message[\\\"content\\\"]\\n                                         ~~~~~~~^^^^^^^^^^^\\nKeyError: 'content'\\n\\n\\nreceived_args={'response_object': {'choices': [{'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}, 'finish_reason': 'content_filter', 'index': 0, 'message': {'reasoning_content': None, 'role': 'assistant', 'tool_calls': []}}], 'created': 1746018586, 'id': '71c3f0ab-2f89-4970-afaf-7af91b59c5c3', 'model': 'Llama-3.3-70B-Instruct', 'object': 'chat.completion', 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'low'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'usage': {'completion_tokens': 16, 'prompt_tokens': 390, 'prompt_tokens_details': None, 'total_tokens': 406}}, 'model_response_object': ModelResponse(id='chatcmpl-2b9fc201-e6ef-4364-ad40-1d5e9586af9a', created=1746018583, model='azure_ai/Llama-3.3-70B-Instruct', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content=None, role='assistant', tool_calls=None, function_call=None, provider_specific_fields=None))], usage=Usage(completion_tokens=0, prompt_tokens=0, total_tokens=0, completion_tokens_details=None, prompt_tokens_details=None)), 'response_type': 'completion', 'stream': False, 'start_time': None, 'end_time': None, 'hidden_params': {'headers': Headers({'content-length': '830', 'content-type': 'application/json', 'prompt_token_len': '390', 'sampling_token_len': '16', 'x-ms-rai-invoked': 'true', 'x-request-id': '71c3f0ab-2f89-4970-afaf-7af91b59c5c3', 'azureml-destination-model-group': 'meta-sdc-3-3-70b-fp8-2-scaleset', 'azureml-destination-region': 'swedencentral', 'azureml-destination-deployment': 'dep-70b-fp8-tp2-250401012727', 'azureml-destination-endpoint': 'sdc-llama-3-3-70b-llgtrt-fp8-ep', 'x-ms-client-request-id': '7fa02871-57ff-4921-bd72-7b8b44dcc4ac', 'request-context': 'appId=cid-v1:bf86afeb-6735-4630-a419-996fd98375c2', 'azureml-model-session': 'dep-70b-fp8-tp2-250401012727', 'azureml-model-group': 'meta-sdc-3-3-70b-fp8-2-scaleset', 'date': 'Wed, 30 Apr 2025 13:09:43 GMT'}), 'additional_headers': {'llm_provider-content-length': '830', 'llm_provider-content-type': 'application/json', 'llm_provider-prompt_token_len': '390', 'llm_provider-sampling_token_len': '16', 'llm_provider-x-ms-rai-invoked': 'true', 'llm_provider-x-request-id': '71c3f0ab-2f89-4970-afaf-7af91b59c5c3', 'llm_provider-azureml-destination-model-group': 'meta-sdc-3-3-70b-fp8-2-scaleset', 'llm_provider-azureml-destination-region': 'swedencentral', 'llm_provider-azureml-destination-deployment': 'dep-70b-fp8-tp2-250401012727', 'llm_provider-azureml-destination-endpoint': 'sdc-llama-3-3-70b-llgtrt-fp8-ep', 'llm_provider-x-ms-client-request-id': '7fa02871-57ff-4921-bd72-7b8b44dcc4ac', 'llm_provider-request-context': 'appId=cid-v1:bf86afeb-6735-4630-a419-996fd98375c2', 'llm_provider-azureml-model-session': 'dep-70b-fp8-tp2-250401012727', 'llm_provider-azureml-model-group': 'meta-sdc-3-3-70b-fp8-2-scaleset', 'llm_provider-date': 'Wed, 30 Apr 2025 13:09:43 GMT'}}, '_response_headers': {'content-length': '830', 'content-type': 'application/json', 'prompt_token_len': '390', 'sampling_token_len': '16', 'x-ms-rai-invoked': 'true', 'x-request-id': '71c3f0ab-2f89-4970-afaf-7af91b59c5c3', 'azureml-destination-model-group': 'meta-sdc-3-3-70b-fp8-2-scaleset', 'azureml-destination-region': 'swedencentral', 'azureml-destination-deployment': 'dep-70b-fp8-tp2-250401012727', 'azureml-destination-endpoint': 'sdc-llama-3-3-70b-llgtrt-fp8-ep', 'x-ms-client-request-id': '7fa02871-57ff-4921-bd72-7b8b44dcc4ac', 'request-context': 'appId=cid-v1:bf86afeb-6735-4630-a419-996fd98375c2', 'azureml-model-session': 'dep-70b-fp8-tp2-250401012727', 'azureml-model-group': 'meta-sdc-3-3-70b-fp8-2-scaleset', 'date': 'Wed, 30 Apr 2025 13:09:43 GMT'}, 'convert_tool_call_to_json_mode': None}. Received Model Group=Llama-3.3-70B-Instruct\\nAvailable Model Group Fallbacks=None\",\"type\":null,\"param\":null,\"code\":\"500\"}}\n",
      "Failed to connect to API. Status code: 500. Retrying in 20 seconds...\n",
      "{\"error\":{\"message\":\"litellm.APIConnectionError: APIConnectionError: Azure_aiException - Invalid response object Traceback (most recent call last):\\n  File \\\"/usr/lib/python3.13/site-packages/litellm/litellm_core_utils/llm_response_utils/convert_dict_to_response.py\\\", line 473, in convert_to_model_response_object\\n    reasoning_content, content = _extract_reasoning_content(\\n                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~^\\n        choice[\\\"message\\\"]\\n        ^^^^^^^^^^^^^^^^^\\n    )\\n    ^\\n  File \\\"/usr/lib/python3.13/site-packages/litellm/litellm_core_utils/llm_response_utils/convert_dict_to_response.py\\\", line 253, in _extract_reasoning_content\\n    return message[\\\"reasoning_content\\\"], message[\\\"content\\\"]\\n                                         ~~~~~~~^^^^^^^^^^^\\nKeyError: 'content'\\n\\n\\nreceived_args={'response_object': {'choices': [{'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}, 'finish_reason': 'content_filter', 'index': 0, 'message': {'reasoning_content': None, 'role': 'assistant', 'tool_calls': []}}], 'created': 1746018607, 'id': 'cb076ae6-8d83-49f4-8e51-c7c825a828d2', 'model': 'Llama-3.3-70B-Instruct', 'object': 'chat.completion', 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'low'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'usage': {'completion_tokens': 16, 'prompt_tokens': 390, 'prompt_tokens_details': None, 'total_tokens': 406}}, 'model_response_object': ModelResponse(id='chatcmpl-7e34ef40-b30c-4631-b1a2-6e57610a0b57', created=1746018606, model='azure_ai/Llama-3.3-70B-Instruct', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content=None, role='assistant', tool_calls=None, function_call=None, provider_specific_fields=None))], usage=Usage(completion_tokens=0, prompt_tokens=0, total_tokens=0, completion_tokens_details=None, prompt_tokens_details=None)), 'response_type': 'completion', 'stream': False, 'start_time': None, 'end_time': None, 'hidden_params': {'headers': Headers({'content-length': '830', 'content-type': 'application/json', 'prompt_token_len': '390', 'sampling_token_len': '16', 'x-ms-rai-invoked': 'true', 'x-request-id': 'cb076ae6-8d83-49f4-8e51-c7c825a828d2', 'azureml-destination-model-group': 'meta-sdc-3-3-70b-fp8-2-scaleset', 'azureml-destination-region': 'swedencentral', 'azureml-destination-deployment': 'dep-70b-fp8-tp2-250401005907', 'azureml-destination-endpoint': 'sdc-llama-3-3-70b-llgtrt-fp8-ep', 'x-ms-client-request-id': 'db13c7a8-557d-470e-990e-79732793b148', 'request-context': 'appId=cid-v1:bf86afeb-6735-4630-a419-996fd98375c2', 'azureml-model-session': 'dep-70b-fp8-tp2-250401005907', 'azureml-model-group': 'meta-sdc-3-3-70b-fp8-2-scaleset', 'date': 'Wed, 30 Apr 2025 13:10:06 GMT'}), 'additional_headers': {'llm_provider-content-length': '830', 'llm_provider-content-type': 'application/json', 'llm_provider-prompt_token_len': '390', 'llm_provider-sampling_token_len': '16', 'llm_provider-x-ms-rai-invoked': 'true', 'llm_provider-x-request-id': 'cb076ae6-8d83-49f4-8e51-c7c825a828d2', 'llm_provider-azureml-destination-model-group': 'meta-sdc-3-3-70b-fp8-2-scaleset', 'llm_provider-azureml-destination-region': 'swedencentral', 'llm_provider-azureml-destination-deployment': 'dep-70b-fp8-tp2-250401005907', 'llm_provider-azureml-destination-endpoint': 'sdc-llama-3-3-70b-llgtrt-fp8-ep', 'llm_provider-x-ms-client-request-id': 'db13c7a8-557d-470e-990e-79732793b148', 'llm_provider-request-context': 'appId=cid-v1:bf86afeb-6735-4630-a419-996fd98375c2', 'llm_provider-azureml-model-session': 'dep-70b-fp8-tp2-250401005907', 'llm_provider-azureml-model-group': 'meta-sdc-3-3-70b-fp8-2-scaleset', 'llm_provider-date': 'Wed, 30 Apr 2025 13:10:06 GMT'}}, '_response_headers': {'content-length': '830', 'content-type': 'application/json', 'prompt_token_len': '390', 'sampling_token_len': '16', 'x-ms-rai-invoked': 'true', 'x-request-id': 'cb076ae6-8d83-49f4-8e51-c7c825a828d2', 'azureml-destination-model-group': 'meta-sdc-3-3-70b-fp8-2-scaleset', 'azureml-destination-region': 'swedencentral', 'azureml-destination-deployment': 'dep-70b-fp8-tp2-250401005907', 'azureml-destination-endpoint': 'sdc-llama-3-3-70b-llgtrt-fp8-ep', 'x-ms-client-request-id': 'db13c7a8-557d-470e-990e-79732793b148', 'request-context': 'appId=cid-v1:bf86afeb-6735-4630-a419-996fd98375c2', 'azureml-model-session': 'dep-70b-fp8-tp2-250401005907', 'azureml-model-group': 'meta-sdc-3-3-70b-fp8-2-scaleset', 'date': 'Wed, 30 Apr 2025 13:10:06 GMT'}, 'convert_tool_call_to_json_mode': None}. Received Model Group=Llama-3.3-70B-Instruct\\nAvailable Model Group Fallbacks=None\",\"type\":null,\"param\":null,\"code\":\"500\"}}\n",
      "Failed to connect to API. Status code: 500. Retrying in 20 seconds...\n",
      "{\"error\":{\"message\":\"litellm.APIConnectionError: APIConnectionError: Azure_aiException - Invalid response object Traceback (most recent call last):\\n  File \\\"/usr/lib/python3.13/site-packages/litellm/litellm_core_utils/llm_response_utils/convert_dict_to_response.py\\\", line 473, in convert_to_model_response_object\\n    reasoning_content, content = _extract_reasoning_content(\\n                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~^\\n        choice[\\\"message\\\"]\\n        ^^^^^^^^^^^^^^^^^\\n    )\\n    ^\\n  File \\\"/usr/lib/python3.13/site-packages/litellm/litellm_core_utils/llm_response_utils/convert_dict_to_response.py\\\", line 253, in _extract_reasoning_content\\n    return message[\\\"reasoning_content\\\"], message[\\\"content\\\"]\\n                                         ~~~~~~~^^^^^^^^^^^\\nKeyError: 'content'\\n\\n\\nreceived_args={'response_object': {'choices': [{'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}, 'finish_reason': 'content_filter', 'index': 0, 'message': {'reasoning_content': None, 'role': 'assistant', 'tool_calls': []}}], 'created': 1746018630, 'id': '774155fa-ce25-4f72-85ee-76abc8707fd3', 'model': 'Llama-3.3-70B-Instruct', 'object': 'chat.completion', 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'low'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'usage': {'completion_tokens': 16, 'prompt_tokens': 390, 'prompt_tokens_details': None, 'total_tokens': 406}}, 'model_response_object': ModelResponse(id='chatcmpl-115935c2-d636-41d4-86a4-bde3bc0f3dc4', created=1746018629, model='azure_ai/Llama-3.3-70B-Instruct', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content=None, role='assistant', tool_calls=None, function_call=None, provider_specific_fields=None))], usage=Usage(completion_tokens=0, prompt_tokens=0, total_tokens=0, completion_tokens_details=None, prompt_tokens_details=None)), 'response_type': 'completion', 'stream': False, 'start_time': None, 'end_time': None, 'hidden_params': {'headers': Headers({'content-length': '830', 'content-type': 'application/json', 'prompt_token_len': '390', 'sampling_token_len': '16', 'x-ms-rai-invoked': 'true', 'x-request-id': '774155fa-ce25-4f72-85ee-76abc8707fd3', 'azureml-destination-model-group': 'meta-sdc-3-3-70b-fp8-2-scaleset', 'azureml-destination-region': 'swedencentral', 'azureml-destination-deployment': 'dep-70b-fp8-tp2-250401005907', 'azureml-destination-endpoint': 'sdc-llama-3-3-70b-llgtrt-fp8-ep', 'x-ms-client-request-id': '2c11b7e5-2316-49d7-b93c-5e729dfe2977', 'request-context': 'appId=cid-v1:bf86afeb-6735-4630-a419-996fd98375c2', 'azureml-model-session': 'dep-70b-fp8-tp2-250401005907', 'azureml-model-group': 'meta-sdc-3-3-70b-fp8-2-scaleset', 'date': 'Wed, 30 Apr 2025 13:10:29 GMT'}), 'additional_headers': {'llm_provider-content-length': '830', 'llm_provider-content-type': 'application/json', 'llm_provider-prompt_token_len': '390', 'llm_provider-sampling_token_len': '16', 'llm_provider-x-ms-rai-invoked': 'true', 'llm_provider-x-request-id': '774155fa-ce25-4f72-85ee-76abc8707fd3', 'llm_provider-azureml-destination-model-group': 'meta-sdc-3-3-70b-fp8-2-scaleset', 'llm_provider-azureml-destination-region': 'swedencentral', 'llm_provider-azureml-destination-deployment': 'dep-70b-fp8-tp2-250401005907', 'llm_provider-azureml-destination-endpoint': 'sdc-llama-3-3-70b-llgtrt-fp8-ep', 'llm_provider-x-ms-client-request-id': '2c11b7e5-2316-49d7-b93c-5e729dfe2977', 'llm_provider-request-context': 'appId=cid-v1:bf86afeb-6735-4630-a419-996fd98375c2', 'llm_provider-azureml-model-session': 'dep-70b-fp8-tp2-250401005907', 'llm_provider-azureml-model-group': 'meta-sdc-3-3-70b-fp8-2-scaleset', 'llm_provider-date': 'Wed, 30 Apr 2025 13:10:29 GMT'}}, '_response_headers': {'content-length': '830', 'content-type': 'application/json', 'prompt_token_len': '390', 'sampling_token_len': '16', 'x-ms-rai-invoked': 'true', 'x-request-id': '774155fa-ce25-4f72-85ee-76abc8707fd3', 'azureml-destination-model-group': 'meta-sdc-3-3-70b-fp8-2-scaleset', 'azureml-destination-region': 'swedencentral', 'azureml-destination-deployment': 'dep-70b-fp8-tp2-250401005907', 'azureml-destination-endpoint': 'sdc-llama-3-3-70b-llgtrt-fp8-ep', 'x-ms-client-request-id': '2c11b7e5-2316-49d7-b93c-5e729dfe2977', 'request-context': 'appId=cid-v1:bf86afeb-6735-4630-a419-996fd98375c2', 'azureml-model-session': 'dep-70b-fp8-tp2-250401005907', 'azureml-model-group': 'meta-sdc-3-3-70b-fp8-2-scaleset', 'date': 'Wed, 30 Apr 2025 13:10:29 GMT'}, 'convert_tool_call_to_json_mode': None}. Received Model Group=Llama-3.3-70B-Instruct\\nAvailable Model Group Fallbacks=None\",\"type\":null,\"param\":null,\"code\":\"500\"}}\n",
      "Failed to connect to API. Status code: 500. Retrying in 20 seconds...\n",
      "{\"error\":{\"message\":\"litellm.APIConnectionError: APIConnectionError: Azure_aiException - Invalid response object Traceback (most recent call last):\\n  File \\\"/usr/lib/python3.13/site-packages/litellm/litellm_core_utils/llm_response_utils/convert_dict_to_response.py\\\", line 473, in convert_to_model_response_object\\n    reasoning_content, content = _extract_reasoning_content(\\n                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~^\\n        choice[\\\"message\\\"]\\n        ^^^^^^^^^^^^^^^^^\\n    )\\n    ^\\n  File \\\"/usr/lib/python3.13/site-packages/litellm/litellm_core_utils/llm_response_utils/convert_dict_to_response.py\\\", line 253, in _extract_reasoning_content\\n    return message[\\\"reasoning_content\\\"], message[\\\"content\\\"]\\n                                         ~~~~~~~^^^^^^^^^^^\\nKeyError: 'content'\\n\\n\\nreceived_args={'response_object': {'choices': [{'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}, 'finish_reason': 'content_filter', 'index': 0, 'message': {'reasoning_content': None, 'role': 'assistant', 'tool_calls': []}}], 'created': 1746018656, 'id': '76af8ef2-a538-4503-9554-3e013a2bb9d2', 'model': 'Llama-3.3-70B-Instruct', 'object': 'chat.completion', 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'low'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'usage': {'completion_tokens': 16, 'prompt_tokens': 390, 'prompt_tokens_details': None, 'total_tokens': 406}}, 'model_response_object': ModelResponse(id='chatcmpl-c51ea364-6080-4b11-a528-a0dd7c0f4d37', created=1746018653, model='azure_ai/Llama-3.3-70B-Instruct', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content=None, role='assistant', tool_calls=None, function_call=None, provider_specific_fields=None))], usage=Usage(completion_tokens=0, prompt_tokens=0, total_tokens=0, completion_tokens_details=None, prompt_tokens_details=None)), 'response_type': 'completion', 'stream': False, 'start_time': None, 'end_time': None, 'hidden_params': {'headers': Headers({'content-length': '830', 'content-type': 'application/json', 'prompt_token_len': '390', 'sampling_token_len': '16', 'x-ms-rai-invoked': 'true', 'x-request-id': '76af8ef2-a538-4503-9554-3e013a2bb9d2', 'azureml-destination-model-group': 'meta-sdc-3-3-70b-fp8-2-scaleset', 'azureml-destination-region': 'swedencentral', 'azureml-destination-deployment': 'dep-70b-fp8-tp2-250401012727', 'azureml-destination-endpoint': 'sdc-llama-3-3-70b-llgtrt-fp8-ep', 'x-ms-client-request-id': '0c4faef3-52d0-4391-9002-ee1812dee846', 'request-context': 'appId=cid-v1:bf86afeb-6735-4630-a419-996fd98375c2', 'azureml-model-session': 'dep-70b-fp8-tp2-250401012727', 'azureml-model-group': 'meta-sdc-3-3-70b-fp8-2-scaleset', 'date': 'Wed, 30 Apr 2025 13:10:52 GMT'}), 'additional_headers': {'llm_provider-content-length': '830', 'llm_provider-content-type': 'application/json', 'llm_provider-prompt_token_len': '390', 'llm_provider-sampling_token_len': '16', 'llm_provider-x-ms-rai-invoked': 'true', 'llm_provider-x-request-id': '76af8ef2-a538-4503-9554-3e013a2bb9d2', 'llm_provider-azureml-destination-model-group': 'meta-sdc-3-3-70b-fp8-2-scaleset', 'llm_provider-azureml-destination-region': 'swedencentral', 'llm_provider-azureml-destination-deployment': 'dep-70b-fp8-tp2-250401012727', 'llm_provider-azureml-destination-endpoint': 'sdc-llama-3-3-70b-llgtrt-fp8-ep', 'llm_provider-x-ms-client-request-id': '0c4faef3-52d0-4391-9002-ee1812dee846', 'llm_provider-request-context': 'appId=cid-v1:bf86afeb-6735-4630-a419-996fd98375c2', 'llm_provider-azureml-model-session': 'dep-70b-fp8-tp2-250401012727', 'llm_provider-azureml-model-group': 'meta-sdc-3-3-70b-fp8-2-scaleset', 'llm_provider-date': 'Wed, 30 Apr 2025 13:10:52 GMT'}}, '_response_headers': {'content-length': '830', 'content-type': 'application/json', 'prompt_token_len': '390', 'sampling_token_len': '16', 'x-ms-rai-invoked': 'true', 'x-request-id': '76af8ef2-a538-4503-9554-3e013a2bb9d2', 'azureml-destination-model-group': 'meta-sdc-3-3-70b-fp8-2-scaleset', 'azureml-destination-region': 'swedencentral', 'azureml-destination-deployment': 'dep-70b-fp8-tp2-250401012727', 'azureml-destination-endpoint': 'sdc-llama-3-3-70b-llgtrt-fp8-ep', 'x-ms-client-request-id': '0c4faef3-52d0-4391-9002-ee1812dee846', 'request-context': 'appId=cid-v1:bf86afeb-6735-4630-a419-996fd98375c2', 'azureml-model-session': 'dep-70b-fp8-tp2-250401012727', 'azureml-model-group': 'meta-sdc-3-3-70b-fp8-2-scaleset', 'date': 'Wed, 30 Apr 2025 13:10:52 GMT'}, 'convert_tool_call_to_json_mode': None}. Received Model Group=Llama-3.3-70B-Instruct\\nAvailable Model Group Fallbacks=None\",\"type\":null,\"param\":null,\"code\":\"500\"}}\n",
      "Failed to connect to API. Status code: 500. Retrying in 20 seconds...\n",
      "{\"error\":{\"message\":\"litellm.APIConnectionError: APIConnectionError: Azure_aiException - Invalid response object Traceback (most recent call last):\\n  File \\\"/usr/lib/python3.13/site-packages/litellm/litellm_core_utils/llm_response_utils/convert_dict_to_response.py\\\", line 473, in convert_to_model_response_object\\n    reasoning_content, content = _extract_reasoning_content(\\n                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~^\\n        choice[\\\"message\\\"]\\n        ^^^^^^^^^^^^^^^^^\\n    )\\n    ^\\n  File \\\"/usr/lib/python3.13/site-packages/litellm/litellm_core_utils/llm_response_utils/convert_dict_to_response.py\\\", line 253, in _extract_reasoning_content\\n    return message[\\\"reasoning_content\\\"], message[\\\"content\\\"]\\n                                         ~~~~~~~^^^^^^^^^^^\\nKeyError: 'content'\\n\\n\\nreceived_args={'response_object': {'choices': [{'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}, 'finish_reason': 'content_filter', 'index': 0, 'message': {'reasoning_content': None, 'role': 'assistant', 'tool_calls': []}}], 'created': 1746018679, 'id': '3bd5262b-61b0-4313-bebf-94f5f2923659', 'model': 'Llama-3.3-70B-Instruct', 'object': 'chat.completion', 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'low'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'usage': {'completion_tokens': 16, 'prompt_tokens': 390, 'prompt_tokens_details': None, 'total_tokens': 406}}, 'model_response_object': ModelResponse(id='chatcmpl-2ef38e25-d02f-4689-acc0-ec0ee314293a', created=1746018676, model='azure_ai/Llama-3.3-70B-Instruct', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content=None, role='assistant', tool_calls=None, function_call=None, provider_specific_fields=None))], usage=Usage(completion_tokens=0, prompt_tokens=0, total_tokens=0, completion_tokens_details=None, prompt_tokens_details=None)), 'response_type': 'completion', 'stream': False, 'start_time': None, 'end_time': None, 'hidden_params': {'headers': Headers({'content-length': '830', 'content-type': 'application/json', 'prompt_token_len': '390', 'sampling_token_len': '16', 'x-ms-rai-invoked': 'true', 'x-request-id': '3bd5262b-61b0-4313-bebf-94f5f2923659', 'azureml-destination-model-group': 'meta-sdc-3-3-70b-fp8-2-scaleset', 'azureml-destination-region': 'swedencentral', 'azureml-destination-deployment': 'dep-70b-fp8-tp2-250401012727', 'azureml-destination-endpoint': 'sdc-llama-3-3-70b-llgtrt-fp8-ep', 'x-ms-client-request-id': 'cef5b029-e48b-448b-8352-fb4f85b69592', 'request-context': 'appId=cid-v1:bf86afeb-6735-4630-a419-996fd98375c2', 'azureml-model-session': 'dep-70b-fp8-tp2-250401012727', 'azureml-model-group': 'meta-sdc-3-3-70b-fp8-2-scaleset', 'date': 'Wed, 30 Apr 2025 13:11:15 GMT'}), 'additional_headers': {'llm_provider-content-length': '830', 'llm_provider-content-type': 'application/json', 'llm_provider-prompt_token_len': '390', 'llm_provider-sampling_token_len': '16', 'llm_provider-x-ms-rai-invoked': 'true', 'llm_provider-x-request-id': '3bd5262b-61b0-4313-bebf-94f5f2923659', 'llm_provider-azureml-destination-model-group': 'meta-sdc-3-3-70b-fp8-2-scaleset', 'llm_provider-azureml-destination-region': 'swedencentral', 'llm_provider-azureml-destination-deployment': 'dep-70b-fp8-tp2-250401012727', 'llm_provider-azureml-destination-endpoint': 'sdc-llama-3-3-70b-llgtrt-fp8-ep', 'llm_provider-x-ms-client-request-id': 'cef5b029-e48b-448b-8352-fb4f85b69592', 'llm_provider-request-context': 'appId=cid-v1:bf86afeb-6735-4630-a419-996fd98375c2', 'llm_provider-azureml-model-session': 'dep-70b-fp8-tp2-250401012727', 'llm_provider-azureml-model-group': 'meta-sdc-3-3-70b-fp8-2-scaleset', 'llm_provider-date': 'Wed, 30 Apr 2025 13:11:15 GMT'}}, '_response_headers': {'content-length': '830', 'content-type': 'application/json', 'prompt_token_len': '390', 'sampling_token_len': '16', 'x-ms-rai-invoked': 'true', 'x-request-id': '3bd5262b-61b0-4313-bebf-94f5f2923659', 'azureml-destination-model-group': 'meta-sdc-3-3-70b-fp8-2-scaleset', 'azureml-destination-region': 'swedencentral', 'azureml-destination-deployment': 'dep-70b-fp8-tp2-250401012727', 'azureml-destination-endpoint': 'sdc-llama-3-3-70b-llgtrt-fp8-ep', 'x-ms-client-request-id': 'cef5b029-e48b-448b-8352-fb4f85b69592', 'request-context': 'appId=cid-v1:bf86afeb-6735-4630-a419-996fd98375c2', 'azureml-model-session': 'dep-70b-fp8-tp2-250401012727', 'azureml-model-group': 'meta-sdc-3-3-70b-fp8-2-scaleset', 'date': 'Wed, 30 Apr 2025 13:11:15 GMT'}, 'convert_tool_call_to_json_mode': None}. Received Model Group=Llama-3.3-70B-Instruct\\nAvailable Model Group Fallbacks=None\",\"type\":null,\"param\":null,\"code\":\"500\"}}\n",
      "Failed to connect to API. Status code: 500. Retrying in 20 seconds...\n",
      "{\"error\":{\"message\":\"litellm.APIConnectionError: APIConnectionError: Azure_aiException - Invalid response object Traceback (most recent call last):\\n  File \\\"/usr/lib/python3.13/site-packages/litellm/litellm_core_utils/llm_response_utils/convert_dict_to_response.py\\\", line 473, in convert_to_model_response_object\\n    reasoning_content, content = _extract_reasoning_content(\\n                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~^\\n        choice[\\\"message\\\"]\\n        ^^^^^^^^^^^^^^^^^\\n    )\\n    ^\\n  File \\\"/usr/lib/python3.13/site-packages/litellm/litellm_core_utils/llm_response_utils/convert_dict_to_response.py\\\", line 253, in _extract_reasoning_content\\n    return message[\\\"reasoning_content\\\"], message[\\\"content\\\"]\\n                                         ~~~~~~~^^^^^^^^^^^\\nKeyError: 'content'\\n\\n\\nreceived_args={'response_object': {'choices': [{'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}, 'finish_reason': 'content_filter', 'index': 0, 'message': {'reasoning_content': None, 'role': 'assistant', 'tool_calls': []}}], 'created': 1746018702, 'id': 'b2887b9a-86a3-47dc-96eb-8c290d92bf6e', 'model': 'Llama-3.3-70B-Instruct', 'object': 'chat.completion', 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'low'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'usage': {'completion_tokens': 16, 'prompt_tokens': 390, 'prompt_tokens_details': None, 'total_tokens': 406}}, 'model_response_object': ModelResponse(id='chatcmpl-e0e9e0bf-89eb-43ac-b5e1-aca41a64fe05', created=1746018699, model='azure_ai/Llama-3.3-70B-Instruct', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content=None, role='assistant', tool_calls=None, function_call=None, provider_specific_fields=None))], usage=Usage(completion_tokens=0, prompt_tokens=0, total_tokens=0, completion_tokens_details=None, prompt_tokens_details=None)), 'response_type': 'completion', 'stream': False, 'start_time': None, 'end_time': None, 'hidden_params': {'headers': Headers({'content-length': '830', 'content-type': 'application/json', 'prompt_token_len': '390', 'sampling_token_len': '16', 'x-ms-rai-invoked': 'true', 'x-request-id': 'b2887b9a-86a3-47dc-96eb-8c290d92bf6e', 'azureml-destination-model-group': 'meta-sdc-3-3-70b-fp8-2-scaleset', 'azureml-destination-region': 'swedencentral', 'azureml-destination-deployment': 'dep-70b-fp8-tp2-250401012727', 'azureml-destination-endpoint': 'sdc-llama-3-3-70b-llgtrt-fp8-ep', 'x-ms-client-request-id': 'bc88d97d-a4d5-4f68-b468-137fe39883f0', 'request-context': 'appId=cid-v1:bf86afeb-6735-4630-a419-996fd98375c2', 'azureml-model-session': 'dep-70b-fp8-tp2-250401012727', 'azureml-model-group': 'meta-sdc-3-3-70b-fp8-2-scaleset', 'date': 'Wed, 30 Apr 2025 13:11:39 GMT'}), 'additional_headers': {'llm_provider-content-length': '830', 'llm_provider-content-type': 'application/json', 'llm_provider-prompt_token_len': '390', 'llm_provider-sampling_token_len': '16', 'llm_provider-x-ms-rai-invoked': 'true', 'llm_provider-x-request-id': 'b2887b9a-86a3-47dc-96eb-8c290d92bf6e', 'llm_provider-azureml-destination-model-group': 'meta-sdc-3-3-70b-fp8-2-scaleset', 'llm_provider-azureml-destination-region': 'swedencentral', 'llm_provider-azureml-destination-deployment': 'dep-70b-fp8-tp2-250401012727', 'llm_provider-azureml-destination-endpoint': 'sdc-llama-3-3-70b-llgtrt-fp8-ep', 'llm_provider-x-ms-client-request-id': 'bc88d97d-a4d5-4f68-b468-137fe39883f0', 'llm_provider-request-context': 'appId=cid-v1:bf86afeb-6735-4630-a419-996fd98375c2', 'llm_provider-azureml-model-session': 'dep-70b-fp8-tp2-250401012727', 'llm_provider-azureml-model-group': 'meta-sdc-3-3-70b-fp8-2-scaleset', 'llm_provider-date': 'Wed, 30 Apr 2025 13:11:39 GMT'}}, '_response_headers': {'content-length': '830', 'content-type': 'application/json', 'prompt_token_len': '390', 'sampling_token_len': '16', 'x-ms-rai-invoked': 'true', 'x-request-id': 'b2887b9a-86a3-47dc-96eb-8c290d92bf6e', 'azureml-destination-model-group': 'meta-sdc-3-3-70b-fp8-2-scaleset', 'azureml-destination-region': 'swedencentral', 'azureml-destination-deployment': 'dep-70b-fp8-tp2-250401012727', 'azureml-destination-endpoint': 'sdc-llama-3-3-70b-llgtrt-fp8-ep', 'x-ms-client-request-id': 'bc88d97d-a4d5-4f68-b468-137fe39883f0', 'request-context': 'appId=cid-v1:bf86afeb-6735-4630-a419-996fd98375c2', 'azureml-model-session': 'dep-70b-fp8-tp2-250401012727', 'azureml-model-group': 'meta-sdc-3-3-70b-fp8-2-scaleset', 'date': 'Wed, 30 Apr 2025 13:11:39 GMT'}, 'convert_tool_call_to_json_mode': None}. Received Model Group=Llama-3.3-70B-Instruct\\nAvailable Model Group Fallbacks=None\",\"type\":null,\"param\":null,\"code\":\"500\"}}\n",
      "Failed to connect to API. Status code: 500. Retrying in 20 seconds...\n",
      "{\"error\":{\"message\":\"litellm.APIConnectionError: APIConnectionError: Azure_aiException - Invalid response object Traceback (most recent call last):\\n  File \\\"/usr/lib/python3.13/site-packages/litellm/litellm_core_utils/llm_response_utils/convert_dict_to_response.py\\\", line 473, in convert_to_model_response_object\\n    reasoning_content, content = _extract_reasoning_content(\\n                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~^\\n        choice[\\\"message\\\"]\\n        ^^^^^^^^^^^^^^^^^\\n    )\\n    ^\\n  File \\\"/usr/lib/python3.13/site-packages/litellm/litellm_core_utils/llm_response_utils/convert_dict_to_response.py\\\", line 253, in _extract_reasoning_content\\n    return message[\\\"reasoning_content\\\"], message[\\\"content\\\"]\\n                                         ~~~~~~~^^^^^^^^^^^\\nKeyError: 'content'\\n\\n\\nreceived_args={'response_object': {'choices': [{'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}, 'finish_reason': 'content_filter', 'index': 0, 'message': {'reasoning_content': None, 'role': 'assistant', 'tool_calls': []}}], 'created': 1746018723, 'id': '04a11c9a-fb17-4ea2-94e9-9c55e1faf619', 'model': 'Llama-3.3-70B-Instruct', 'object': 'chat.completion', 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'low'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'usage': {'completion_tokens': 16, 'prompt_tokens': 390, 'prompt_tokens_details': None, 'total_tokens': 406}}, 'model_response_object': ModelResponse(id='chatcmpl-27463569-ce99-410f-ad15-5e56ee1a296b', created=1746018722, model='azure_ai/Llama-3.3-70B-Instruct', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content=None, role='assistant', tool_calls=None, function_call=None, provider_specific_fields=None))], usage=Usage(completion_tokens=0, prompt_tokens=0, total_tokens=0, completion_tokens_details=None, prompt_tokens_details=None)), 'response_type': 'completion', 'stream': False, 'start_time': None, 'end_time': None, 'hidden_params': {'headers': Headers({'content-length': '830', 'content-type': 'application/json', 'prompt_token_len': '390', 'sampling_token_len': '16', 'x-ms-rai-invoked': 'true', 'x-request-id': '04a11c9a-fb17-4ea2-94e9-9c55e1faf619', 'azureml-destination-model-group': 'meta-sdc-3-3-70b-fp8-2-scaleset', 'azureml-destination-region': 'swedencentral', 'azureml-destination-deployment': 'dep-70b-fp8-tp2-250401005907', 'azureml-destination-endpoint': 'sdc-llama-3-3-70b-llgtrt-fp8-ep', 'x-ms-client-request-id': 'faf25cac-bccf-4997-8a1d-05dddd7e9bab', 'request-context': 'appId=cid-v1:bf86afeb-6735-4630-a419-996fd98375c2', 'azureml-model-session': 'dep-70b-fp8-tp2-250401005907', 'azureml-model-group': 'meta-sdc-3-3-70b-fp8-2-scaleset', 'date': 'Wed, 30 Apr 2025 13:12:02 GMT'}), 'additional_headers': {'llm_provider-content-length': '830', 'llm_provider-content-type': 'application/json', 'llm_provider-prompt_token_len': '390', 'llm_provider-sampling_token_len': '16', 'llm_provider-x-ms-rai-invoked': 'true', 'llm_provider-x-request-id': '04a11c9a-fb17-4ea2-94e9-9c55e1faf619', 'llm_provider-azureml-destination-model-group': 'meta-sdc-3-3-70b-fp8-2-scaleset', 'llm_provider-azureml-destination-region': 'swedencentral', 'llm_provider-azureml-destination-deployment': 'dep-70b-fp8-tp2-250401005907', 'llm_provider-azureml-destination-endpoint': 'sdc-llama-3-3-70b-llgtrt-fp8-ep', 'llm_provider-x-ms-client-request-id': 'faf25cac-bccf-4997-8a1d-05dddd7e9bab', 'llm_provider-request-context': 'appId=cid-v1:bf86afeb-6735-4630-a419-996fd98375c2', 'llm_provider-azureml-model-session': 'dep-70b-fp8-tp2-250401005907', 'llm_provider-azureml-model-group': 'meta-sdc-3-3-70b-fp8-2-scaleset', 'llm_provider-date': 'Wed, 30 Apr 2025 13:12:02 GMT'}}, '_response_headers': {'content-length': '830', 'content-type': 'application/json', 'prompt_token_len': '390', 'sampling_token_len': '16', 'x-ms-rai-invoked': 'true', 'x-request-id': '04a11c9a-fb17-4ea2-94e9-9c55e1faf619', 'azureml-destination-model-group': 'meta-sdc-3-3-70b-fp8-2-scaleset', 'azureml-destination-region': 'swedencentral', 'azureml-destination-deployment': 'dep-70b-fp8-tp2-250401005907', 'azureml-destination-endpoint': 'sdc-llama-3-3-70b-llgtrt-fp8-ep', 'x-ms-client-request-id': 'faf25cac-bccf-4997-8a1d-05dddd7e9bab', 'request-context': 'appId=cid-v1:bf86afeb-6735-4630-a419-996fd98375c2', 'azureml-model-session': 'dep-70b-fp8-tp2-250401005907', 'azureml-model-group': 'meta-sdc-3-3-70b-fp8-2-scaleset', 'date': 'Wed, 30 Apr 2025 13:12:02 GMT'}, 'convert_tool_call_to_json_mode': None}. Received Model Group=Llama-3.3-70B-Instruct\\nAvailable Model Group Fallbacks=None\",\"type\":null,\"param\":null,\"code\":\"500\"}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 44/234 [04:32<05:50,  1.84s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to connect to API. Status code: 400\n",
      "{\"error\":{\"message\":\"litellm.BadRequestError: Azure_aiException - {\\\"error\\\":{\\\"message\\\":\\\"The response was filtered due to the prompt triggering Microsoft's content management policy. Please modify your prompt and retry.\\\",\\\"type\\\":null,\\\"param\\\":\\\"prompt\\\",\\\"code\\\":\\\"content_filter\\\",\\\"status\\\":400,\\\"innererror\\\":{\\\"code\\\":\\\"ResponsibleAIPolicyViolation\\\",\\\"content_filter_result\\\":{\\\"hate\\\":{\\\"filtered\\\":false,\\\"severity\\\":\\\"safe\\\"},\\\"self_harm\\\":{\\\"filtered\\\":false,\\\"severity\\\":\\\"safe\\\"},\\\"sexual\\\":{\\\"filtered\\\":false,\\\"severity\\\":\\\"safe\\\"},\\\"violence\\\":{\\\"filtered\\\":true,\\\"severity\\\":\\\"medium\\\"}}}}}. Received Model Group=Llama-3.3-70B-Instruct\\nAvailable Model Group Fallbacks=None\",\"type\":null,\"param\":null,\"code\":\"400\"}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 52/234 [04:40<04:58,  1.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to connect to API. Status code: 400\n",
      "{\"error\":{\"message\":\"litellm.BadRequestError: Azure_aiException - {\\\"error\\\":{\\\"message\\\":\\\"The response was filtered due to the prompt triggering Microsoft's content management policy. Please modify your prompt and retry.\\\",\\\"type\\\":null,\\\"param\\\":\\\"prompt\\\",\\\"code\\\":\\\"content_filter\\\",\\\"status\\\":400,\\\"innererror\\\":{\\\"code\\\":\\\"ResponsibleAIPolicyViolation\\\",\\\"content_filter_result\\\":{\\\"hate\\\":{\\\"filtered\\\":false,\\\"severity\\\":\\\"low\\\"},\\\"self_harm\\\":{\\\"filtered\\\":false,\\\"severity\\\":\\\"safe\\\"},\\\"sexual\\\":{\\\"filtered\\\":false,\\\"severity\\\":\\\"safe\\\"},\\\"violence\\\":{\\\"filtered\\\":true,\\\"severity\\\":\\\"medium\\\"}}}}}. Received Model Group=Llama-3.3-70B-Instruct\\nAvailable Model Group Fallbacks=None\",\"type\":null,\"param\":null,\"code\":\"400\"}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|████▉     | 116/234 [05:18<01:03,  1.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to connect to API. Status code: 400\n",
      "{\"error\":{\"message\":\"litellm.BadRequestError: Azure_aiException - {\\\"error\\\":{\\\"message\\\":\\\"The response was filtered due to the prompt triggering Microsoft's content management policy. Please modify your prompt and retry.\\\",\\\"type\\\":null,\\\"param\\\":\\\"prompt\\\",\\\"code\\\":\\\"content_filter\\\",\\\"status\\\":400,\\\"innererror\\\":{\\\"code\\\":\\\"ResponsibleAIPolicyViolation\\\",\\\"content_filter_result\\\":{\\\"hate\\\":{\\\"filtered\\\":true,\\\"severity\\\":\\\"high\\\"},\\\"self_harm\\\":{\\\"filtered\\\":false,\\\"severity\\\":\\\"safe\\\"},\\\"sexual\\\":{\\\"filtered\\\":false,\\\"severity\\\":\\\"safe\\\"},\\\"violence\\\":{\\\"filtered\\\":false,\\\"severity\\\":\\\"safe\\\"}}}}}. Received Model Group=Llama-3.3-70B-Instruct\\nAvailable Model Group Fallbacks=None\",\"type\":null,\"param\":null,\"code\":\"400\"}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 117/234 [05:24<04:04,  2.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to connect to API. Status code: 400\n",
      "{\"error\":{\"message\":\"litellm.BadRequestError: Azure_aiException - {\\\"error\\\":{\\\"message\\\":\\\"The response was filtered due to the prompt triggering Microsoft's content management policy. Please modify your prompt and retry.\\\",\\\"type\\\":null,\\\"param\\\":\\\"prompt\\\",\\\"code\\\":\\\"content_filter\\\",\\\"status\\\":400,\\\"innererror\\\":{\\\"code\\\":\\\"ResponsibleAIPolicyViolation\\\",\\\"content_filter_result\\\":{\\\"hate\\\":{\\\"filtered\\\":true,\\\"severity\\\":\\\"high\\\"},\\\"self_harm\\\":{\\\"filtered\\\":false,\\\"severity\\\":\\\"safe\\\"},\\\"sexual\\\":{\\\"filtered\\\":false,\\\"severity\\\":\\\"safe\\\"},\\\"violence\\\":{\\\"filtered\\\":false,\\\"severity\\\":\\\"safe\\\"}}}}}. Received Model Group=Llama-3.3-70B-Instruct\\nAvailable Model Group Fallbacks=None\",\"type\":null,\"param\":null,\"code\":\"400\"}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████▏    | 120/234 [05:27<02:55,  1.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to connect to API. Status code: 400\n",
      "{\"error\":{\"message\":\"litellm.BadRequestError: Azure_aiException - {\\\"error\\\":{\\\"message\\\":\\\"The response was filtered due to the prompt triggering Microsoft's content management policy. Please modify your prompt and retry.\\\",\\\"type\\\":null,\\\"param\\\":\\\"prompt\\\",\\\"code\\\":\\\"content_filter\\\",\\\"status\\\":400,\\\"innererror\\\":{\\\"code\\\":\\\"ResponsibleAIPolicyViolation\\\",\\\"content_filter_result\\\":{\\\"hate\\\":{\\\"filtered\\\":true,\\\"severity\\\":\\\"high\\\"},\\\"self_harm\\\":{\\\"filtered\\\":false,\\\"severity\\\":\\\"safe\\\"},\\\"sexual\\\":{\\\"filtered\\\":false,\\\"severity\\\":\\\"safe\\\"},\\\"violence\\\":{\\\"filtered\\\":false,\\\"severity\\\":\\\"low\\\"}}}}}. Received Model Group=Llama-3.3-70B-Instruct\\nAvailable Model Group Fallbacks=None\",\"type\":null,\"param\":null,\"code\":\"400\"}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 121/234 [05:33<05:23,  2.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to connect to API. Status code: 400\n",
      "{\"error\":{\"message\":\"litellm.BadRequestError: Azure_aiException - {\\\"error\\\":{\\\"message\\\":\\\"The response was filtered due to the prompt triggering Microsoft's content management policy. Please modify your prompt and retry.\\\",\\\"type\\\":null,\\\"param\\\":\\\"prompt\\\",\\\"code\\\":\\\"content_filter\\\",\\\"status\\\":400,\\\"innererror\\\":{\\\"code\\\":\\\"ResponsibleAIPolicyViolation\\\",\\\"content_filter_result\\\":{\\\"hate\\\":{\\\"filtered\\\":true,\\\"severity\\\":\\\"high\\\"},\\\"self_harm\\\":{\\\"filtered\\\":false,\\\"severity\\\":\\\"safe\\\"},\\\"sexual\\\":{\\\"filtered\\\":false,\\\"severity\\\":\\\"safe\\\"},\\\"violence\\\":{\\\"filtered\\\":false,\\\"severity\\\":\\\"low\\\"}}}}}. Received Model Group=Llama-3.3-70B-Instruct\\nAvailable Model Group Fallbacks=None\",\"type\":null,\"param\":null,\"code\":\"400\"}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 124/234 [05:37<03:47,  2.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to connect to API. Status code: 400\n",
      "{\"error\":{\"message\":\"litellm.BadRequestError: Azure_aiException - {\\\"error\\\":{\\\"message\\\":\\\"The response was filtered due to the prompt triggering Microsoft's content management policy. Please modify your prompt and retry.\\\",\\\"type\\\":null,\\\"param\\\":\\\"prompt\\\",\\\"code\\\":\\\"content_filter\\\",\\\"status\\\":400,\\\"innererror\\\":{\\\"code\\\":\\\"ResponsibleAIPolicyViolation\\\",\\\"content_filter_result\\\":{\\\"hate\\\":{\\\"filtered\\\":true,\\\"severity\\\":\\\"high\\\"},\\\"self_harm\\\":{\\\"filtered\\\":false,\\\"severity\\\":\\\"safe\\\"},\\\"sexual\\\":{\\\"filtered\\\":false,\\\"severity\\\":\\\"safe\\\"},\\\"violence\\\":{\\\"filtered\\\":false,\\\"severity\\\":\\\"safe\\\"}}}}}. Received Model Group=Llama-3.3-70B-Instruct\\nAvailable Model Group Fallbacks=None\",\"type\":null,\"param\":null,\"code\":\"400\"}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 166/234 [06:06<01:25,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to connect to API. Status code: 400\n",
      "{\"error\":{\"message\":\"litellm.BadRequestError: Azure_aiException - {\\\"error\\\":{\\\"message\\\":\\\"The response was filtered due to the prompt triggering Microsoft's content management policy. Please modify your prompt and retry.\\\",\\\"type\\\":null,\\\"param\\\":\\\"prompt\\\",\\\"code\\\":\\\"content_filter\\\",\\\"status\\\":400,\\\"innererror\\\":{\\\"code\\\":\\\"ResponsibleAIPolicyViolation\\\",\\\"content_filter_result\\\":{\\\"hate\\\":{\\\"filtered\\\":true,\\\"severity\\\":\\\"medium\\\"},\\\"self_harm\\\":{\\\"filtered\\\":false,\\\"severity\\\":\\\"safe\\\"},\\\"sexual\\\":{\\\"filtered\\\":false,\\\"severity\\\":\\\"safe\\\"},\\\"violence\\\":{\\\"filtered\\\":false,\\\"severity\\\":\\\"safe\\\"}}}}}. Received Model Group=Llama-3.3-70B-Instruct\\nAvailable Model Group Fallbacks=None\",\"type\":null,\"param\":null,\"code\":\"400\"}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 204/234 [06:31<00:40,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to connect to API. Status code: 400\n",
      "{\"error\":{\"message\":\"litellm.BadRequestError: Azure_aiException - {\\\"error\\\":{\\\"message\\\":\\\"The response was filtered due to the prompt triggering Microsoft's content management policy. Please modify your prompt and retry.\\\",\\\"type\\\":null,\\\"param\\\":\\\"prompt\\\",\\\"code\\\":\\\"content_filter\\\",\\\"status\\\":400,\\\"innererror\\\":{\\\"code\\\":\\\"ResponsibleAIPolicyViolation\\\",\\\"content_filter_result\\\":{\\\"hate\\\":{\\\"filtered\\\":false,\\\"severity\\\":\\\"safe\\\"},\\\"self_harm\\\":{\\\"filtered\\\":false,\\\"severity\\\":\\\"safe\\\"},\\\"sexual\\\":{\\\"filtered\\\":false,\\\"severity\\\":\\\"safe\\\"},\\\"violence\\\":{\\\"filtered\\\":true,\\\"severity\\\":\\\"medium\\\"}}}}}. Received Model Group=Llama-3.3-70B-Instruct\\nAvailable Model Group Fallbacks=None\",\"type\":null,\"param\":null,\"code\":\"400\"}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 205/234 [06:34<00:52,  1.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to connect to API. Status code: 400\n",
      "{\"error\":{\"message\":\"litellm.BadRequestError: Azure_aiException - {\\\"error\\\":{\\\"message\\\":\\\"The response was filtered due to the prompt triggering Microsoft's content management policy. Please modify your prompt and retry.\\\",\\\"type\\\":null,\\\"param\\\":\\\"prompt\\\",\\\"code\\\":\\\"content_filter\\\",\\\"status\\\":400,\\\"innererror\\\":{\\\"code\\\":\\\"ResponsibleAIPolicyViolation\\\",\\\"content_filter_result\\\":{\\\"hate\\\":{\\\"filtered\\\":false,\\\"severity\\\":\\\"safe\\\"},\\\"self_harm\\\":{\\\"filtered\\\":false,\\\"severity\\\":\\\"safe\\\"},\\\"sexual\\\":{\\\"filtered\\\":false,\\\"severity\\\":\\\"safe\\\"},\\\"violence\\\":{\\\"filtered\\\":true,\\\"severity\\\":\\\"medium\\\"}}}}}. Received Model Group=Llama-3.3-70B-Instruct\\nAvailable Model Group Fallbacks=None\",\"type\":null,\"param\":null,\"code\":\"400\"}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▎| 219/234 [06:47<00:22,  1.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to connect to API. Status code: 400\n",
      "{\"error\":{\"message\":\"litellm.BadRequestError: Azure_aiException - {\\\"error\\\":{\\\"message\\\":\\\"The response was filtered due to the prompt triggering Microsoft's content management policy. Please modify your prompt and retry.\\\",\\\"type\\\":null,\\\"param\\\":\\\"prompt\\\",\\\"code\\\":\\\"content_filter\\\",\\\"status\\\":400,\\\"innererror\\\":{\\\"code\\\":\\\"ResponsibleAIPolicyViolation\\\",\\\"content_filter_result\\\":{\\\"hate\\\":{\\\"filtered\\\":true,\\\"severity\\\":\\\"medium\\\"},\\\"self_harm\\\":{\\\"filtered\\\":false,\\\"severity\\\":\\\"safe\\\"},\\\"sexual\\\":{\\\"filtered\\\":false,\\\"severity\\\":\\\"safe\\\"},\\\"violence\\\":{\\\"filtered\\\":false,\\\"severity\\\":\\\"safe\\\"}}}}}. Received Model Group=Llama-3.3-70B-Instruct\\nAvailable Model Group Fallbacks=None\",\"type\":null,\"param\":null,\"code\":\"400\"}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 234/234 [06:57<00:00,  1.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     index  ascending_comment_index  thread_id  \\\n",
      "0      117                        0         57   \n",
      "1      118                        1         57   \n",
      "2      483                        0        234   \n",
      "3      484                        1        234   \n",
      "4      713                        0        346   \n",
      "..     ...                      ...        ...   \n",
      "482  50284                        1      24261   \n",
      "483  50675                        0      24451   \n",
      "484  50676                        1      24451   \n",
      "485  50788                        0      24507   \n",
      "486  50789                        1      24507   \n",
      "\n",
      "                                                                                          post_expansion  \n",
      "0    If every American would get out of their partisan boxes, this is what every American SHOULD say:...  \n",
      "1                  I agree with your statement that if there's sufficient evidence, Hunter Biden and Joe  \n",
      "2                                                  We’re gonna get a mugshot in Georgia aren’t we?!?!? 😈  \n",
      "3                                                                    '@JoJoFromJerz https://t.co/iICBTy7  \n",
      "4    Bring back Aunt Jemima!\\nBring back Uncle Ben!\\nBring back the Land O’ Lakes Indian!\\nBring back...  \n",
      "..                                                                                                   ...  \n",
      "482                                          @NickAdamsinUSA, your idea of an alpha male, which includes  \n",
      "483  There is nothing wrong with going to Hooters with the boys, or even taking your family to Hooter...  \n",
      "484                   You're obsessed with criticizing people for choosing to dine at Hooters with their  \n",
      "485                                    They Think We Are Stupid, downer edition… https://t.co/1Xswp3IvTi  \n",
      "486                                                                                           @akheriaty  \n",
      "\n",
      "[487 rows x 4 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#try running the expansion via the UvA Azure cloud:\n",
    "#test post expansion with Llama3.3:\n",
    "chunked_result: typing.List[pd.DataFrame] = []\n",
    "GROUPER='thread_id'\n",
    "\n",
    "#sort data by Time_comment\n",
    "groupeddata = NilsThread1k[:500].groupby(GROUPER)\n",
    "for group, df in tqdm.tqdm(groupeddata):\n",
    "    df = df.loc[df.full_text != '',:]\n",
    "    df.sort_values(by=['parsed_create_time'], ascending=True).reset_index(drop=True, inplace=True)\n",
    "    df.loc[:, 'preceding_index'] = df.ascending_comment_index.shift(1)\n",
    "    df.set_index('ascending_comment_index', inplace=True, drop=False)\n",
    "    for index, row in df.iterrows():\n",
    "        if pd.isna(row['preceding_index']):\n",
    "            chunked_result.append(\n",
    "                pd.DataFrame(\n",
    "                    data=[[row['indexcol'], row['ascending_comment_index'], row['thread_id'], row['full_text']]],\n",
    "                    columns=['index', 'ascending_comment_index', 'thread_id', 'post_expansion']\n",
    "                )\n",
    "            )\n",
    "        if pd.notna(row['preceding_index']):\n",
    "            retry_count = 0\n",
    "            max_retries = 10\n",
    "           \n",
    "            while retry_count < max_retries:\n",
    "                try: \n",
    "                    response = requests.post(\n",
    "                            url=api_endpoint,\n",
    "                            headers=headers,\n",
    "                            json={\n",
    "                                'model': MODEL33largeAzure,\n",
    "                                'messages': [\n",
    "                                    {\n",
    "                                        \"role\": \"system\",\n",
    "                                        \"content\": SYSTEM_expansion\n",
    "                                    },\n",
    "                                    {\n",
    "                                        \"role\": \"user\",\n",
    "                                        \"content\": f'\"Thread\":\\n<{df[\"full_text\"][:index].to_list()}>, \"Target reply\":<<{row[\"full_text\"]}>>',\n",
    "                                    }\n",
    "                                ],\n",
    "                                'temperature': temperature_0,  \n",
    "                                'seed': SEED,\n",
    "                                \n",
    "                            }\n",
    "                        )  \n",
    "                    if response.status_code == 200:\n",
    "                        data_response = response.json()\n",
    "                        chunked_result.append(\n",
    "                        pd.DataFrame(\n",
    "                            data=[[row['indexcol'], row['ascending_comment_index'], row['thread_id'], data_response[\"choices\"][0][\"message\"][\"content\"]]],                                \n",
    "                            columns=['index', 'ascending_comment_index', 'thread_id', 'post_expansion']\n",
    "                            )\n",
    "                        )\n",
    "                        break  # Exit the retry loop on success\n",
    "                    elif response.status_code == 429:\n",
    "                        retry_count += 1\n",
    "                        retry_after = response.headers.get(\"Retry-After\")\n",
    "                        error_message = response.json().get(\"error\", {}).get(\"message\", \"\")\n",
    "                        retry_after = 30  # Default to 30 seconds if not found\n",
    "\n",
    "                        # Extract retry time from the error message\n",
    "                        if \"Try again in\" in error_message:\n",
    "                            match = re.search(r\"Try again in (\\d+) second\", error_message)\n",
    "                            if match:\n",
    "                                try:\n",
    "                                    retry_after = int(match.group(1))\n",
    "                                except (IndexError, ValueError) as e:\n",
    "                                    print(f\"Rate limit exceeded. Error extracting retry time: {e}. Retrying in {retry_after} seconds.\")\n",
    "                                    pass\n",
    "                        elif \"Please retry after\" in error_message:\n",
    "                            match = re.search(r\"Please retry after (\\d+) second\", error_message)\n",
    "                            if match:\n",
    "                                try:\n",
    "                                    retry_after = int(match.group(1))\n",
    "                                except (IndexError, ValueError) as e:\n",
    "                                    print(f\"Rate limit exceeded. Error extracting retry time: {e}. Retrying in {retry_after} seconds.\")\n",
    "                                    pass\n",
    "                                \n",
    "                        else:\n",
    "                            retry_after = 30  # Default to 30 seconds if not found\n",
    "                            print(f\"Rate limit exceeded. Defaulting to retry in {retry_after} seconds.\")\n",
    "\n",
    "                        print(f\"Rate limit exceeded. Retrying in {retry_after} seconds: {response.json()}. Retry count = {retry_count}\") \n",
    "                        time.sleep(retry_after)\n",
    "\n",
    "                    #    print(f\"Rate limit exceeded. Retrying in {wait_time} seconds...\")\n",
    "                    #    print(response.text)\n",
    "                    #    time.sleep(wait_time)\n",
    "                    elif response.status_code == 500:\n",
    "                        retry_count += 1\n",
    "                        wait_time = 20\n",
    "                        print(f\"Failed to connect to API. Status code: {response.status_code}. Retrying in {wait_time} seconds...\")\n",
    "                        print(response.text)\n",
    "                        time.sleep(wait_time)\n",
    "                    else:\n",
    "                        print(f\"Failed to connect to API. Status code: {response.status_code}\")\n",
    "                        print(response.text)\n",
    "                        break\n",
    "                except requests.exceptions.RequestException as e:   \n",
    "                    print(f\"Failed to connect to API: {e}\")\n",
    "                    retry_count += 1\n",
    "                    wait_time = 60\n",
    "                    print(f\"Retrying in {wait_time} seconds...\")\n",
    "                    time.sleep(wait_time)                 \n",
    "\n",
    "expanded_X500_posts_Azure33 = pd.concat(chunked_result, ignore_index=True)\n",
    "print(expanded_X500_posts_Azure33)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e8739c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "NilsThread500merge = NilsThread500merge.merge(expanded_X500_posts_Azure33.add_suffix('_Azure33'), left_on='indexcol', right_on='index_Azure33', how='left').drop(columns=['index_Azure33', 'ascending_comment_index_Azure33', 'thread_id_Azure33'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f554d27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save dataframe to parquet file: \n",
    "NilsThread500merge.to_parquet('data/TWON_export_Nils/NilsThread500merge.parquet', index=False, engine='pyarrow')\n",
    "expanded_X500_posts_Azure33.to_parquet('data/TWON_export_Nils/expanded_X500_posts_Azure33.parquet', index=False, engine='pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a48afb9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the parquet file:\n",
    "NilsThread500merge = pd.read_parquet('data/TWON_export_Nils/NilsThread500merge.parquet', engine='pyarrow')\n",
    "expanded_X500_posts_Azure33 = pd.read_parquet('data/TWON_export_Nils/expanded_X500_posts_Azure33.parquet', engine='pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4d8c03a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 11/234 [00:10<04:43,  1.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to connect to API. Status code: 400\n",
      "{\"error\":{\"message\":\"litellm.BadRequestError: Azure_aiException - {\\\"error\\\":{\\\"message\\\":\\\"The response was filtered due to the prompt triggering Microsoft's content management policy. Please modify your prompt and retry.\\\",\\\"type\\\":null,\\\"param\\\":\\\"prompt\\\",\\\"code\\\":\\\"content_filter\\\",\\\"status\\\":400}}. Received Model Group=nf-Llama-3.1-70b-instruct\\nAvailable Model Group Fallbacks=None\",\"type\":null,\"param\":null,\"code\":\"400\"}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 44/234 [00:53<07:58,  2.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to connect to API. Status code: 400\n",
      "{\"error\":{\"message\":\"litellm.BadRequestError: Azure_aiException - {\\\"error\\\":{\\\"message\\\":\\\"The response was filtered due to the prompt triggering Microsoft's content management policy. Please modify your prompt and retry.\\\",\\\"type\\\":null,\\\"param\\\":\\\"prompt\\\",\\\"code\\\":\\\"content_filter\\\",\\\"status\\\":400}}. Received Model Group=nf-Llama-3.1-70b-instruct\\nAvailable Model Group Fallbacks=None\",\"type\":null,\"param\":null,\"code\":\"400\"}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 52/234 [01:04<06:52,  2.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to connect to API. Status code: 400\n",
      "{\"error\":{\"message\":\"litellm.BadRequestError: Azure_aiException - {\\\"error\\\":{\\\"message\\\":\\\"The response was filtered due to the prompt triggering Microsoft's content management policy. Please modify your prompt and retry.\\\",\\\"type\\\":null,\\\"param\\\":\\\"prompt\\\",\\\"code\\\":\\\"content_filter\\\",\\\"status\\\":400}}. Received Model Group=nf-Llama-3.1-70b-instruct\\nAvailable Model Group Fallbacks=None\",\"type\":null,\"param\":null,\"code\":\"400\"}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|████▉     | 116/234 [01:57<01:21,  1.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to connect to API. Status code: 400\n",
      "{\"error\":{\"message\":\"litellm.BadRequestError: Azure_aiException - {\\\"error\\\":{\\\"message\\\":\\\"The response was filtered due to the prompt triggering Microsoft's content management policy. Please modify your prompt and retry.\\\",\\\"type\\\":null,\\\"param\\\":\\\"prompt\\\",\\\"code\\\":\\\"content_filter\\\",\\\"status\\\":400}}. Received Model Group=nf-Llama-3.1-70b-instruct\\nAvailable Model Group Fallbacks=None\",\"type\":null,\"param\":null,\"code\":\"400\"}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 117/234 [02:03<04:27,  2.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to connect to API. Status code: 400\n",
      "{\"error\":{\"message\":\"litellm.BadRequestError: Azure_aiException - {\\\"error\\\":{\\\"message\\\":\\\"The response was filtered due to the prompt triggering Microsoft's content management policy. Please modify your prompt and retry.\\\",\\\"type\\\":null,\\\"param\\\":\\\"prompt\\\",\\\"code\\\":\\\"content_filter\\\",\\\"status\\\":400}}. Received Model Group=nf-Llama-3.1-70b-instruct\\nAvailable Model Group Fallbacks=None\",\"type\":null,\"param\":null,\"code\":\"400\"}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████▏    | 120/234 [02:06<02:41,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to connect to API. Status code: 400\n",
      "{\"error\":{\"message\":\"litellm.BadRequestError: Azure_aiException - {\\\"error\\\":{\\\"message\\\":\\\"The response was filtered due to the prompt triggering Microsoft's content management policy. Please modify your prompt and retry.\\\",\\\"type\\\":null,\\\"param\\\":\\\"prompt\\\",\\\"code\\\":\\\"content_filter\\\",\\\"status\\\":400}}. Received Model Group=nf-Llama-3.1-70b-instruct\\nAvailable Model Group Fallbacks=None\",\"type\":null,\"param\":null,\"code\":\"400\"}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 121/234 [02:13<05:28,  2.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to connect to API. Status code: 400\n",
      "{\"error\":{\"message\":\"litellm.BadRequestError: Azure_aiException - {\\\"error\\\":{\\\"message\\\":\\\"The response was filtered due to the prompt triggering Microsoft's content management policy. Please modify your prompt and retry.\\\",\\\"type\\\":null,\\\"param\\\":\\\"prompt\\\",\\\"code\\\":\\\"content_filter\\\",\\\"status\\\":400}}. Received Model Group=nf-Llama-3.1-70b-instruct\\nAvailable Model Group Fallbacks=None\",\"type\":null,\"param\":null,\"code\":\"400\"}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 124/234 [02:21<05:29,  3.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to connect to API. Status code: 400\n",
      "{\"error\":{\"message\":\"litellm.BadRequestError: Azure_aiException - {\\\"error\\\":{\\\"message\\\":\\\"The response was filtered due to the prompt triggering Microsoft's content management policy. Please modify your prompt and retry.\\\",\\\"type\\\":null,\\\"param\\\":\\\"prompt\\\",\\\"code\\\":\\\"content_filter\\\",\\\"status\\\":400}}. Received Model Group=nf-Llama-3.1-70b-instruct\\nAvailable Model Group Fallbacks=None\",\"type\":null,\"param\":null,\"code\":\"400\"}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 166/234 [03:02<01:48,  1.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to connect to API. Status code: 400\n",
      "{\"error\":{\"message\":\"litellm.BadRequestError: Azure_aiException - {\\\"error\\\":{\\\"message\\\":\\\"The response was filtered due to the prompt triggering Microsoft's content management policy. Please modify your prompt and retry.\\\",\\\"type\\\":null,\\\"param\\\":\\\"prompt\\\",\\\"code\\\":\\\"content_filter\\\",\\\"status\\\":400}}. Received Model Group=nf-Llama-3.1-70b-instruct\\nAvailable Model Group Fallbacks=None\",\"type\":null,\"param\":null,\"code\":\"400\"}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 204/234 [03:33<00:42,  1.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to connect to API. Status code: 400\n",
      "{\"error\":{\"message\":\"litellm.BadRequestError: Azure_aiException - {\\\"error\\\":{\\\"message\\\":\\\"The response was filtered due to the prompt triggering Microsoft's content management policy. Please modify your prompt and retry.\\\",\\\"type\\\":null,\\\"param\\\":\\\"prompt\\\",\\\"code\\\":\\\"content_filter\\\",\\\"status\\\":400}}. Received Model Group=nf-Llama-3.1-70b-instruct\\nAvailable Model Group Fallbacks=None\",\"type\":null,\"param\":null,\"code\":\"400\"}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 205/234 [03:37<01:00,  2.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to connect to API. Status code: 400\n",
      "{\"error\":{\"message\":\"litellm.BadRequestError: Azure_aiException - {\\\"error\\\":{\\\"message\\\":\\\"The response was filtered due to the prompt triggering Microsoft's content management policy. Please modify your prompt and retry.\\\",\\\"type\\\":null,\\\"param\\\":\\\"prompt\\\",\\\"code\\\":\\\"content_filter\\\",\\\"status\\\":400}}. Received Model Group=nf-Llama-3.1-70b-instruct\\nAvailable Model Group Fallbacks=None\",\"type\":null,\"param\":null,\"code\":\"400\"}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▎| 219/234 [03:56<00:31,  2.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to connect to API. Status code: 400\n",
      "{\"error\":{\"message\":\"litellm.BadRequestError: Azure_aiException - {\\\"error\\\":{\\\"message\\\":\\\"The response was filtered due to the prompt triggering Microsoft's content management policy. Please modify your prompt and retry.\\\",\\\"type\\\":null,\\\"param\\\":\\\"prompt\\\",\\\"code\\\":\\\"content_filter\\\",\\\"status\\\":400}}. Received Model Group=nf-Llama-3.1-70b-instruct\\nAvailable Model Group Fallbacks=None\",\"type\":null,\"param\":null,\"code\":\"400\"}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 234/234 [04:06<00:00,  1.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     index  ascending_comment_index  thread_id  \\\n",
      "0      117                        0         57   \n",
      "1      118                        1         57   \n",
      "2      483                        0        234   \n",
      "3      484                        1        234   \n",
      "4      713                        0        346   \n",
      "..     ...                      ...        ...   \n",
      "483  50284                        1      24261   \n",
      "484  50675                        0      24451   \n",
      "485  50676                        1      24451   \n",
      "486  50788                        0      24507   \n",
      "487  50789                        1      24507   \n",
      "\n",
      "                                                                                          post_expansion  \n",
      "0    If every American would get out of their partisan boxes, this is what every American SHOULD say:...  \n",
      "1    I agree with the statement that if there's sufficient evidence, Hunter Biden and Joe Biden shoul...  \n",
      "2                                                  We’re gonna get a mugshot in Georgia aren’t we?!?!? 😈  \n",
      "3    @JoJoFromJerz, I think we're going to end up with a mugshot in Georgia, check this out: https://...  \n",
      "4    Bring back Aunt Jemima!\\nBring back Uncle Ben!\\nBring back the Land O’ Lakes Indian!\\nBring back...  \n",
      "..                                                                                                   ...  \n",
      "483                  @NickAdamsinUSA, your idea of an alpha male is actually more like an \"alfalfa male\"  \n",
      "484  There is nothing wrong with going to Hooters with the boys, or even taking your family to Hooter...  \n",
      "485                       You're obsessed with the idea that Hooters isn't a suitable family restaurant.  \n",
      "486                                    They Think We Are Stupid, downer edition… https://t.co/1Xswp3IvTi  \n",
      "487                                                                                           @akheriaty  \n",
      "\n",
      "[488 rows x 4 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#now try with Azure llama3.1 nf:\n",
    "#try running the expansion via the UvA Azure cloud:\n",
    "\n",
    "chunked_result: typing.List[pd.DataFrame] = []\n",
    "GROUPER='thread_id'\n",
    "\n",
    "#sort data by Time_comment\n",
    "groupeddata = NilsThread1k[:500].groupby(GROUPER)\n",
    "for group, df in tqdm.tqdm(groupeddata):\n",
    "    df = df.loc[df.full_text != '',:]\n",
    "    df.sort_values(by=['parsed_create_time'], ascending=True).reset_index(drop=True, inplace=True)\n",
    "    df.loc[:, 'preceding_index'] = df.ascending_comment_index.shift(1)\n",
    "    df.set_index('ascending_comment_index', inplace=True, drop=False)\n",
    "    for index, row in df.iterrows():\n",
    "        if pd.isna(row['preceding_index']):\n",
    "            chunked_result.append(\n",
    "                pd.DataFrame(\n",
    "                    data=[[row['indexcol'], row['ascending_comment_index'], row['thread_id'], row['full_text']]],\n",
    "                    columns=['index', 'ascending_comment_index', 'thread_id', 'post_expansion']\n",
    "                )\n",
    "            )\n",
    "        if pd.notna(row['preceding_index']):\n",
    "            retry_count = 0\n",
    "            max_retries = 5\n",
    "           \n",
    "            while retry_count < max_retries:\n",
    "                try: \n",
    "                    response = requests.post(\n",
    "                            url=api_endpoint,\n",
    "                            headers=headers,\n",
    "                            json={\n",
    "                                'model': MODEL31largeAzureNF,\n",
    "                                'messages': [\n",
    "                                    {\n",
    "                                        \"role\": \"system\",\n",
    "                                        \"content\": SYSTEM_expansion\n",
    "                                    },\n",
    "                                    {\n",
    "                                        \"role\": \"user\",\n",
    "                                        \"content\": f'\"Thread\":\\n<{df[\"full_text\"][:index].to_list()}>, \"Target reply\":<<{row[\"full_text\"]}>>',\n",
    "                                    }\n",
    "                                ],\n",
    "                                'temperature': temperature_0,  \n",
    "                                'seed': SEED,\n",
    "                                'max_tokens' : 5000,\n",
    "                                \n",
    "                            }\n",
    "                        )  \n",
    "                    if response.status_code == 200:\n",
    "                        data_response = response.json()\n",
    "                        chunked_result.append(\n",
    "                        pd.DataFrame(\n",
    "                            data=[[row['indexcol'], row['ascending_comment_index'], row['thread_id'], data_response[\"choices\"][0][\"message\"][\"content\"]]],                                \n",
    "                            columns=['index', 'ascending_comment_index', 'thread_id', 'post_expansion']\n",
    "                            )\n",
    "                        )\n",
    "                        break  # Exit the retry loop on success\n",
    "                    elif response.status_code == 429:\n",
    "                        retry_count += 1\n",
    "                        retry_after = response.headers.get(\"Retry-After\")\n",
    "                        error_message = response.json().get(\"error\", {}).get(\"message\", \"\")\n",
    "                        retry_after = 30  # Default to 30 seconds if not found\n",
    "\n",
    "                        # Extract retry time from the error message\n",
    "                        if \"Try again in\" in error_message:\n",
    "                            match = re.search(r\"Try again in (\\d+) second\", error_message)\n",
    "                            if match:\n",
    "                                try:\n",
    "                                    retry_after = int(match.group(1))\n",
    "                                except (IndexError, ValueError) as e:\n",
    "                                    print(f\"Rate limit exceeded. Error extracting retry time: {e}. Retrying in {retry_after} seconds.\")\n",
    "                                    pass\n",
    "                        elif \"Please retry after\" in error_message:\n",
    "                            match = re.search(r\"Please retry after (\\d+) second\", error_message)\n",
    "                            if match:\n",
    "                                try:\n",
    "                                    retry_after = int(match.group(1))\n",
    "                                except (IndexError, ValueError) as e:\n",
    "                                    print(f\"Rate limit exceeded. Error extracting retry time: {e}. Retrying in {retry_after} seconds.\")\n",
    "                                    pass\n",
    "                                \n",
    "                        else:\n",
    "                            retry_after = 30  # Default to 30 seconds if not found\n",
    "                            print(f\"Rate limit exceeded. Defaulting to retry in {retry_after} seconds.\")\n",
    "\n",
    "                        print(f\"Rate limit exceeded. Retrying in {retry_after} seconds: {response.json()}. Retry count = {retry_count}\") \n",
    "                        time.sleep(retry_after)\n",
    "\n",
    "                    #    print(f\"Rate limit exceeded. Retrying in {wait_time} seconds...\")\n",
    "                    #    print(response.text)\n",
    "                    #    time.sleep(wait_time)\n",
    "                    elif response.status_code == 500:\n",
    "                        retry_count += 1\n",
    "                        wait_time = 20\n",
    "                        print(f\"Failed to connect to API. Status code: {response.status_code}. Retrying in {wait_time} seconds...\")\n",
    "                        print(response.text)\n",
    "                        time.sleep(wait_time)\n",
    "                    else:\n",
    "                        print(f\"Failed to connect to API. Status code: {response.status_code}\")\n",
    "                        print(response.text)\n",
    "                        break\n",
    "                except requests.exceptions.RequestException as e:   \n",
    "                    print(f\"Failed to connect to API: {e}\")\n",
    "                    retry_count += 1\n",
    "                    wait_time = 60\n",
    "                    print(f\"Retrying in {wait_time} seconds...\")\n",
    "                    time.sleep(wait_time)                 \n",
    "\n",
    "expanded_X500_posts_Azure31NF0 = pd.concat(chunked_result, ignore_index=True)\n",
    "print(expanded_X500_posts_Azure31NF0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "245515da",
   "metadata": {},
   "outputs": [],
   "source": [
    "NilsThread500merge = NilsThread500merge.merge(expanded_X500_posts_Azure31NF0.add_suffix('_Azure31NF0'), left_on='indexcol', right_on='index_Azure31NF0', how='left').drop(columns=['index_Azure31NF0', 'ascending_comment_index_Azure31NF0', 'thread_id_Azure31NF0'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0c2b5e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save dataframe to parquet file: \n",
    "NilsThread500merge.to_parquet('data/TWON_export_Nils/NilsThread500merge.parquet', index=False, engine='pyarrow')\n",
    "expanded_X500_posts_Azure31NF0.to_parquet('data/TWON_export_Nils/expanded_X500_posts_Azure31NF0.parquet', index=False, engine='pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ac4c4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>ascending_comment_index</th>\n",
       "      <th>thread_id</th>\n",
       "      <th>post_expansion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1086</td>\n",
       "      <td>2</td>\n",
       "      <td>527</td>\n",
       "      <td>@Theo_TJ_Jordan, I appreciate you too, thank you for sharing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1200</td>\n",
       "      <td>0</td>\n",
       "      <td>583</td>\n",
       "      <td>Is there a reality TV show you'll admit to watching?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1201</td>\n",
       "      <td>1</td>\n",
       "      <td>583</td>\n",
       "      <td>\"@Mollyploofkins, is the reality TV show you watch '</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1276</td>\n",
       "      <td>0</td>\n",
       "      <td>618</td>\n",
       "      <td>You know you all these indictments in Georgia have gotten me thinking about the scale of the cri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1277</td>\n",
       "      <td>1</td>\n",
       "      <td>618</td>\n",
       "      <td>The plan was to claim it was the will of the people, a movement of</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1361</td>\n",
       "      <td>0</td>\n",
       "      <td>658</td>\n",
       "      <td>Breaking: Trump was just indicted on his most serious charges to date, those concerning Jan 6. \\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1362</td>\n",
       "      <td>1</td>\n",
       "      <td>658</td>\n",
       "      <td>@RepAdamSchiff https://t.co/an27n8zNz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1943</td>\n",
       "      <td>0</td>\n",
       "      <td>939</td>\n",
       "      <td>Today makes 13 months since your life was stolen. I love you and miss you Christian💔 https://t.c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1944</td>\n",
       "      <td>1</td>\n",
       "      <td>939</td>\n",
       "      <td>It's been 13 months since Christian's passing, praying for you and your</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2315</td>\n",
       "      <td>0</td>\n",
       "      <td>1121</td>\n",
       "      <td>The DOJ is either diverting attention away from Biden family corruption or attempting take out t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    index  ascending_comment_index  thread_id  \\\n",
       "10   1086                        2        527   \n",
       "11   1200                        0        583   \n",
       "12   1201                        1        583   \n",
       "13   1276                        0        618   \n",
       "14   1277                        1        618   \n",
       "15   1361                        0        658   \n",
       "16   1362                        1        658   \n",
       "17   1943                        0        939   \n",
       "18   1944                        1        939   \n",
       "19   2315                        0       1121   \n",
       "\n",
       "                                                                                         post_expansion  \n",
       "10                                         @Theo_TJ_Jordan, I appreciate you too, thank you for sharing  \n",
       "11                                                 Is there a reality TV show you'll admit to watching?  \n",
       "12                                                 \"@Mollyploofkins, is the reality TV show you watch '  \n",
       "13  You know you all these indictments in Georgia have gotten me thinking about the scale of the cri...  \n",
       "14                                   The plan was to claim it was the will of the people, a movement of  \n",
       "15  Breaking: Trump was just indicted on his most serious charges to date, those concerning Jan 6. \\...  \n",
       "16                                                                @RepAdamSchiff https://t.co/an27n8zNz  \n",
       "17  Today makes 13 months since your life was stolen. I love you and miss you Christian💔 https://t.c...  \n",
       "18                              It's been 13 months since Christian's passing, praying for you and your  \n",
       "19  The DOJ is either diverting attention away from Biden family corruption or attempting take out t...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#inspect errors: row 11 (filtered) and 17 (invalid, code 500) are supposed to be errors\n",
    "expanded_X500_posts_Azure31NF.iloc[10:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "47a9e42b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['@Theo_TJ_Jordan, I appreciate you too, thank you for sharing the link and your thoughts.',\n",
       " \"Is there a reality TV show you'll admit to watching?\",\n",
       " '\\'@Mollyploofkins, is the reality TV show you watch \"Is it cake?\"?\\'',\n",
       " 'You know you all these indictments in Georgia have gotten me thinking about the scale of the criminality involved in trying to steal the election for Donald Trump.\\n\\nWe had lawyers writing papers on how to keep Trump in office, slates of fake electors in several states, people… https://t.co/nxu6g1dZR2',\n",
       " 'The plan was to claim it was the will of the people, a movement of so many people involved in trying to steal the election for Donald Trump that it would be too many to jail.',\n",
       " 'Breaking: Trump was just indicted on his most serious charges to date, those concerning Jan 6. \\n\\nThis will put our democracy to a new test:\\n\\nCan the rule of law be enforced against a former president and current candidate?\\n\\nFor the sake of our democracy, that answer must be yes.',\n",
       " '@RepAdamSchiff, regarding the breaking news that Trump was just indicted on his most serious charges to date, those concerning Jan 6, and the test it poses to our democracy in enforcing the rule of law against a former president and current candidate.',\n",
       " 'Today makes 13 months since your life was stolen. I love you and miss you Christian💔 https://t.co/6wg2N82czD',\n",
       " \"It's been 13 months since Christian's passing, praying for you and your family.\",\n",
       " 'The DOJ is either diverting attention away from Biden family corruption or attempting take out their top political opponent ahead of an upcoming election. \\n\\nRegardless, Americans can see right through this.\\n\\nThe swamp will always move to protect the swamp. @IngrahamAngle https://t.co/UPZt5NmFqf']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expanded_X500_posts_Azure31NF0.iloc[10:20].post_expansion.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cfb74ad0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>full_text</th>\n",
       "      <th>ascending_comment_index</th>\n",
       "      <th>post_expansion</th>\n",
       "      <th>post_expansion_Azure33</th>\n",
       "      <th>post_expansion_Azure31NF</th>\n",
       "      <th>post_expansion_Azure31NF0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>@Theo_TJ_Jordan I appreciate you too. Thank you.</td>\n",
       "      <td>2</td>\n",
       "      <td>I appreciate you too, Theo, thank you for your kind words and for sharing the post https://t.co/...</td>\n",
       "      <td>@Theo_TJ_Jordan, I appreciate you too, thank you, in</td>\n",
       "      <td>@Theo_TJ_Jordan, I appreciate you too, thank you for sharing</td>\n",
       "      <td>@Theo_TJ_Jordan, I appreciate you too, thank you for sharing the link and your thoughts.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Is there a reality TV show you'll admit to watching?</td>\n",
       "      <td>0</td>\n",
       "      <td>Is there a reality TV show you'll admit to watching?</td>\n",
       "      <td>Is there a reality TV show you'll admit to watching?</td>\n",
       "      <td>Is there a reality TV show you'll admit to watching?</td>\n",
       "      <td>Is there a reality TV show you'll admit to watching?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>@Mollyploofkins Is it cake?!?!</td>\n",
       "      <td>1</td>\n",
       "      <td>'@Mollyploofkins, when you asked if there's a reality TV show I'll admit to watching, my answer...</td>\n",
       "      <td>'@Mollyploofkins, as for the reality TV show you watch</td>\n",
       "      <td>\"@Mollyploofkins, is the reality TV show you watch '</td>\n",
       "      <td>'@Mollyploofkins, is the reality TV show you watch \"Is it cake?\"?'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>You know you all these indictments in Georgia have gotten me thinking about the scale of the cri...</td>\n",
       "      <td>0</td>\n",
       "      <td>You know you all these indictments in Georgia have gotten me thinking about the scale of the cri...</td>\n",
       "      <td>You know you all these indictments in Georgia have gotten me thinking about the scale of the cri...</td>\n",
       "      <td>You know you all these indictments in Georgia have gotten me thinking about the scale of the cri...</td>\n",
       "      <td>You know you all these indictments in Georgia have gotten me thinking about the scale of the cri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>@TheRealThelmaJ1 Their plan was “it was the will of the people,” a movement of so many “too many...</td>\n",
       "      <td>1</td>\n",
       "      <td>Their plan was to claim it was the will of the people, framing it as a widespread movement with ...</td>\n",
       "      <td>Their plan was to claim it was the will of the people, framing it as</td>\n",
       "      <td>The plan was to claim it was the will of the people, a movement of</td>\n",
       "      <td>The plan was to claim it was the will of the people, a movement of so many people involved in tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Breaking: Trump was just indicted on his most serious charges to date, those concerning Jan 6. \\...</td>\n",
       "      <td>0</td>\n",
       "      <td>Breaking: Trump was just indicted on his most serious charges to date, those concerning Jan 6. \\...</td>\n",
       "      <td>Breaking: Trump was just indicted on his most serious charges to date, those concerning Jan 6. \\...</td>\n",
       "      <td>Breaking: Trump was just indicted on his most serious charges to date, those concerning Jan 6. \\...</td>\n",
       "      <td>Breaking: Trump was just indicted on his most serious charges to date, those concerning Jan 6. \\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>@RepAdamSchiff https://t.co/an27n8zNzj</td>\n",
       "      <td>1</td>\n",
       "      <td>In response to the news about Trump's indictment on serious charges related to Jan 6, @RepAdamSc...</td>\n",
       "      <td>'In response to the indictment of Trump on serious charges concerning Jan 6,</td>\n",
       "      <td>@RepAdamSchiff https://t.co/an27n8zNz</td>\n",
       "      <td>@RepAdamSchiff, regarding the breaking news that Trump was just indicted on his most serious cha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Today makes 13 months since your life was stolen. I love you and miss you Christian💔 https://t.c...</td>\n",
       "      <td>0</td>\n",
       "      <td>Today makes 13 months since your life was stolen. I love you and miss you Christian💔 https://t.c...</td>\n",
       "      <td>Today makes 13 months since your life was stolen. I love you and miss you Christian💔 https://t.c...</td>\n",
       "      <td>Today makes 13 months since your life was stolen. I love you and miss you Christian💔 https://t.c...</td>\n",
       "      <td>Today makes 13 months since your life was stolen. I love you and miss you Christian💔 https://t.c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>@GiannoCaldwell Praying for you and your family</td>\n",
       "      <td>1</td>\n",
       "      <td>Praying for you and your family as you remember Christian 13 months after their passing.</td>\n",
       "      <td>It's been 13 months since Christian's passing, and I'm praying for</td>\n",
       "      <td>It's been 13 months since Christian's passing, praying for you and your</td>\n",
       "      <td>It's been 13 months since Christian's passing, praying for you and your family.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>The DOJ is either diverting attention away from Biden family corruption or attempting take out t...</td>\n",
       "      <td>0</td>\n",
       "      <td>The DOJ is either diverting attention away from Biden family corruption or attempting take out t...</td>\n",
       "      <td>The DOJ is either diverting attention away from Biden family corruption or attempting take out t...</td>\n",
       "      <td>The DOJ is either diverting attention away from Biden family corruption or attempting take out t...</td>\n",
       "      <td>The DOJ is either diverting attention away from Biden family corruption or attempting take out t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                              full_text  \\\n",
       "10                                                     @Theo_TJ_Jordan I appreciate you too. Thank you.   \n",
       "11                                                 Is there a reality TV show you'll admit to watching?   \n",
       "12                                                                       @Mollyploofkins Is it cake?!?!   \n",
       "13  You know you all these indictments in Georgia have gotten me thinking about the scale of the cri...   \n",
       "14  @TheRealThelmaJ1 Their plan was “it was the will of the people,” a movement of so many “too many...   \n",
       "15  Breaking: Trump was just indicted on his most serious charges to date, those concerning Jan 6. \\...   \n",
       "16                                                               @RepAdamSchiff https://t.co/an27n8zNzj   \n",
       "17  Today makes 13 months since your life was stolen. I love you and miss you Christian💔 https://t.c...   \n",
       "18                                                      @GiannoCaldwell Praying for you and your family   \n",
       "19  The DOJ is either diverting attention away from Biden family corruption or attempting take out t...   \n",
       "\n",
       "    ascending_comment_index  \\\n",
       "10                        2   \n",
       "11                        0   \n",
       "12                        1   \n",
       "13                        0   \n",
       "14                        1   \n",
       "15                        0   \n",
       "16                        1   \n",
       "17                        0   \n",
       "18                        1   \n",
       "19                        0   \n",
       "\n",
       "                                                                                         post_expansion  \\\n",
       "10  I appreciate you too, Theo, thank you for your kind words and for sharing the post https://t.co/...   \n",
       "11                                                 Is there a reality TV show you'll admit to watching?   \n",
       "12   '@Mollyploofkins, when you asked if there's a reality TV show I'll admit to watching, my answer...   \n",
       "13  You know you all these indictments in Georgia have gotten me thinking about the scale of the cri...   \n",
       "14  Their plan was to claim it was the will of the people, framing it as a widespread movement with ...   \n",
       "15  Breaking: Trump was just indicted on his most serious charges to date, those concerning Jan 6. \\...   \n",
       "16  In response to the news about Trump's indictment on serious charges related to Jan 6, @RepAdamSc...   \n",
       "17  Today makes 13 months since your life was stolen. I love you and miss you Christian💔 https://t.c...   \n",
       "18             Praying for you and your family as you remember Christian 13 months after their passing.   \n",
       "19  The DOJ is either diverting attention away from Biden family corruption or attempting take out t...   \n",
       "\n",
       "                                                                                 post_expansion_Azure33  \\\n",
       "10                                                 @Theo_TJ_Jordan, I appreciate you too, thank you, in   \n",
       "11                                                 Is there a reality TV show you'll admit to watching?   \n",
       "12                                               '@Mollyploofkins, as for the reality TV show you watch   \n",
       "13  You know you all these indictments in Georgia have gotten me thinking about the scale of the cri...   \n",
       "14                                 Their plan was to claim it was the will of the people, framing it as   \n",
       "15  Breaking: Trump was just indicted on his most serious charges to date, those concerning Jan 6. \\...   \n",
       "16                         'In response to the indictment of Trump on serious charges concerning Jan 6,   \n",
       "17  Today makes 13 months since your life was stolen. I love you and miss you Christian💔 https://t.c...   \n",
       "18                                   It's been 13 months since Christian's passing, and I'm praying for   \n",
       "19  The DOJ is either diverting attention away from Biden family corruption or attempting take out t...   \n",
       "\n",
       "                                                                               post_expansion_Azure31NF  \\\n",
       "10                                         @Theo_TJ_Jordan, I appreciate you too, thank you for sharing   \n",
       "11                                                 Is there a reality TV show you'll admit to watching?   \n",
       "12                                                 \"@Mollyploofkins, is the reality TV show you watch '   \n",
       "13  You know you all these indictments in Georgia have gotten me thinking about the scale of the cri...   \n",
       "14                                   The plan was to claim it was the will of the people, a movement of   \n",
       "15  Breaking: Trump was just indicted on his most serious charges to date, those concerning Jan 6. \\...   \n",
       "16                                                                @RepAdamSchiff https://t.co/an27n8zNz   \n",
       "17  Today makes 13 months since your life was stolen. I love you and miss you Christian💔 https://t.c...   \n",
       "18                              It's been 13 months since Christian's passing, praying for you and your   \n",
       "19  The DOJ is either diverting attention away from Biden family corruption or attempting take out t...   \n",
       "\n",
       "                                                                              post_expansion_Azure31NF0  \n",
       "10             @Theo_TJ_Jordan, I appreciate you too, thank you for sharing the link and your thoughts.  \n",
       "11                                                 Is there a reality TV show you'll admit to watching?  \n",
       "12                                   '@Mollyploofkins, is the reality TV show you watch \"Is it cake?\"?'  \n",
       "13  You know you all these indictments in Georgia have gotten me thinking about the scale of the cri...  \n",
       "14  The plan was to claim it was the will of the people, a movement of so many people involved in tr...  \n",
       "15  Breaking: Trump was just indicted on his most serious charges to date, those concerning Jan 6. \\...  \n",
       "16  @RepAdamSchiff, regarding the breaking news that Trump was just indicted on his most serious cha...  \n",
       "17  Today makes 13 months since your life was stolen. I love you and miss you Christian💔 https://t.c...  \n",
       "18                      It's been 13 months since Christian's passing, praying for you and your family.  \n",
       "19  The DOJ is either diverting attention away from Biden family corruption or attempting take out t...  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NilsThread500merge.iloc[10:20].loc[:, ['full_text', 'ascending_comment_index', 'post_expansion', 'post_expansion_Azure33', 'post_expansion_Azure31NF','post_expansion_Azure31NF0']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f770b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#it appears some max_tokens is quietly passed restricting output length, so we need to set this to 5000..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4b8faa",
   "metadata": {},
   "source": [
    "below are only notes, analysis ends here for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af559704",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get similarity scores for claims using GPT4o:\n",
    "#adjusted request to compare claims only with preceding claims in previous comments, so not preceding claims within the same comment:\n",
    "#get simscore for posts using GPT4o:\n",
    "chunked_result: typing.List[pd.DataFrame] = []\n",
    "GROUPER='videoTitle'\n",
    "#sort data by Time_comment\n",
    "YT_input_sort = YT_claim_embeds_exp_explode.sort_values(by=['Time_comment_dt','ascending_comment_index'], ascending=True).reset_index(drop=False)\n",
    "groupeddata = YT_input_sort.groupby(GROUPER)\n",
    "\n",
    "for group, df in tqdm.tqdm(groupeddata):\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    df = df.loc[df.claims_exp_split != '',:]\n",
    "    df.sort_values(by=['Time_comment_dt','ascending_comment_index'], ascending=True).reset_index(drop=False)\n",
    "    for index, row in df.iterrows():\n",
    "        if pd.notna(row['preceding_comment_index']):\n",
    "            if row['claims_exp_split'] != '':\n",
    "                retry_count = 0\n",
    "                max_retries = 10\n",
    "\n",
    "                while retry_count < max_retries:\n",
    "                    try:  \n",
    "                        response = requests.post(\n",
    "                                url=api_endpoint,\n",
    "                                headers=headers,\n",
    "                                json={\n",
    "                                    'model': MODELgpt4o,\n",
    "                                    'messages': [\n",
    "                                        {\n",
    "                                            \"role\": \"system\",\n",
    "                                            \"content\": SYSTEM_claim_simscore                        \n",
    "                                        },\n",
    "                                        {\n",
    "                                            \"role\": \"user\",\n",
    "                                            \"content\": f'\"Target claim\":<<{row[\"claims_exp_split\"]}>>, \"preceding claims\":\\n<{df.loc[df.ascending_comment_index != row['ascending_comment_index'], ['exp_explode_index', 'claims_exp_split']].iloc[:index].to_json(orient='records')}>'\n",
    "                                        }\n",
    "                                    ],\n",
    "                                    'temperature': temperature_0,  \n",
    "                                    'seed': SEED\n",
    "                                }\n",
    "                            )\n",
    "\n",
    "                        if response.status_code == 200:\n",
    "                            data_response = response.json()\n",
    "                            # Check if 'response' key exists in the JSON response\n",
    "                            if 'choices' in data_response:\n",
    "                                content = data_response[\"choices\"][0][\"message\"][\"content\"]\n",
    "                                # Remove code block markers if present\n",
    "                                content = re.sub(r'```json|```', '', content).strip()\n",
    "                                try:\n",
    "                                    content = json.loads(content)   \n",
    "                                    # Check if content_json is a list or a dictionary\n",
    "                                    if isinstance(content, dict):\n",
    "                                        chunked_result.append({\n",
    "                                            \"claim_exp_index\": row['exp_explode_index'],\n",
    "                                            \"most_similar_claim_index\": content.get('most_similar_claim_index', ''),\n",
    "                                            'similarity_score': content.get('similarity_score', '')  \n",
    "                                        })\n",
    "                                    elif isinstance(content, list):\n",
    "                                        for item in content:\n",
    "                                            chunked_result.append({\n",
    "                                            \"claim_exp_index\": row['exp_explode_index'],\n",
    "                                            \"most_similar_claim_index\": item.get('most_similar_claim_index', ''),\n",
    "                                            'similarity_score': item.get('similarity_score', '') \n",
    "                                            })\n",
    "                                    else:\n",
    "                                        print(f\"Unexpected content type: {type(content)}\")\n",
    "\n",
    "                                except json.JSONDecodeError as e:\n",
    "                                    print(f\"Error decoding JSON: {e}\")\n",
    "                                    print(data_response)\n",
    "                            else:\n",
    "                                print(f\"No 'choices' key in response or 'choices' is empty: {content}\")                        \n",
    "                            break  # Exit the retry loop on success  \n",
    "                        elif response.status_code == 429:\n",
    "                            retry_count += 1\n",
    "                            retry_after = response.headers.get(\"Retry-After\")\n",
    "                            error_message = response.json().get(\"error\", {}).get(\"message\", \"\")\n",
    "                            retry_after = 30  # Default to 30 seconds if not found\n",
    "\n",
    "                            # Extract retry time from the error message\n",
    "                            if \"Try again in\" in error_message:\n",
    "                                match = re.search(r\"Try again in (\\d+) second\", error_message)\n",
    "                                if match:\n",
    "                                    try:\n",
    "                                        retry_after = int(match.group(1))\n",
    "                                    except (IndexError, ValueError) as e:\n",
    "                                        print(f\"Rate limit exceeded. Error extracting retry time: {e}. Retrying in {retry_after} seconds.\")\n",
    "                                        pass\n",
    "                            elif \"Please retry after\" in error_message:\n",
    "                                match = re.search(r\"Please retry after (\\d+) second\", error_message)\n",
    "                                if match:\n",
    "                                    try:\n",
    "                                        retry_after = int(match.group(1))\n",
    "                                    except (IndexError, ValueError) as e:\n",
    "                                        print(f\"Rate limit exceeded. Error extracting retry time: {e}. Retrying in {retry_after} seconds.\")\n",
    "                                        pass\n",
    "                                    \n",
    "                                    \n",
    "                            else:\n",
    "                                retry_after = 30  # Default to 30 seconds if not found\n",
    "                                print(f\"Rate limit exceeded. Defaulting to retry in {retry_after} seconds.\")\n",
    "\n",
    "\n",
    "                            print(f\"Rate limit exceeded. Retrying in {retry_after} seconds: {response.json()}. Retry count = {retry_count}\") \n",
    "                            time.sleep(retry_after)\n",
    "\n",
    "                        #    print(f\"Rate limit exceeded. Retrying in {wait_time} seconds...\")\n",
    "                        #    print(response.text)\n",
    "                        #    time.sleep(wait_time)\n",
    "                        elif response.status_code == 500:\n",
    "                            retry_count += 1\n",
    "                            wait_time = 20\n",
    "                            print(f\"Failed to connect to API. Status code: {response.status_code}. Retrying in {wait_time} seconds...\")\n",
    "                            print(response.text)\n",
    "                            time.sleep(wait_time)\n",
    "                        else:\n",
    "                            print(f\"Failed to connect to API. Status code: {response.status_code}\")\n",
    "                            print(response.text)\n",
    "                            break\n",
    "                    except requests.exceptions.RequestException as e:   \n",
    "                        print(f\"Failed to connect to API: {e}\")\n",
    "                        retry_count += 1\n",
    "                        wait_time = 60\n",
    "                        print(f\"Retrying in {wait_time} seconds...\")\n",
    "                        time.sleep(wait_time)   \n",
    "\n",
    "                    if retry_count >= max_retries:\n",
    "                        print(f\"Max retries reached for index {index}. Skipping to next item.\")\n",
    "                        break  # Exit the loop if max retries are reached              \n",
    "\n",
    "# Convert the list of dictionaries to a DataFrame\n",
    "similarity_claim_exp = pd.DataFrame(chunked_result)\n",
    "\n",
    "# Display the DataFrame to verify the changes\n",
    "print(similarity_claim_exp.head(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c103202",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get claims for the expanded posts:\n",
    "\n",
    "chunked_result: typing.List[pd.DataFrame] = []\n",
    "for index, row in tqdm(reddit_ptbod_depth1.iterrows()):\n",
    "    try: \n",
    "        chunked_result.append(\n",
    "            pd.DataFrame(\n",
    "                data=[[row['id'],\n",
    "                    requests.post(\n",
    "                        'https://inf.cl.uni-trier.de/',\n",
    "                        json={\n",
    "                            'model': MODEL33large,\n",
    "                            'system': SYSTEM_claim,\n",
    "                            'prompt': f'Check whether your answer strictly adheres to the specified format. \\n\"Posts\":\\n<{row[\"post_expansion_llama33_70b\"]}>',\n",
    "                            'options': options_zero\n",
    "                            }).json()['response']                       \n",
    "                ]],\n",
    "                columns=['id', 'claims']\n",
    "            )\n",
    "        )\n",
    "        \n",
    "    except json.JSONDecodeError:\n",
    "        print(\"invalid json response, skipping to next batch\")\n",
    "\n",
    "claim_exp_posts = pd.concat(chunked_result, ignore_index=True)\n",
    "print(claim_exp_posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c69dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now try mining the claims from the original posts with context in one go:\n",
    "chunked_result2: typing.List[pd.DataFrame] = []\n",
    "for index, row in tqdm(reddit_ptbod_depth1.iterrows()):\n",
    "    try: \n",
    "        chunked_result.append(\n",
    "            pd.DataFrame(\n",
    "                data=[[row['id'],\n",
    "                    requests.post(\n",
    "                        'https://inf.cl.uni-trier.de/',\n",
    "                        json={\n",
    "                            'model': MODEL33large,\n",
    "                            'system': SYSTEM_context_claim,\n",
    "                            'prompt': f'Check whether your answer strictly adheres to the specified format. \\n\"Target reply\":\\n<{row[\"body\"]}\\n\"Context\":\\n<{row[\"thread_title\"]}>',\n",
    "                            'options': options_zero\n",
    "                            }).json()['response']                       \n",
    "                ]],\n",
    "                columns=['id', 'claims']\n",
    "            )\n",
    "        )\n",
    "        \n",
    "    except json.JSONDecodeError:\n",
    "        print(\"invalid json response, skipping to next batch\")\n",
    "\n",
    "claim_exp_posts = pd.concat(chunked_result2, ignore_index=True)\n",
    "print(claim_exp_posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d42e57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now find the simscore for the expanded claims of on-topic political posts using GPT4o:\n",
    "#get similarity scores for claims using GPT4o:\n",
    "#adjusted request to compare claims only with preceding claims in previous comments, so not preceding claims within the same comment:\n",
    "#get simscore for posts using GPT4o:\n",
    "\n",
    "chunked_result: typing.List[pd.DataFrame] = []\n",
    "GROUPER='videoTitle'\n",
    "#sort data by Time_comment\n",
    "YT_input_sort = YT_claim_embeds_exp_explode.loc[(YT_claim_embeds_exp_explode.HAS_OPINION_DUMMY==1)&(YT_claim_embeds_exp_explode.topiccode!=0),:].sort_values(by=['Time_comment_dt','ascending_comment_index'], ascending=True).reset_index(drop=False)\n",
    "groupeddata = YT_input_sort.groupby(GROUPER)\n",
    "\n",
    "for group, df in tqdm.tqdm(groupeddata):\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    df = df.loc[df.claims_exp_split != '',:]\n",
    "    df.sort_values(by=['Time_comment_dt','ascending_comment_index'], ascending=True).reset_index(drop=False)\n",
    "    for index, row in df.iterrows():\n",
    "        if pd.notna(row['preceding_comment_index']):\n",
    "            if row['claims_exp_split'] != '':\n",
    "                retry_count = 0\n",
    "                max_retries = 10\n",
    "\n",
    "                while retry_count < max_retries:\n",
    "                    try:  \n",
    "                        response = requests.post(\n",
    "                                url=api_endpoint,\n",
    "                                headers=headers,\n",
    "                                json={\n",
    "                                    'model': MODELgpt4o,\n",
    "                                    'messages': [\n",
    "                                        {\n",
    "                                            \"role\": \"system\",\n",
    "                                            \"content\": SYSTEM_claim_simscore                        \n",
    "                                        },\n",
    "                                        {\n",
    "                                            \"role\": \"user\",\n",
    "                                            \"content\": f'\"Target claim\":<<{row[\"claims_exp_split\"]}>>, \"preceding claims\":\\n<{df.loc[df.ascending_comment_index != row['ascending_comment_index'], ['exp_explode_index', 'claims_exp_split']].iloc[:index].to_json(orient='records')}>'\n",
    "                                        }\n",
    "                                    ],\n",
    "                                    'temperature': temperature_0,  \n",
    "                                    'seed': SEED\n",
    "                                }\n",
    "                            )\n",
    "\n",
    "                        if response.status_code == 200:\n",
    "                            data_response = response.json()\n",
    "                            # Check if 'response' key exists in the JSON response\n",
    "                            if 'choices' in data_response:\n",
    "                                content = data_response[\"choices\"][0][\"message\"][\"content\"]\n",
    "                                # Remove code block markers if present\n",
    "                                content = re.sub(r'```json|```', '', content).strip()\n",
    "                                try:\n",
    "                                    content = json.loads(content)   \n",
    "                                    # Check if content_json is a list or a dictionary\n",
    "                                    if isinstance(content, dict):\n",
    "                                        chunked_result.append({\n",
    "                                            \"claim_exp_index\": row['exp_explode_index'],\n",
    "                                            \"most_similar_pol_exp_claim_index\": content.get('most_similar_claim_index', ''),\n",
    "                                            'pol_exp_claim_similarity_score': content.get('similarity_score', '')  \n",
    "                                        })\n",
    "                                    elif isinstance(content, list):\n",
    "                                        for item in content:\n",
    "                                            chunked_result.append({\n",
    "                                            \"claim_exp_index\": row['exp_explode_index'],\n",
    "                                            \"most_similar_pol_exp_claim_index\": item.get('most_similar_claim_index', ''),\n",
    "                                            'pol_exp_claim_similarity_score': item.get('similarity_score', '') \n",
    "                                            })\n",
    "                                    else:\n",
    "                                        print(f\"Unexpected content type: {type(content)}\")\n",
    "\n",
    "                                except json.JSONDecodeError as e:\n",
    "                                    print(f\"Error decoding JSON: {e}\")\n",
    "                                    print(data_response)\n",
    "                            else:\n",
    "                                print(f\"No 'choices' key in response or 'choices' is empty: {content}\")                        \n",
    "                            break  # Exit the retry loop on success  \n",
    "                        elif response.status_code == 429:\n",
    "                            retry_count += 1\n",
    "                            retry_after = response.headers.get(\"Retry-After\")\n",
    "                            error_message = response.json().get(\"error\", {}).get(\"message\", \"\")\n",
    "                            retry_after = 30  # Default to 30 seconds if not found\n",
    "\n",
    "                            # Extract retry time from the error message\n",
    "                            if \"Try again in\" in error_message:\n",
    "                                match = re.search(r\"Try again in (\\d+) second\", error_message)\n",
    "                                if match:\n",
    "                                    try:\n",
    "                                        retry_after = int(match.group(1))\n",
    "                                    except (IndexError, ValueError) as e:\n",
    "                                        print(f\"Rate limit exceeded. Error extracting retry time: {e}. Retrying in {retry_after} seconds.\")\n",
    "                                        pass\n",
    "                            elif \"Please retry after\" in error_message:\n",
    "                                match = re.search(r\"Please retry after (\\d+) second\", error_message)\n",
    "                                if match:\n",
    "                                    try:\n",
    "                                        retry_after = int(match.group(1))\n",
    "                                    except (IndexError, ValueError) as e:\n",
    "                                        print(f\"Rate limit exceeded. Error extracting retry time: {e}. Retrying in {retry_after} seconds.\")\n",
    "                                        pass\n",
    "                                    \n",
    "                                    \n",
    "                            else:\n",
    "                                retry_after = 30  # Default to 30 seconds if not found\n",
    "                                print(f\"Rate limit exceeded. Defaulting to retry in {retry_after} seconds.\")\n",
    "\n",
    "\n",
    "                            print(f\"Rate limit exceeded. Retrying in {retry_after} seconds: {response.json()}. Retry count = {retry_count}\") \n",
    "                            time.sleep(retry_after)\n",
    "\n",
    "                        #    print(f\"Rate limit exceeded. Retrying in {wait_time} seconds...\")\n",
    "                        #    print(response.text)\n",
    "                        #    time.sleep(wait_time)\n",
    "                        elif response.status_code == 500:\n",
    "                            retry_count += 1\n",
    "                            wait_time = 20\n",
    "                            print(f\"Failed to connect to API. Status code: {response.status_code}. Retrying in {wait_time} seconds...\")\n",
    "                            print(response.text)\n",
    "                            time.sleep(wait_time)\n",
    "                        else:\n",
    "                            print(f\"Failed to connect to API. Status code: {response.status_code}\")\n",
    "                            print(response.text)\n",
    "                            break\n",
    "                    except requests.exceptions.RequestException as e:   \n",
    "                        print(f\"Failed to connect to API: {e}\")\n",
    "                        retry_count += 1\n",
    "                        wait_time = 60\n",
    "                        print(f\"Retrying in {wait_time} seconds...\")\n",
    "                        time.sleep(wait_time)   \n",
    "\n",
    "                    if retry_count >= max_retries:\n",
    "                        print(f\"Max retries reached for index {index}. Skipping to next item.\")\n",
    "                        break  # Exit the loop if max retries are reached              \n",
    "\n",
    "# Convert the list of dictionaries to a DataFrame\n",
    "pol_exp_similarity_claim_exp = pd.DataFrame(chunked_result)\n",
    "\n",
    "# Display the DataFrame to verify the changes\n",
    "print(pol_exp_similarity_claim_exp.head(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54db7b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "#try asking for a similarity score instead:\n",
    "\n",
    "chunked_result: typing.List[pd.DataFrame] = []\n",
    "GROUPER='thread_title'\n",
    "groupeddata = claims.groupby(GROUPER)\n",
    "for group, df in tqdm.tqdm(groupeddata):\n",
    "    df['preceding_index'] = df.claim_index.shift(1)\n",
    "    for index, row in df.iterrows():\n",
    "        if pd.isna(row['preceding_index']):\n",
    "            chunked_result.append({\n",
    "                \"claim_index\": row['claim_index'],\n",
    "                \"most_similar_claim_index\": '',\n",
    "                'similarity_score': ''  \n",
    "                    })\n",
    "                \n",
    "        if pd.notna(row['preceding_index']):\n",
    "            threadset = int(row['preceding_index'])\n",
    "            if row['claims'] == '[]':\n",
    "                chunked_result.append({\n",
    "                    \"claim_index\": row['claim_index'],\n",
    "                    \"most_similar_claim_index\": '',\n",
    "                    'similarity_score': ''  \n",
    "                    })\n",
    "            else:\n",
    "                try: \n",
    "                    response = requests.post(\n",
    "                                    'https://inf.cl.uni-trier.de/',\n",
    "                                    json={\n",
    "                                        'model': MODELsmall,\n",
    "                                        'system': SYSTEM_claim_simscore,\n",
    "                                        'prompt': f'Reason about your respons, but respond with nothing else than the JSON. \"Target claim\":<<{row[\"claims\"]}>>, \"preceding claims and indices\":\\n<{df.loc[:,['claim_index', 'claims']][:threadset].to_json(orient='records')}>',\n",
    "                                        'options': options_zero\n",
    "                                        }).json()\n",
    "                    # Check if 'response' key exists in the JSON response\n",
    "                    if 'response' in response:\n",
    "                        for claim in json.loads(response['response']):\n",
    "                            chunked_result.append({\n",
    "                                \"claim_index\": row['claim_index'],\n",
    "                                \"most_similar_claim_index\": claim.get('most_similar_claim_index', ''),\n",
    "                                'similarity_score': claim.get('similarity_score', '')  \n",
    "                        })\n",
    "                    else:\n",
    "                        print(f\"Key 'response' not found in the response: {response}\")                      \n",
    "    \n",
    "                except json.JSONDecodeError:\n",
    "                    print(\"invalid json response, skipping to next batch\")\n",
    "\n",
    "# Convert the list of dictionaries to a DataFrame\n",
    "result_df = pd.DataFrame(chunked_result)\n",
    "\n",
    "# Display the DataFrame to verify the changes\n",
    "print(result_df.head(4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmdiv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
